# requestsëŠ” HTTP ìš”ì²­ì„ ë³´ë‚´ëŠ” ë„êµ¬ì˜ˆìš”!
# ë§ˆì¹˜ "ì›¹ì‚¬ì´íŠ¸ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ìš°ì²´êµ­" ê°™ì€ ê±°ì˜ˆìš”!
import requests
# BeautifulSoupì€ HTMLì„ íŒŒì‹±í•˜ëŠ” ë„êµ¬ì˜ˆìš”!
# ë§ˆì¹˜ "HTML ë¬¸ì„œë¥¼ ì½ê³  í•„ìš”í•œ ë¶€ë¶„ë§Œ ë½‘ì•„ë‚´ëŠ” ë„êµ¬" ê°™ì€ ê±°ì˜ˆìš”!
from bs4 import BeautifulSoup
# timeì€ ì‹œê°„ ê´€ë ¨ ì‘ì—…ì„ í•˜ëŠ” ë„êµ¬ì˜ˆìš”!
import time
# sysëŠ” ì‹œìŠ¤í…œ ê´€ë ¨ ì‘ì—…ì„ í•˜ëŠ” ë„êµ¬ì˜ˆìš”!
# ë§ˆì¹˜ "ì»´í“¨í„° ì‹œìŠ¤í…œê³¼ ëŒ€í™”í•˜ëŠ”" ê²ƒì²˜ëŸ¼!
import sys

def auto_researcher(url):
    """
    ì›¹ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì„œ GraphRAG APIë¡œ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Args:
        url: í¬ë¡¤ë§í•  ì›¹ í˜ì´ì§€ URL
        
    Returns:
        API ì‘ë‹µ ê²°ê³¼ (dict)
    ì°¸ê³ : url ì •ë³´ë„ í•¨ê»˜ APIë¡œ ì „ì†¡í•´ìš”!
    """
    try:
        # 1. Web Crawler: ë‰´ìŠ¤ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        # requests.get()ì€ ì›¹ í˜ì´ì§€ì— GET ìš”ì²­ì„ ë³´ë‚´ëŠ” ê±°ì˜ˆìš”!
        # ë§ˆì¹˜ "ì›¹ì‚¬ì´íŠ¸ì— 'ì´ í˜ì´ì§€ë¥¼ ë³´ì—¬ì¤˜'ë¼ê³  ìš”ì²­í•˜ëŠ”" ê²ƒì²˜ëŸ¼!
        print(f"ğŸŒ ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {url}")
        
        # headersëŠ” "ìš”ì²­ í—¤ë”"ì˜ˆìš”! ì›¹ì‚¬ì´íŠ¸ì— "ë‚˜ëŠ” ì´ëŸ° ë¸Œë¼ìš°ì €ì•¼"ë¼ê³  ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”!
        # User-Agentë¥¼ ì„¤ì •í•˜ë©´ ì¼ë¶€ ì›¹ì‚¬ì´íŠ¸ê°€ í¬ë¡¤ë§ì„ ì°¨ë‹¨í•˜ì§€ ì•Šì•„ìš”!
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # requests.get()ì— headersë¥¼ ì¶”ê°€í•´ì„œ ìš”ì²­í•´ìš”!
        response = requests.get(url, headers=headers, timeout=10)  # timeoutì€ "10ì´ˆ ì•ˆì— ì‘ë‹µì´ ì—†ìœ¼ë©´ í¬ê¸°"ë¼ëŠ” ëœ»ì´ì—ìš”!
        
        # response.raise_for_status()ëŠ” "ì‘ë‹µì´ ì„±ê³µì ì´ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œ"ë¼ëŠ” ëœ»ì´ì—ìš”!
        response.raise_for_status()
        
        # 2. Parser: ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        # BeautifulSoup()ì€ HTMLì„ íŒŒì‹±í•˜ëŠ” ê±°ì˜ˆìš”!
        # 'html.parser'ëŠ” "HTML íŒŒì„œë¥¼ ì‚¬ìš©í•œë‹¤"ëŠ” ëœ»ì´ì—ìš”!
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # soup.find_all('p')ëŠ” "ëª¨ë“  <p> íƒœê·¸ë¥¼ ì°¾ì•„"ë¼ëŠ” ëœ»ì´ì—ìš”!
        # p.get_text()ëŠ” "ê·¸ íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì™€"ë¼ëŠ” ëœ»ì´ì—ìš”!
        # " ".join()ì€ "ê³µë°±ìœ¼ë¡œ ì—°ê²°í•´ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“¤ì–´"ë¼ëŠ” ëœ»ì´ì—ìš”!
        text_content = " ".join([p.get_text() for p in soup.find_all('p')])
        
        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
        if not text_content.strip():
            # <p> íƒœê·¸ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ íƒœê·¸ë“¤ë„ ì‹œë„í•´ìš”!
            text_content = " ".join([tag.get_text() for tag in soup.find_all(['article', 'div', 'main'])])
        
        # í…ìŠ¤íŠ¸ê°€ ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
        if not text_content.strip():
            raise ValueError("ì›¹ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ìš”!")
        
        print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text_content)} ê¸€ì")
        
        # 3. API Integration: GraphRAG APIë¡œ ì „ì†¡
        api_url = "http://127.0.0.1:8000/insert"
        # payloadëŠ” "ì „ì†¡í•  ë°ì´í„°"ì˜ˆìš”!
        # text_content[:1000]ì€ "ì• 1000ê¸€ìë§Œ ì‚¬ìš©í•œë‹¤"ëŠ” ëœ»ì´ì—ìš”! (í…ŒìŠ¤íŠ¸ìš©)
        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ [:1000]ì„ ì œê±°í•˜ë©´ ë¼ìš”!
        # í•˜ì§€ë§Œ ë„ˆë¬´ ê¸¸ë©´ API í˜¸ì¶œì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”!
        max_length = 5000  # ìµœëŒ€ 5000ê¸€ìë¡œ ì œí•œ (í•„ìš”í•˜ë©´ ë³€ê²½ ê°€ëŠ¥)
        text_to_send = text_content[:max_length] if len(text_content) > max_length else text_content
        
        print(f"ğŸš€ GraphRAG APIë¡œ ì „ì†¡ ì¤‘...")
        print(f"â±ï¸  ì¸ë±ì‹±ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. (ìµœëŒ€ 5ë¶„ ëŒ€ê¸°)")
        print(f"ğŸ’¡ íŒ: í…ìŠ¤íŠ¸ê°€ ê¸¸ìˆ˜ë¡ ë” ì˜¤ë˜ ê±¸ë ¤ìš”!")
        
        # timeoutì„ 300ì´ˆ(5ë¶„)ë¡œ ëŠ˜ë ¤ìš”! ì¸ë±ì‹±ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”!
        # connect timeoutì€ "ì—°ê²°í•˜ëŠ”ë° 10ì´ˆ", read timeoutì€ "ì‘ë‹µì„ ì½ëŠ”ë° 300ì´ˆ"ë¼ëŠ” ëœ»ì´ì—ìš”!
        res = requests.post(api_url, json={"text": text_to_send}, timeout=(10, 300))
        
        # res.raise_for_status()ëŠ” "ì‘ë‹µì´ ì„±ê³µì ì´ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œ"ë¼ëŠ” ëœ»ì´ì—ìš”!
        res.raise_for_status()
        
        result = res.json()
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ!")
        return result
        
    except requests.exceptions.RequestException as e:
        # requests.exceptions.RequestExceptionì€ "HTTP ìš”ì²­ ê´€ë ¨ ì—ëŸ¬"ì˜ˆìš”!
        error_msg = f"HTTP ìš”ì²­ ì—ëŸ¬: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg, "status": "error"}
    except ValueError as e:
        # ValueErrorëŠ” "ê°’ì´ ì˜ëª»ë˜ì—ˆë‹¤"ëŠ” ì—ëŸ¬ì˜ˆìš”!
        error_msg = f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—ëŸ¬: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg, "status": "error"}
    except Exception as e:
        # Exceptionì€ "ëª¨ë“  ì¢…ë¥˜ì˜ ì—ëŸ¬"ì˜ˆìš”!
        error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg, "status": "error"}

# if __name__ == "__main__": ì´ê±´ "ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
# ë§ˆì¹˜ "ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´"ë¼ëŠ” ì˜ë¯¸ì˜ˆìš”!
if __name__ == "__main__":
    # sys.argvëŠ” "ëª…ë ¹ì¤„ì—ì„œ ì…ë ¥í•œ ì¸ìë“¤"ì´ì—ìš”!
    # ì˜ˆ: python3 url.py https://example.com
    #     sys.argv[0] = "url.py"
    #     sys.argv[1] = "https://example.com"
    
    # len(sys.argv)ëŠ” "ì¸ìì˜ ê°œìˆ˜"ì˜ˆìš”!
    if len(sys.argv) < 2:
        # URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì‚¬ìš©ë²•ì„ ì•Œë ¤ì¤˜ìš”!
        print("=" * 60)
        print("ğŸ“š ì‚¬ìš©ë²•:")
        print("=" * 60)
        print("python3 src/url.py <URL>")
        print()
        print("ì˜ˆì‹œ:")
        print("  python3 src/url.py https://www.example.com")
        print("  python3 src/url.py https://news.ycombinator.com")
        print("=" * 60)
        # sys.exit(1)ì€ "í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê³  ì—ëŸ¬ ì½”ë“œ 1ì„ ë°˜í™˜í•œë‹¤"ëŠ” ëœ»ì´ì—ìš”!
        sys.exit(1)
    
    # sys.argv[1]ì€ "ì²« ë²ˆì§¸ ì¸ì(URL)"ì˜ˆìš”!
    url = sys.argv[1]
    
    print("=" * 60)
    print("ğŸš€ ì›¹ í¬ë¡¤ë§ ë° GraphRAG ì¸ë±ì‹± ì‹œì‘")
    print("=" * 60)
    print(f"URL: {url}")
    print()
    
    # auto_researcher í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ìš”!
    result = auto_researcher(url)
    
    print()
    print("=" * 60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print("=" * 60)
    # json.dumps()ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” ê±°ì˜ˆìš”!
    # indent=2ëŠ” "ë“¤ì—¬ì“°ê¸°ë¥¼ 2ì¹¸ìœ¼ë¡œ í•´ì„œ ë³´ê¸° ì¢‹ê²Œ ë§Œë“¤ì–´"ë¼ëŠ” ëœ»ì´ì—ìš”!
    # ensure_ascii=FalseëŠ” "í•œê¸€ë„ ì œëŒ€ë¡œ í‘œì‹œí•´"ë¼ëŠ” ëœ»ì´ì—ìš”!
    import json
    print(json.dumps(result, indent=2, ensure_ascii=False))