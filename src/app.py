# app.pyëŠ” "FastAPI ì„œë²„"ë¥¼ ë§Œë“œëŠ” íŒŒì¼ì´ì—ìš”!
# ë§ˆì¹˜ "ì›¹ ì„œë²„ë¥¼ ë§Œë“œëŠ” ë„êµ¬ ìƒì" ê°™ì€ ê±°ì˜ˆìš”!

from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
import uvicorn
import os
import sys

# src ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€í•´ìš”!
# ì´ë ‡ê²Œ í•˜ë©´ 'from engine import ...' ê°™ì€ importê°€ ì‘ë™í•´ìš”!
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# engine ëª¨ë“ˆì—ì„œ HybridGraphRAGEngineì„ ê°€ì ¸ì™€ìš”!
from engine import HybridGraphRAGEngine
from config import print_config, validate_config, ROUTER_MODEL, ROUTER_TEMPERATURE, WEB_SEARCH_MAX_RESULTS, OPENAI_API_KEY, OPENAI_BASE_URL, MCP_CONFIG_PATH
from search import web_search, format_search_results
from openai import AsyncOpenAI
from utils import get_executive_report_prompt, get_web_search_report_prompt
from mcp import MCPManager

# --- [1] ì „ì—­ ë³€ìˆ˜ ---
# engineì€ "GraphRAG ì—”ì§„"ì´ì—ìš”!
# Noneì€ "ì•„ì§ ì•„ë¬´ê²ƒë„ ì—†ë‹¤"ëŠ” ëœ»ì´ì—ìš”!
engine: HybridGraphRAGEngine = None
mcp_manager: MCPManager = None

# --- [2] ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
# @asynccontextmanagerëŠ” "ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"ë¥¼ ë§Œë“œëŠ” ê±°ì˜ˆìš”!
# ë§ˆì¹˜ "ì„œë²„ê°€ ì‹œì‘ë  ë•Œì™€ ëë‚  ë•Œ ë­”ê°€ë¥¼ í•˜ëŠ”" ê²ƒì²˜ëŸ¼!
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ì´ì—ìš”!
    global engine, mcp_manager
    
    # ì„¤ì • ì •ë³´ë¥¼ ì¶œë ¥í•´ìš”!
    print_config()
    
    # validate_config()ëŠ” "ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ëŠ”" í•¨ìˆ˜ì˜ˆìš”!
    validate_config()
    
    # HybridGraphRAGEngineì„ ì´ˆê¸°í™”í•˜ëŠ” ê±°ì˜ˆìš”!
    # ë§ˆì¹˜ "GraphRAG ì—”ì§„ì„ ì¤€ë¹„í•˜ëŠ”" ê²ƒì²˜ëŸ¼!
    print("ğŸš€ HybridGraphRAGEngine ì´ˆê¸°í™” ì¤‘...")
    engine = HybridGraphRAGEngine()
    print("âœ… HybridGraphRAGEngine ì¤€ë¹„ ì™„ë£Œ!")
    
    # MCP Manager ì´ˆê¸°í™”
    print("ğŸš€ MCP Manager ì´ˆê¸°í™” ì¤‘...")
    mcp_manager = MCPManager(config_path=MCP_CONFIG_PATH)
    await mcp_manager.start_cleanup_task()
    print("âœ… MCP Manager ì¤€ë¹„ ì™„ë£Œ (lazy load ëª¨ë“œ)")
    
    # yieldëŠ” "ì—¬ê¸°ì„œ ì ì‹œ ë©ˆì¶°ì„œ ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê³ , ë‚˜ì¤‘ì— ë‹¤ì‹œ ëŒì•„ì™€"ë¼ëŠ” ëœ»ì´ì—ìš”!
    yield
    
    # ì„œë²„ê°€ ì¢…ë£Œë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ì´ì—ìš”!
    if mcp_manager:
        await mcp_manager.shutdown()
        print("âœ… MCP Manager ì¢…ë£Œ ì™„ë£Œ")

# --- [3] FastAPI ì•± ì´ˆê¸°í™” ---
# FastAPI()ëŠ” "ì›¹ ì„œë²„ ì•±ì„ ë§Œë“¤ì–´ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
# lifespanì€ "ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"ì˜ˆìš”!
app = FastAPI(
    title="VIK AI: Hybrid GraphRAG API",
    description="ê¸ˆìœµ ë¶„ì„ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ GraphRAG APIì˜ˆìš”! ì¸ë±ì‹±ì€ OpenAI API, ì§ˆë¬¸ì€ API/LOCAL ì„ íƒ ê°€ëŠ¥í•´ìš”!",
    version="2.0.0",
    lifespan=lifespan  # lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ì—°ê²°í•´ìš”!
)

# --- [4] Pydantic ëª¨ë¸ ---
# Pydantic ëª¨ë¸ì€ "ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒ"ì´ì—ìš”!
# ë§ˆì¹˜ "ì´ëŸ° ëª¨ì–‘ì˜ ë°ì´í„°ë¥¼ ë°›ì„ê²Œìš”!"ë¼ê³  ë¯¸ë¦¬ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”!

# QueryRequestëŠ” "ì§ˆë¬¸ ìš”ì²­"ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì´ì—ìš”!
class QueryRequest(BaseModel):
    # questionì€ "ì§ˆë¬¸ ë‚´ìš©"ì´ì—ìš”!
    question: str
    # modeëŠ” "ì–´ë–¤ ëª¨ë“œë¥¼ ì‚¬ìš©í• ì§€" ì •í•˜ëŠ” ê±°ì˜ˆìš”. "api" ë˜ëŠ” "local"!
    # ê¸°ë³¸ê°’ì€ "local"ì´ì—ìš”!
    mode: str = "local"
    # temperatureëŠ” "ì‘ë‹µì˜ ì°½ì˜ì„±"ì„ ì¡°ì ˆí•´ìš”! (0.0 = ì •í™•, 2.0 = ì°½ì˜ì )
    temperature: float = 0.2
    # top_këŠ” "ê²€ìƒ‰í•  ì²­í¬ ê°œìˆ˜"ë¥¼ ì •í•´ìš”!
    top_k: int = 30
    # search_typeì€ "local" (íŠ¹ì • ê²€ìƒ‰) ë˜ëŠ” "global" (ì „ì²´ ìš”ì•½)
    search_type: str = "local"
    # enable_web_searchëŠ” "ì›¹ ê²€ìƒ‰ì„ í™œì„±í™”í• ì§€" ì •í•´ìš”! (ê¸°ë³¸ê°’: False)
    enable_web_search: bool = False
    # use_multi_agentëŠ” "Multi-Agent ëª¨ë“œë¥¼ ì‚¬ìš©í• ì§€" ì •í•´ìš”! (ê¸°ë³¸ê°’: False)
    use_multi_agent: bool = False
    
    # Pydantic v2 ìŠ¤íƒ€ì¼ë¡œ ì˜ˆì‹œ ì„¤ì •
    model_config = {
        "json_schema_extra": {
            "example": {
                "question": "What is NVIDIA revenue?",
                "mode": "local",
                "search_type": "local",
                "enable_web_search": False,
                "use_multi_agent": False
            }
        }
    }

# InsertRequestëŠ” "í…ìŠ¤íŠ¸ ì¶”ê°€ ìš”ì²­"ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì´ì—ìš”!
class InsertRequest(BaseModel):
    # textëŠ” "ì¶”ê°€í•  í…ìŠ¤íŠ¸"ì˜ˆìš”!
    text: str
    
    # Pydantic v2 ìŠ¤íƒ€ì¼ë¡œ ì„¤ì • (deprecation warning í•´ê²°)
    model_config = {
        "json_schema_extra": {
            "example": {
                "text": "NVIDIA reported record revenue of $57.0 billion in Q3 2026."
            }
        }
    }

# --- [5] Router í•¨ìˆ˜ë“¤ (Decision Layer) ---
# ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì›¹ ê²€ìƒ‰ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë“¤ì´ì—ìš”!

async def classify_query(question: str) -> str:
    """
    GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” Router í•¨ìˆ˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        "GRAPH_RAG" ë˜ëŠ” "WEB_SEARCH"
    """
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        # ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """You are a query classifier for a financial AI system.

Your task is to classify user questions into two categories:

1. GRAPH_RAG: Questions about uploaded PDF documents, company information, people, financials from internal reports
   Examples:
   - "What is NVIDIA's Q3 revenue?"
   - "Who is Jensen Huang?" (person information from documents)
   - "How old is the CEO?" (biographical information)
   - "Summarize the uploaded report"
   - "What are the key findings in the document?"

2. WEB_SEARCH: Questions EXPLICITLY requiring TODAY's/LATEST/CURRENT real-time market data or breaking news
   Examples:
   - "What is today's stock price?"
   - "Latest news TODAY about Tesla"
   - "Current inflation rate RIGHT NOW"
   - "What happened in the market TODAY?"

IMPORTANT: Default to GRAPH_RAG unless the question EXPLICITLY asks for TODAY/LATEST/CURRENT/NOW information.

Respond with ONLY ONE WORD: Either "GRAPH_RAG" or "WEB_SEARCH" - nothing else."""

        # GPT-4o-mini í˜¸ì¶œ
        response = await client.chat.completions.create(
            model=ROUTER_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Classify this question: {question}"}
            ],
            temperature=ROUTER_TEMPERATURE,
            max_tokens=10
        )
        
        # ì‘ë‹µ ì¶”ì¶œ ë° ì •ê·œí™”
        classification = response.choices[0].message.content.strip().upper()
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if "GRAPH_RAG" in classification:
            return "GRAPH_RAG"
        elif "WEB_SEARCH" in classification or "WEB" in classification:
            return "WEB_SEARCH"
        else:
            # ê¸°ë³¸ê°’: GRAPH_RAG (ë‚´ë¶€ ë¬¸ì„œ ìš°ì„ )
            print(f"âš ï¸ ë¶„ë¥˜ ê²°ê³¼ê°€ ëª…í™•í•˜ì§€ ì•Šì•„ìš”: {classification}, ê¸°ë³¸ê°’ GRAPH_RAG ì‚¬ìš©")
            return "GRAPH_RAG"
    
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’: GRAPH_RAG
        return "GRAPH_RAG"


async def handle_web_search(question: str) -> str:
    """
    ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€
    """
    try:
        # 1. DuckDuckGoë¡œ ì›¹ ê²€ìƒ‰
        print(f"ğŸ” ì›¹ ê²€ìƒ‰ ì‹œì‘: {question}")
        search_results = await web_search(question, max_results=WEB_SEARCH_MAX_RESULTS)
        
        if not search_results:
            return "ì£„ì†¡í•´ìš”, ê´€ë ¨ëœ ìµœì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
        
        # 2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·
        formatted_results = await format_search_results(search_results)
        
        # 3. GPT-4o-minië¡œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        synthesis_prompt = f"""Based on the following web search results, answer the user's question comprehensively.
Include relevant data and cite sources with URLs when possible.

User Question: {question}

Search Results:
{formatted_results}

Provide a clear, concise answer with sources."""

        response = await client.chat.completions.create(
            model=ROUTER_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful financial assistant that synthesizes web search results into clear answers."},
                {"role": "user", "content": synthesis_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content.strip()
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        sources = "\n\nğŸ“š ì¶œì²˜:\n"
        for idx, result in enumerate(search_results[:3], 1):
            sources += f"{idx}. {result['title']}\n   {result['url']}\n"
        
        return answer + sources
    
    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}"


# --- [6] ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/")ëŠ” "ë£¨íŠ¸ ê²½ë¡œ(/)ì— GET ìš”ì²­ì´ ì˜¤ë©´" ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
# ë§ˆì¹˜ "í™ˆí˜ì´ì§€ì— ì ‘ì†í•˜ë©´" ì‹¤í–‰ë˜ëŠ” ê±°ì˜ˆìš”!
@app.get("/")
async def root():
    # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
    return {
        "message": "VIK AI Hybrid GraphRAG APIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•´ìš”!",
        "description": "ì¸ë±ì‹±ì€ OpenAI API(gpt-5-mini)ë¥¼ ì‚¬ìš©í•˜ê³ , ì§ˆë¬¸ì€ API/LOCAL ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!",
        "endpoints": {
            "/insert": "í…ìŠ¤íŠ¸ ì¸ë±ì‹±í•˜ê¸° (POST) - OpenAI API ì‚¬ìš©",
            "/query": "ì§ˆë¬¸í•˜ê¸° (POST) - mode íŒŒë¼ë¯¸í„°ë¡œ 'api' ë˜ëŠ” 'local' ì„ íƒ",
            "/health": "ì„œë²„ ìƒíƒœ í™•ì¸ (GET)",
            "/graph_stats": "ê·¸ë˜í”„ í˜„í™© í™•ì¸ (GET)",
            "/visualize": "ê·¸ë˜í”„ ì‹œê°í™” HTML ìƒì„± (GET)",
            "/docs": "API ë¬¸ì„œ ë³´ê¸° (GET)"
        },
        "usage": {
            "insert": {
                "method": "POST",
                "url": "/insert",
                "body": {"text": "ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸"}
            },
            "query": {
                "method": "POST",
                "url": "/query",
                "body": {
                    "question": "ì§ˆë¬¸ ë‚´ìš©",
                    "mode": "api ë˜ëŠ” local (ê¸°ë³¸ê°’: local)"
                }
            }
        }
    }

# --- [7] ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/health")ëŠ” "ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/health")
async def health():
    # ì„œë²„ê°€ ì˜ ì‘ë™í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”!
    return {
        "status": "healthy",
        "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì´ì—ìš”!",
        "engine_ready": engine is not None
    }

# --- [7-1] MCP ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/mcp/status")
async def mcp_status():
    """MCP Manager ìƒíƒœ í™•ì¸"""
    if mcp_manager is None:
        return {
            "status": "disabled",
            "message": "MCP Managerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        }
    
    status = mcp_manager.get_status()
    return {
        "status": "active",
        "message": "MCP Managerê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤",
        **status
    }

# --- [7-2] ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/memory")
async def memory_status():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸ (8GB RAM í™˜ê²½ ëª¨ë‹ˆí„°ë§)"""
    import psutil
    
    mem = psutil.virtual_memory()
    return {
        "total_gb": round(mem.total / (1024 ** 3), 2),
        "used_gb": round(mem.used / (1024 ** 3), 2),
        "available_gb": round(mem.available / (1024 ** 3), 2),
        "percent": mem.percent,
        "status": "healthy" if mem.percent < 85 else "warning",
        "message": "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ì •ìƒì…ë‹ˆë‹¤." if mem.percent < 85 else "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤!"
    }

# --- [8] ê·¸ë˜í”„ í†µê³„ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/graph_stats")ëŠ” "ê·¸ë˜í”„ í†µê³„ë¥¼ ë³´ì—¬ì£¼ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/graph_stats")
async def graph_stats():
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        return {"nodes": 0, "edges": 0, "message": "ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!"}
    
    # engine.get_graph_stats()ëŠ” ê·¸ë˜í”„ í†µê³„ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê±°ì˜ˆìš”!
    return engine.get_graph_stats()

# --- [9] ê·¸ë˜í”„ ì´ˆê¸°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# @app.post("/reset")ëŠ” "ê·¸ë˜í”„ë¥¼ ì´ˆê¸°í™”í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.post("/reset",
          summary="ê·¸ë˜í”„ ì´ˆê¸°í™”",
          description="ê¸°ì¡´ ê·¸ë˜í”„ ìŠ¤í† ë¦¬ì§€ë¥¼ ë°±ì—…í•˜ê³  ì‚­ì œí•œ í›„ ìƒˆë¡œìš´ ê·¸ë˜í”„ë¡œ ì‹œì‘í•´ìš”!")
async def reset_graph():
    global engine
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    try:
        import shutil
        from datetime import datetime
        
        # ë°±ì—… í´ë” ì´ë¦„ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        backup_dir = f"{engine.working_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ê¸°ì¡´ ê·¸ë˜í”„ ìŠ¤í† ë¦¬ì§€ê°€ ìˆìœ¼ë©´ ë°±ì—…
        if os.path.exists(engine.working_dir):
            shutil.move(engine.working_dir, backup_dir)
            print(f"âœ… ê¸°ì¡´ ê·¸ë˜í”„ ë°±ì—… ì™„ë£Œ: {backup_dir}")
        
        # ì—”ì§„ ì¬ì´ˆê¸°í™”
        engine = HybridGraphRAGEngine()
        
        return {
            "message": "ê·¸ë˜í”„ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆì–´ìš”!",
            "status": "success",
            "backup_dir": backup_dir if os.path.exists(backup_dir) else None
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê·¸ë˜í”„ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [10] ê·¸ë˜í”„ ì‹œê°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/visualize")ëŠ” "ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ëŠ” HTML íŒŒì¼ì„ ìƒì„±í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/visualize",
         summary="ê·¸ë˜í”„ ì‹œê°í™”",
         description="GraphRAG ê·¸ë˜í”„ë¥¼ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ ì‹œê°í™”í•œ HTML íŒŒì¼ì„ ìƒì„±í•´ìš”!")
async def visualize():
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    try:
        # visualize.pyì—ì„œ visualize_graph í•¨ìˆ˜ë¥¼ ê°€ì ¸ì™€ìš”!
        from visualize import visualize_graph
        
        # ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•´ì„œ HTML íŒŒì¼ì„ ìƒì„±í•´ìš”!
        output_path = visualize_graph(working_dir=engine.working_dir, output_file="graph_visualization.html")
        
        if output_path and os.path.exists(output_path):
            # FileResponseëŠ” "íŒŒì¼ì„ ë°˜í™˜í•˜ëŠ”" ê±°ì˜ˆìš”!
            # ë§ˆì¹˜ "ì´ HTML íŒŒì¼ì„ ë¸Œë¼ìš°ì €ë¡œ ë³´ì—¬ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
            return FileResponse(
                output_path,
                media_type="text/html",
                filename="graph_visualization.html"
            )
        else:
            raise HTTPException(status_code=500, detail="ê·¸ë˜í”„ ì‹œê°í™” íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ìš”!")
            
    except ImportError:
        raise HTTPException(status_code=500, detail="pyvis íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì–´ìš”! 'pip install pyvis'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”!")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [11] í…ìŠ¤íŠ¸ ì¸ë±ì‹± ì—”ë“œí¬ì¸íŠ¸ ---
# @app.post("/insert")ëŠ” "í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ì‹±í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
# ì¸ë±ì‹±ì€ í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”! (ì •í™•í•œ ê¸ˆìœµ ìˆ˜ì¹˜ ì¶”ì¶œì„ ìœ„í•´)
@app.post("/insert", 
          summary="í…ìŠ¤íŠ¸ ì¸ë±ì‹±",
          description="í…ìŠ¤íŠ¸ë¥¼ GraphRAGì— ì¸ë±ì‹±í•´ìš”. í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”!",
          response_description="ì¸ë±ì‹± ê²°ê³¼")
async def insert(request: InsertRequest):
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        # HTTPExceptionì€ "ì—ëŸ¬ë¥¼ ë˜ì§€ëŠ”" ê±°ì˜ˆìš”!
        # 503ì€ "ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    # text í•„ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if not request.text or not request.text.strip():
        raise HTTPException(
            status_code=422, 
            detail="'text' í•„ë“œëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ì–´ìš”! í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )
    
    try:
        # tryëŠ” "ì‹œë„í•´ë´"ë¼ëŠ” ëœ»ì´ì—ìš”!
        # engine.ainsert()ëŠ” ë¹„ë™ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë˜í”„ì— ë„£ëŠ” ê±°ì˜ˆìš”!
        # ì¸ë±ì‹±ì€ í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”!
        await engine.ainsert(request.text)
        
        # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
        return {
            "message": "í…ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì¸ë±ì‹±ë˜ì—ˆì–´ìš”! (OpenAI API ì‚¬ìš©)",
            "status": "success",
            "mode": "openai_api"
        }
    except Exception as e:
        # exceptëŠ” "ë§Œì•½ ì—ëŸ¬ê°€ ìƒê¸°ë©´"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        # Exceptionì€ "ëª¨ë“  ì¢…ë¥˜ì˜ ì—ëŸ¬"ì˜ˆìš”!
        # eëŠ” ì—ëŸ¬ ë‚´ìš©ì´ì—ìš”!
        # HTTPExceptionìœ¼ë¡œ ì—ëŸ¬ë¥¼ ë°˜í™˜í•´ìš”!
        import traceback
        error_detail = f"ì¸ë±ì‹± ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}\n\nìƒì„¸ ì •ë³´:\n{traceback.format_exc()}"
        print(f"âŒ ì¸ë±ì‹± ì—ëŸ¬:\n{error_detail}")
        raise HTTPException(status_code=500, detail=f"ì¸ë±ì‹± ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [12] ì§ˆë¬¸-ë‹µë³€ ì—”ë“œí¬ì¸íŠ¸ (Decision Layer í†µí•©) ---
# @app.post("/query")ëŠ” "ì§ˆë¬¸ì„ ë°›ì•„ì„œ ë‹µë³€ì„ ì£¼ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
# mode íŒŒë¼ë¯¸í„°ë¡œ "api" ë˜ëŠ” "local"ì„ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!
@app.post("/query",
          summary="ì§ˆë¬¸-ë‹µë³€",
          description="GraphRAGì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ì•„ìš”. modeë¡œ 'api' ë˜ëŠ” 'local'ì„ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!\n\n**ìš”ì²­ í˜•ì‹**:\n```json\n{\n  \"question\": \"ì§ˆë¬¸ ë‚´ìš©\",\n  \"mode\": \"local\"\n}\n```",
          response_description="ì§ˆë¬¸ê³¼ ë‹µë³€",
          responses={
              200: {
                  "description": "ì§ˆë¬¸ ì„±ê³µ",
                  "content": {
                      "application/json": {
                          "example": {
                              "question": "What is NVIDIA revenue?",
                              "answer": "NVIDIA's revenue is $57.0 billion in Q3 2026.",
                              "mode": "local",
                              "status": "success"
                          }
                      }
                  }
              },
              422: {
                  "description": "ìš”ì²­ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜",
                  "content": {
                      "application/json": {
                          "example": {
                              "detail": [
                                  {
                                      "type": "missing",
                                      "loc": ["body", "question"],
                                      "msg": "Field required",
                                      "input": {}
                                  }
                              ]
                          }
                      }
                  }
              }
          })
async def query(request: QueryRequest):
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    # question í•„ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if not request.question or not request.question.strip():
        raise HTTPException(
            status_code=422,
            detail="'question' í•„ë“œëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ì–´ìš”! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )
    
    # modeê°€ "api" ë˜ëŠ” "local"ì´ ì•„ë‹ˆë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if request.mode not in ["api", "local"]:
        raise HTTPException(
            status_code=400,
            detail="modeëŠ” 'api' ë˜ëŠ” 'local'ì´ì–´ì•¼ í•´ìš”! (í˜„ì¬ ê°’: '{}')".format(request.mode)
        )
    
    try:
        # #region agent log
        import json
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:325","message":"Query entry","data":{"question":request.question,"mode":request.mode,"enable_web_search":request.enable_web_search,"use_multi_agent":request.use_multi_agent},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H2,H5"})+'\n')
        # #endregion
        
        # --- Multi-Agent Mode ---
        if request.use_multi_agent:
            print(f"ğŸ¤– Multi-Agent ëª¨ë“œë¡œ ì²˜ë¦¬: '{request.question}'")
            
            from agents import MasterAgent
            from agents.agent_context import AgentContext, QueryComplexity
            
            # AgentContext ìƒì„±
            context = AgentContext(
                question=request.question,
                complexity=QueryComplexity.COMPLEX,  # ê¸°ë³¸ê°’, Masterê°€ ì¬ë¶„ì„
                enable_web_search=request.enable_web_search
            )
            
            # Master Agent ì‹¤í–‰ (MCP Manager ì „ë‹¬)
            master = MasterAgent(engine=engine, mcp_manager=mcp_manager)
            context = await master.execute(context)
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "question": request.question,
                "answer": context.final_report,
                "sources": context.sources,
                "confidence": context.confidence,
                "recommendation": context.recommendation,
                "insights": context.insights,
                "retrieval_backend": context.retrieval_backend,
                "processing_steps": context.processing_steps,
                "mode": "MULTI_AGENT",
                "status": "success"
            }
        
        # --- Decision Layer (Router) ---
        # ì›¹ ê²€ìƒ‰ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ ë¶„ë¥˜
        if request.enable_web_search:
            # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ë¥˜ (GRAPH_RAG vs WEB_SEARCH)
            print(f"ğŸ¤” ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ (ì›¹ ê²€ìƒ‰ í™œì„±í™”ë¨): '{request.question}'")
            query_type = await classify_query(request.question)
            print(f"âœ… ë¶„ë¥˜ ê²°ê³¼: {query_type}")
        else:
            # ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™” ì‹œ í•­ìƒ GraphRAG ì‚¬ìš©
            query_type = "GRAPH_RAG"
            print(f"ğŸ“š ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™” - ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤")
        
        # 2ë‹¨ê³„: ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ì²˜ë¦¬
        sources_list = []
        
        if query_type == "WEB_SEARCH":
            # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬
            print(f"ğŸŒ ì›¹ ê²€ìƒ‰ ëª¨ë“œë¡œ ì²˜ë¦¬")
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = await web_search(request.question, max_results=WEB_SEARCH_MAX_RESULTS)
            
            if search_results:
                # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                sources_list = [
                    {
                        "id": idx,
                        "file": result["title"],
                        "chunk_id": result["url"],
                        "excerpt": result["snippet"],
                        "url": result["url"]
                    }
                    for idx, result in enumerate(search_results, 1)
                ]
                
                # Report í˜•ì‹ í”„ë¡¬í”„íŠ¸ ìƒì„±
                report_prompt = get_web_search_report_prompt(request.question, search_results)
                
                # LLMìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„± (ì‚¬ìš©ì ì§€ì • temperature ì‚¬ìš©)
                client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
                llm_response = await client.chat.completions.create(
                    model=ROUTER_MODEL,
                    messages=[
                        {"role": "system", "content": report_prompt},
                        {"role": "user", "content": request.question}
                    ],
                    temperature=request.temperature,
                    max_tokens=2000
                )
                response = llm_response.choices[0].message.content.strip()
            else:
                response = "ì£„ì†¡í•´ìš”, ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
            
            source = "WEB_SEARCH"
        else:
            # GraphRAGë¡œ ì²˜ë¦¬ (ì¶œì²˜ ì •ë³´ í¬í•¨)
            print(f"ğŸ“š GraphRAG ëª¨ë“œë¡œ ì²˜ë¦¬ (mode: {request.mode}, search_type: {request.search_type}, temperature: {request.temperature}, top_k: {request.top_k})")
            retrieval_backend = "unknown"
            retrieval_context = ""
            
            # Global vs Local search ë¶„ê¸°
            if request.search_type == "global":
                # Global Search: ì „ì²´ ë¬¸ì„œ ìš”ì•½
                result = await engine.aglobal_search(
                    request.question,
                    top_k=request.top_k,
                    temperature=request.temperature
                )
                base_answer = result.get("answer", "")
                sources_list = result.get("sources", [])
                retrieval_backend = "community"
            else:
                # Local Search: íŠ¹ì • ì—”í‹°í‹° ê²€ìƒ‰
                result = await engine.aquery(
                    request.question,
                    mode=request.mode,
                    return_context=True,
                    top_k=request.top_k
                )
                
                if isinstance(result, dict):
                    base_answer = result.get("answer", "")
                    sources_list = result.get("sources", [])
                    retrieval_backend = result.get("retrieval_backend", "unknown")
                    retrieval_context = result.get("context", "") or ""
                else:
                    base_answer = result
                    sources_list = []
            
            # Strict Groundingìœ¼ë¡œ ë³´ê³ ì„œ ì¬ìƒì„±
            if sources_list:
                # #region agent log
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"app.py:567","message":"sources_list before grounding","data":{"count":len(sources_list),"sources":[{"id":s.get("id"),"file":s.get("file"),"excerpt":s.get("excerpt","")[:100]} for s in sources_list[:3]]},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H4"})+'\n')
                # #endregion
                # ì‹¤ì œ ì†ŒìŠ¤ ê°œìˆ˜ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì œí•œ
                max_sources = min(len(sources_list), 10)  # ìµœëŒ€ 10ê°œ
                sources_list = sources_list[:max_sources]
                
                # Strict Grounding Prompt ì‚¬ìš©
                from utils import get_strict_grounding_prompt
                strict_prompt = get_strict_grounding_prompt(request.question, sources_list)
                
                client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
                llm_response = await client.chat.completions.create(
                    model=ROUTER_MODEL,
                    messages=[
                        {"role": "system", "content": strict_prompt},
                        {"role": "user", "content": request.question}
                    ],
                    temperature=0.0,  # Strict grounding: ì°½ì˜ì„± ì œê±°
                    max_tokens=2000
                )
                response = llm_response.choices[0].message.content.strip()
                # #region agent log
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"app.py:584","message":"LLM response before validation","data":{"response_length":len(response),"response_preview":response[:300]},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H5"})+'\n')
                # #endregion
                
                # Self-Correction: Citation Validation
                from citation_validator import CitationValidator
                validator = CitationValidator(sources_list)
                validation_result = validator.validate_response(response)
                evidence = []
                # #region agent log
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"app.py:596","message":"validation result","data":{"confidence":validation_result.get('confidence_score'),"valid_citations":validation_result.get('valid_citations'),"total_citations":validation_result.get('total_citations'),"missing_citations":validation_result.get('missing_citations',[])},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H5"})+'\n')
                # #endregion
                
                print(f"[VALIDATION] Confidence: {validation_result['confidence_score']:.1%}")
                print(f"[VALIDATION] Valid citations: {validation_result['valid_citations']}/{validation_result['total_citations']}")

                # #region agent log
                import json
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(json.dumps({"location":"app.py:607","message":"Response before override check","data":{"response_preview":response[:500],"has_html_tags":"<a href" in response or "<div" in response,"sources_count":len(sources_list)},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H4"})+'\n')
                # #endregion
                
                # Strict Grounding LLMì´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ë‹µí–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ì†ŒìŠ¤ê°€ ì¶©ë¶„í•œ ê²½ìš° ë³´ì •
                override_applied = False
                if response.strip() == "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." and len(sources_list) > 0:
                    print("[WARNING] Strict grounding LLM returned 'no info' despite non-empty sources. Falling back to base GraphRAG answer.")
                    # #region agent log
                    with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                        f.write(__import__('json').dumps({
                            "location": "app.py:610",
                            "message": "override no-info with base_answer",
                            "data": {
                                "base_answer_preview": base_answer[:200] if base_answer else None,
                                "sources_count": len(sources_list)
                            },
                            "timestamp": __import__('time').time()*1000,
                            "sessionId": "debug-session",
                            "runId": "run1",
                            "hypothesisId": "H4,H5"
                        }) + '\n')
                    # #endregion
                    response = base_answer or response
                    # base_answerì—ëŠ” citationì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ validationì€ ìœ ì§€í•˜ë˜ ì‹ ë¢°ë„ëŠ” 0.7 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í›„ì† ì²´í¬ë¥¼ í†µê³¼ì‹œí‚´
                    validation_result = {"confidence_score": 0.75, "is_valid": True}
                    evidence = []
                    override_applied = True

                # ì‹ ë¢°ë„ê°€ ë‚®ê±°ë‚˜ ì‘ë‹µì´ ë¹„ì •ìƒì ì´ë©´ "ì •ë³´ ì—†ìŒ" ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
                # ë˜ëŠ” ì‘ë‹µì— HTML/ì›¹ ê²€ìƒ‰ í”ì ì´ ìˆìœ¼ë©´ ê±°ë¶€
                # ë‹¨, overrideê°€ ì ìš©ëœ ê²½ìš°ëŠ” ìŠ¤í‚µ (base_answerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ)
                if not override_applied and (validation_result["confidence_score"] < 0.7 or 
                    "<a href" in response or 
                    "Thesaurus.com" in response or
                    "WordHippo" in response or
                    len(response.strip()) < 50):
                    print(f"[WARNING] Low confidence or invalid response, replacing with 'no info' response")
                    response = "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    sources_list = []
                    validation_result = {"confidence_score": 0.0, "is_valid": False}
                    evidence = []
                else:
                    # ì‘ë‹µì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ëœ citation ë²ˆí˜¸ ì¶”ì¶œ ë° í•„í„°ë§
                    import re
                    citation_pattern = r'\[(\d+)\]'
                    used_citations = set()
                    for match in re.finditer(citation_pattern, response):
                        citation_num = int(match.group(1))
                        if 1 <= citation_num <= len(sources_list):
                            used_citations.add(citation_num)
                    
                    # ì‚¬ìš©ëœ citationì— í•´ë‹¹í•˜ëŠ” ì†ŒìŠ¤ë§Œ ìœ ì§€
                    if used_citations:
                        sources_list = [s for s in sources_list if s['id'] in used_citations]
                        # IDë¥¼ 1ë¶€í„° ë‹¤ì‹œ ë§¤í•‘
                        for idx, source in enumerate(sources_list, 1):
                            old_id = source['id']
                            source['id'] = idx
                            # ì‘ë‹µì—ì„œ citation ë²ˆí˜¸ ì¬ë§¤í•‘
                            response = response.replace(f'[{old_id}]', f'[{idx}]')
                            response = re.sub(rf'\[{old_id}\]', f'[{idx}]', response)

                    # evidence(í´ë ˆì„-ê·¼ê±°) êµ¬ì¡° ìƒì„± (citation remap ì´í›„)
                    # overrideê°€ ì ìš©ëœ ê²½ìš°ëŠ” evidenceë¥¼ ë¹ˆ ë°°ì—´ë¡œ ìœ ì§€ (base_answerì—ëŠ” citationì´ ì—†ìŒ)
                    if not override_applied:
                        validator = CitationValidator(sources_list)
                        evidence = validator.build_evidence(response)
                    # override_appliedì¸ ê²½ìš° evidenceëŠ” ì´ë¯¸ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •ë¨
            else:
                # ì¶œì²˜ê°€ ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ" ì‘ë‹µ
                response = "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                validation_result = {"confidence_score": 0.0, "is_valid": False}
                evidence = []
            
            source = "GRAPH_RAG"
        
        # #region agent log
        import json
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:678","message":"Final response before return","data":{"response_preview":response[:500],"has_html_tags":"<a href" in response or "<div" in response,"sources_count":len(sources_list),"validation_confidence":validation_result.get('confidence_score',0) if 'validation_result' in locals() else None},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H4"})+'\n')
        # #endregion
        
        # #region agent log
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:338","message":"Query response","data":{"response":response[:500] if response else None,"response_type":type(response).__name__,"source":source},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H3"})+'\n')
        # #endregion
        
        # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
        return {
            "question": request.question,
            "answer": response,
            "sources": sources_list,  # Citationìš© ì¶œì²˜ ë¦¬ìŠ¤íŠ¸
            "source": source,  # ì–´ë””ì„œ ë‹µë³€ì„ ê°€ì ¸ì™”ëŠ”ì§€ ì•Œë ¤ì¤˜ìš”!
            "mode": request.mode if source == "GRAPH_RAG" else "N/A",  # GraphRAGì¼ ë•Œë§Œ ì˜ë¯¸ ìˆì–´ìš”
            "search_type": request.search_type if source == "GRAPH_RAG" else "N/A",
            "validation": validation_result if source == "GRAPH_RAG" and 'validation_result' in locals() else None,
            "evidence": evidence if source == "GRAPH_RAG" and 'evidence' in locals() else [],
            "retrieval_backend": retrieval_backend if source == "GRAPH_RAG" and 'retrieval_backend' in locals() else "N/A",
            "retrieval_context": retrieval_context if source == "GRAPH_RAG" and 'retrieval_context' in locals() else "",
            "status": "success"
        }
    except Exception as e:
        # #region agent log
        import traceback
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:352","message":"Query error","data":{"error":str(e),"error_type":type(e).__name__,"traceback":traceback.format_exc()},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H4"})+'\n')
        # #endregion
        # exceptëŠ” "ë§Œì•½ ì—ëŸ¬ê°€ ìƒê¸°ë©´"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")


# --- [12] ë„ë©”ì¸ íŠ¹í™” ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/domain/event/{event_name}")
async def get_event_impact(event_name: str):
    """
    Eventì˜ ì¸ê³¼ê´€ê³„ ì²´ì¸ ì¡°íšŒ
    
    Args:
        event_name: Event ì´ë¦„
    
    Returns:
        Event â†’ Factor â†’ Asset ì¸ê³¼ê´€ê³„ ì²´ì¸
    """
    try:
        from engine.executor import QueryExecutor
        from engine.neo4j_retriever import query_event_impact_chain
        
        executor = QueryExecutor()
        try:
            impact_chain = query_event_impact_chain(executor, event_name)
            
            return {
                "event": event_name,
                "impact_chain": impact_chain,
                "count": len(impact_chain),
                "status": "success"
            }
        finally:
            executor.close()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Event impact chain ì¡°íšŒ ì¤‘ ì—ëŸ¬: {str(e)}")


@app.get("/domain/actor/{actor_name}")
async def get_actor_influence(actor_name: str):
    """
    Actorì˜ ì˜í–¥ë ¥ ì¡°íšŒ
    
    Args:
        actor_name: Actor ì´ë¦„
    
    Returns:
        Actorê°€ ê´€ì—¬í•œ Eventì™€ ê·¸ ì˜í–¥
    """
    try:
        from engine.executor import QueryExecutor
        from engine.neo4j_retriever import query_actor_influence
        
        executor = QueryExecutor()
        try:
            influence_data = query_actor_influence(executor, actor_name)
            
            return {
                "actor": actor_name,
                "influence": influence_data,
                "count": len(influence_data),
                "status": "success"
            }
        finally:
            executor.close()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Actor influence ì¡°íšŒ ì¤‘ ì—ëŸ¬: {str(e)}")


@app.get("/domain/region/{region_name}")
async def get_regional_events(region_name: str):
    """
    ì§€ì—­ë³„ Event ì¡°íšŒ
    
    Args:
        region_name: Region ì´ë¦„
    
    Returns:
        íŠ¹ì • ì§€ì—­ì˜ Eventì™€ ì˜í–¥ë°›ì€ Asset
    """
    try:
        from engine.executor import QueryExecutor
        from engine.neo4j_retriever import query_regional_events
        
        executor = QueryExecutor()
        try:
            regional_events = query_regional_events(executor, region_name)
            
            return {
                "region": region_name,
                "events": regional_events,
                "count": len(regional_events),
                "status": "success"
            }
        finally:
            executor.close()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Regional events ì¡°íšŒ ì¤‘ ì—ëŸ¬: {str(e)}")


@app.get("/domain/asset/{asset_name}")
async def get_asset_factors(asset_name: str):
    """
    Assetì— ì˜í–¥ì„ ì£¼ëŠ” Factorë“¤ ì¡°íšŒ
    
    Args:
        asset_name: Asset ì´ë¦„
    
    Returns:
        Assetì— ì˜í–¥ì„ ì£¼ëŠ” Factor ë¦¬ìŠ¤íŠ¸
    """
    try:
        from engine.executor import QueryExecutor
        from engine.neo4j_retriever import query_asset_factors
        
        executor = QueryExecutor()
        try:
            factors = query_asset_factors(executor, asset_name)
            
            return {
                "asset": asset_name,
                "factors": factors,
                "count": len(factors),
                "status": "success"
            }
        finally:
            executor.close()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Asset factors ì¡°íšŒ ì¤‘ ì—ëŸ¬: {str(e)}")


@app.post("/domain/schema/init")
async def initialize_domain_schema():
    """
    ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” (Constraint ë° Index ìƒì„±)
    
    Returns:
        ì´ˆê¸°í™” ê²°ê³¼
    """
    try:
        from db.neo4j_db import Neo4jDatabase
        
        db = Neo4jDatabase()
        result = db.create_domain_schema()
        
        return result
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬: {str(e)}")


# --- [13] ì„œë²„ ì‹¤í–‰ ---
# if __name__ == "__main__": ì´ê±´ "ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
if __name__ == "__main__":
    # uvicorn.run()ì€ "ì„œë²„ë¥¼ ì‹¤í–‰í•˜ëŠ”" ê±°ì˜ˆìš”!
    # appì€ "FastAPI ì•±"ì´ì—ìš”!
    # host="0.0.0.0"ì€ "ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ì† ê°€ëŠ¥"í•˜ë‹¤ëŠ” ëœ»ì´ì—ìš”!
    # port=8000ì€ "8000ë²ˆ í¬íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤"ëŠ” ëœ»ì´ì—ìš”!
    uvicorn.run(app, host="0.0.0.0", port=8000)

