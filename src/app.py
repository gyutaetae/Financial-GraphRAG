# app.pyëŠ” "FastAPI ì„œë²„"ë¥¼ ë§Œë“œëŠ” íŒŒì¼ì´ì—ìš”!
# ë§ˆì¹˜ "ì›¹ ì„œë²„ë¥¼ ë§Œë“œëŠ” ë„êµ¬ ìƒì" ê°™ì€ ê±°ì˜ˆìš”!

from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
import uvicorn
import os
import sys

# src ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€í•´ìš”!
# ì´ë ‡ê²Œ í•˜ë©´ 'from engine import ...' ê°™ì€ importê°€ ì‘ë™í•´ìš”!
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# engine ëª¨ë“ˆì—ì„œ HybridGraphRAGEngineì„ ê°€ì ¸ì™€ìš”!
from engine import HybridGraphRAGEngine
from config import print_config, validate_config, ROUTER_MODEL, ROUTER_TEMPERATURE, WEB_SEARCH_MAX_RESULTS, OPENAI_API_KEY, OPENAI_BASE_URL
from search import web_search, format_search_results
from openai import AsyncOpenAI
from utils import get_executive_report_prompt, get_web_search_report_prompt

# --- [1] ì „ì—­ ë³€ìˆ˜ ---
# engineì€ "GraphRAG ì—”ì§„"ì´ì—ìš”!
# Noneì€ "ì•„ì§ ì•„ë¬´ê²ƒë„ ì—†ë‹¤"ëŠ” ëœ»ì´ì—ìš”!
engine: HybridGraphRAGEngine = None

# --- [2] ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
# @asynccontextmanagerëŠ” "ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"ë¥¼ ë§Œë“œëŠ” ê±°ì˜ˆìš”!
# ë§ˆì¹˜ "ì„œë²„ê°€ ì‹œì‘ë  ë•Œì™€ ëë‚  ë•Œ ë­”ê°€ë¥¼ í•˜ëŠ”" ê²ƒì²˜ëŸ¼!
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ì´ì—ìš”!
    global engine
    
    # ì„¤ì • ì •ë³´ë¥¼ ì¶œë ¥í•´ìš”!
    print_config()
    
    # validate_config()ëŠ” "ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ëŠ”" í•¨ìˆ˜ì˜ˆìš”!
    validate_config()
    
    # HybridGraphRAGEngineì„ ì´ˆê¸°í™”í•˜ëŠ” ê±°ì˜ˆìš”!
    # ë§ˆì¹˜ "GraphRAG ì—”ì§„ì„ ì¤€ë¹„í•˜ëŠ”" ê²ƒì²˜ëŸ¼!
    print("ğŸš€ HybridGraphRAGEngine ì´ˆê¸°í™” ì¤‘...")
    engine = HybridGraphRAGEngine()
    print("âœ… HybridGraphRAGEngine ì¤€ë¹„ ì™„ë£Œ!")
    
    # yieldëŠ” "ì—¬ê¸°ì„œ ì ì‹œ ë©ˆì¶°ì„œ ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê³ , ë‚˜ì¤‘ì— ë‹¤ì‹œ ëŒì•„ì™€"ë¼ëŠ” ëœ»ì´ì—ìš”!
    yield
    
    # ì„œë²„ê°€ ì¢…ë£Œë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ì´ì—ìš”! (í˜„ì¬ëŠ” ë¹„ì–´ìˆì–´ìš”)
    pass

# --- [3] FastAPI ì•± ì´ˆê¸°í™” ---
# FastAPI()ëŠ” "ì›¹ ì„œë²„ ì•±ì„ ë§Œë“¤ì–´ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
# lifespanì€ "ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"ì˜ˆìš”!
app = FastAPI(
    title="VIK AI: Hybrid GraphRAG API",
    description="ê¸ˆìœµ ë¶„ì„ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ GraphRAG APIì˜ˆìš”! ì¸ë±ì‹±ì€ OpenAI API, ì§ˆë¬¸ì€ API/LOCAL ì„ íƒ ê°€ëŠ¥í•´ìš”!",
    version="2.0.0",
    lifespan=lifespan  # lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ì—°ê²°í•´ìš”!
)

# --- [4] Pydantic ëª¨ë¸ ---
# Pydantic ëª¨ë¸ì€ "ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒ"ì´ì—ìš”!
# ë§ˆì¹˜ "ì´ëŸ° ëª¨ì–‘ì˜ ë°ì´í„°ë¥¼ ë°›ì„ê²Œìš”!"ë¼ê³  ë¯¸ë¦¬ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”!

# QueryRequestëŠ” "ì§ˆë¬¸ ìš”ì²­"ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì´ì—ìš”!
class QueryRequest(BaseModel):
    # questionì€ "ì§ˆë¬¸ ë‚´ìš©"ì´ì—ìš”!
    question: str
    # modeëŠ” "ì–´ë–¤ ëª¨ë“œë¥¼ ì‚¬ìš©í• ì§€" ì •í•˜ëŠ” ê±°ì˜ˆìš”. "api" ë˜ëŠ” "local"!
    # ê¸°ë³¸ê°’ì€ "local"ì´ì—ìš”!
    mode: str = "local"
    # temperatureëŠ” "ì‘ë‹µì˜ ì°½ì˜ì„±"ì„ ì¡°ì ˆí•´ìš”! (0.0 = ì •í™•, 2.0 = ì°½ì˜ì )
    temperature: float = 0.2
    # top_këŠ” "ê²€ìƒ‰í•  ì²­í¬ ê°œìˆ˜"ë¥¼ ì •í•´ìš”!
    top_k: int = 30
    
    # Pydantic v2 ìŠ¤íƒ€ì¼ë¡œ ì˜ˆì‹œ ì„¤ì •
    model_config = {
        "json_schema_extra": {
            "example": {
                "question": "What is NVIDIA revenue?",
                "mode": "local"
            }
        }
    }

# InsertRequestëŠ” "í…ìŠ¤íŠ¸ ì¶”ê°€ ìš”ì²­"ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì´ì—ìš”!
class InsertRequest(BaseModel):
    # textëŠ” "ì¶”ê°€í•  í…ìŠ¤íŠ¸"ì˜ˆìš”!
    text: str
    
    # Pydantic v2 ìŠ¤íƒ€ì¼ë¡œ ì„¤ì • (deprecation warning í•´ê²°)
    model_config = {
        "json_schema_extra": {
            "example": {
                "text": "NVIDIA reported record revenue of $57.0 billion in Q3 2026."
            }
        }
    }

# --- [5] Router í•¨ìˆ˜ë“¤ (Decision Layer) ---
# ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì›¹ ê²€ìƒ‰ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë“¤ì´ì—ìš”!

async def classify_query(question: str) -> str:
    """
    GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” Router í•¨ìˆ˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        "GRAPH_RAG" ë˜ëŠ” "WEB_SEARCH"
    """
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        # ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """You are a query classifier for a financial AI system.

Your task is to classify user questions into two categories:

1. GRAPH_RAG: Questions about uploaded PDF documents, company financials from internal reports, historical data that was indexed
   Examples:
   - "What is NVIDIA's Q3 revenue?"
   - "Summarize the uploaded report"
   - "What are the key findings in the document?"
   - "Show me the financial metrics"

2. WEB_SEARCH: Questions requiring latest market data, real-time information, news, or information not in uploaded documents
   Examples:
   - "What is today's stock price?"
   - "Latest news about Tesla"
   - "Current inflation rate"
   - "What happened in the market today?"

Respond with ONLY ONE WORD: Either "GRAPH_RAG" or "WEB_SEARCH" - nothing else."""

        # GPT-4o-mini í˜¸ì¶œ
        response = await client.chat.completions.create(
            model=ROUTER_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Classify this question: {question}"}
            ],
            temperature=ROUTER_TEMPERATURE,
            max_tokens=10
        )
        
        # ì‘ë‹µ ì¶”ì¶œ ë° ì •ê·œí™”
        classification = response.choices[0].message.content.strip().upper()
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if "GRAPH_RAG" in classification:
            return "GRAPH_RAG"
        elif "WEB_SEARCH" in classification or "WEB" in classification:
            return "WEB_SEARCH"
        else:
            # ê¸°ë³¸ê°’: GRAPH_RAG (ë‚´ë¶€ ë¬¸ì„œ ìš°ì„ )
            print(f"âš ï¸ ë¶„ë¥˜ ê²°ê³¼ê°€ ëª…í™•í•˜ì§€ ì•Šì•„ìš”: {classification}, ê¸°ë³¸ê°’ GRAPH_RAG ì‚¬ìš©")
            return "GRAPH_RAG"
    
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’: GRAPH_RAG
        return "GRAPH_RAG"


async def handle_web_search(question: str) -> str:
    """
    ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€
    """
    try:
        # 1. DuckDuckGoë¡œ ì›¹ ê²€ìƒ‰
        print(f"ğŸ” ì›¹ ê²€ìƒ‰ ì‹œì‘: {question}")
        search_results = await web_search(question, max_results=WEB_SEARCH_MAX_RESULTS)
        
        if not search_results:
            return "ì£„ì†¡í•´ìš”, ê´€ë ¨ëœ ìµœì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
        
        # 2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·
        formatted_results = await format_search_results(search_results)
        
        # 3. GPT-4o-minië¡œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        synthesis_prompt = f"""Based on the following web search results, answer the user's question comprehensively.
Include relevant data and cite sources with URLs when possible.

User Question: {question}

Search Results:
{formatted_results}

Provide a clear, concise answer with sources."""

        response = await client.chat.completions.create(
            model=ROUTER_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful financial assistant that synthesizes web search results into clear answers."},
                {"role": "user", "content": synthesis_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content.strip()
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        sources = "\n\nğŸ“š ì¶œì²˜:\n"
        for idx, result in enumerate(search_results[:3], 1):
            sources += f"{idx}. {result['title']}\n   {result['url']}\n"
        
        return answer + sources
    
    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}"


# --- [6] ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/")ëŠ” "ë£¨íŠ¸ ê²½ë¡œ(/)ì— GET ìš”ì²­ì´ ì˜¤ë©´" ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
# ë§ˆì¹˜ "í™ˆí˜ì´ì§€ì— ì ‘ì†í•˜ë©´" ì‹¤í–‰ë˜ëŠ” ê±°ì˜ˆìš”!
@app.get("/")
async def root():
    # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
    return {
        "message": "VIK AI Hybrid GraphRAG APIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•´ìš”!",
        "description": "ì¸ë±ì‹±ì€ OpenAI API(gpt-5-mini)ë¥¼ ì‚¬ìš©í•˜ê³ , ì§ˆë¬¸ì€ API/LOCAL ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!",
        "endpoints": {
            "/insert": "í…ìŠ¤íŠ¸ ì¸ë±ì‹±í•˜ê¸° (POST) - OpenAI API ì‚¬ìš©",
            "/query": "ì§ˆë¬¸í•˜ê¸° (POST) - mode íŒŒë¼ë¯¸í„°ë¡œ 'api' ë˜ëŠ” 'local' ì„ íƒ",
            "/health": "ì„œë²„ ìƒíƒœ í™•ì¸ (GET)",
            "/graph_stats": "ê·¸ë˜í”„ í˜„í™© í™•ì¸ (GET)",
            "/visualize": "ê·¸ë˜í”„ ì‹œê°í™” HTML ìƒì„± (GET)",
            "/docs": "API ë¬¸ì„œ ë³´ê¸° (GET)"
        },
        "usage": {
            "insert": {
                "method": "POST",
                "url": "/insert",
                "body": {"text": "ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸"}
            },
            "query": {
                "method": "POST",
                "url": "/query",
                "body": {
                    "question": "ì§ˆë¬¸ ë‚´ìš©",
                    "mode": "api ë˜ëŠ” local (ê¸°ë³¸ê°’: local)"
                }
            }
        }
    }

# --- [7] ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/health")ëŠ” "ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/health")
async def health():
    # ì„œë²„ê°€ ì˜ ì‘ë™í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”!
    return {
        "status": "healthy",
        "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì´ì—ìš”!",
        "engine_ready": engine is not None
    }

# --- [8] ê·¸ë˜í”„ í†µê³„ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/graph_stats")ëŠ” "ê·¸ë˜í”„ í†µê³„ë¥¼ ë³´ì—¬ì£¼ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/graph_stats")
async def graph_stats():
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        return {"nodes": 0, "edges": 0, "message": "ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!"}
    
    # engine.get_graph_stats()ëŠ” ê·¸ë˜í”„ í†µê³„ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê±°ì˜ˆìš”!
    return engine.get_graph_stats()

# --- [9] ê·¸ë˜í”„ ì´ˆê¸°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# @app.post("/reset")ëŠ” "ê·¸ë˜í”„ë¥¼ ì´ˆê¸°í™”í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.post("/reset",
          summary="ê·¸ë˜í”„ ì´ˆê¸°í™”",
          description="ê¸°ì¡´ ê·¸ë˜í”„ ìŠ¤í† ë¦¬ì§€ë¥¼ ë°±ì—…í•˜ê³  ì‚­ì œí•œ í›„ ìƒˆë¡œìš´ ê·¸ë˜í”„ë¡œ ì‹œì‘í•´ìš”!")
async def reset_graph():
    global engine
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    try:
        import shutil
        from datetime import datetime
        
        # ë°±ì—… í´ë” ì´ë¦„ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        backup_dir = f"{engine.working_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ê¸°ì¡´ ê·¸ë˜í”„ ìŠ¤í† ë¦¬ì§€ê°€ ìˆìœ¼ë©´ ë°±ì—…
        if os.path.exists(engine.working_dir):
            shutil.move(engine.working_dir, backup_dir)
            print(f"âœ… ê¸°ì¡´ ê·¸ë˜í”„ ë°±ì—… ì™„ë£Œ: {backup_dir}")
        
        # ì—”ì§„ ì¬ì´ˆê¸°í™”
        engine = HybridGraphRAGEngine()
        
        return {
            "message": "ê·¸ë˜í”„ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆì–´ìš”!",
            "status": "success",
            "backup_dir": backup_dir if os.path.exists(backup_dir) else None
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê·¸ë˜í”„ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [10] ê·¸ë˜í”„ ì‹œê°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/visualize")ëŠ” "ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ëŠ” HTML íŒŒì¼ì„ ìƒì„±í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/visualize",
         summary="ê·¸ë˜í”„ ì‹œê°í™”",
         description="GraphRAG ê·¸ë˜í”„ë¥¼ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ ì‹œê°í™”í•œ HTML íŒŒì¼ì„ ìƒì„±í•´ìš”!")
async def visualize():
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    try:
        # visualize.pyì—ì„œ visualize_graph í•¨ìˆ˜ë¥¼ ê°€ì ¸ì™€ìš”!
        from visualize import visualize_graph
        
        # ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•´ì„œ HTML íŒŒì¼ì„ ìƒì„±í•´ìš”!
        output_path = visualize_graph(working_dir=engine.working_dir, output_file="graph_visualization.html")
        
        if output_path and os.path.exists(output_path):
            # FileResponseëŠ” "íŒŒì¼ì„ ë°˜í™˜í•˜ëŠ”" ê±°ì˜ˆìš”!
            # ë§ˆì¹˜ "ì´ HTML íŒŒì¼ì„ ë¸Œë¼ìš°ì €ë¡œ ë³´ì—¬ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
            return FileResponse(
                output_path,
                media_type="text/html",
                filename="graph_visualization.html"
            )
        else:
            raise HTTPException(status_code=500, detail="ê·¸ë˜í”„ ì‹œê°í™” íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ìš”!")
            
    except ImportError:
        raise HTTPException(status_code=500, detail="pyvis íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì–´ìš”! 'pip install pyvis'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”!")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [11] í…ìŠ¤íŠ¸ ì¸ë±ì‹± ì—”ë“œí¬ì¸íŠ¸ ---
# @app.post("/insert")ëŠ” "í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ì‹±í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
# ì¸ë±ì‹±ì€ í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”! (ì •í™•í•œ ê¸ˆìœµ ìˆ˜ì¹˜ ì¶”ì¶œì„ ìœ„í•´)
@app.post("/insert", 
          summary="í…ìŠ¤íŠ¸ ì¸ë±ì‹±",
          description="í…ìŠ¤íŠ¸ë¥¼ GraphRAGì— ì¸ë±ì‹±í•´ìš”. í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”!",
          response_description="ì¸ë±ì‹± ê²°ê³¼")
async def insert(request: InsertRequest):
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        # HTTPExceptionì€ "ì—ëŸ¬ë¥¼ ë˜ì§€ëŠ”" ê±°ì˜ˆìš”!
        # 503ì€ "ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    # text í•„ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if not request.text or not request.text.strip():
        raise HTTPException(
            status_code=422, 
            detail="'text' í•„ë“œëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ì–´ìš”! í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )
    
    try:
        # tryëŠ” "ì‹œë„í•´ë´"ë¼ëŠ” ëœ»ì´ì—ìš”!
        # engine.ainsert()ëŠ” ë¹„ë™ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë˜í”„ì— ë„£ëŠ” ê±°ì˜ˆìš”!
        # ì¸ë±ì‹±ì€ í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”!
        await engine.ainsert(request.text)
        
        # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
        return {
            "message": "í…ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì¸ë±ì‹±ë˜ì—ˆì–´ìš”! (OpenAI API ì‚¬ìš©)",
            "status": "success",
            "mode": "openai_api"
        }
    except Exception as e:
        # exceptëŠ” "ë§Œì•½ ì—ëŸ¬ê°€ ìƒê¸°ë©´"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        # Exceptionì€ "ëª¨ë“  ì¢…ë¥˜ì˜ ì—ëŸ¬"ì˜ˆìš”!
        # eëŠ” ì—ëŸ¬ ë‚´ìš©ì´ì—ìš”!
        # HTTPExceptionìœ¼ë¡œ ì—ëŸ¬ë¥¼ ë°˜í™˜í•´ìš”!
        import traceback
        error_detail = f"ì¸ë±ì‹± ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}\n\nìƒì„¸ ì •ë³´:\n{traceback.format_exc()}"
        print(f"âŒ ì¸ë±ì‹± ì—ëŸ¬:\n{error_detail}")
        raise HTTPException(status_code=500, detail=f"ì¸ë±ì‹± ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [12] ì§ˆë¬¸-ë‹µë³€ ì—”ë“œí¬ì¸íŠ¸ (Decision Layer í†µí•©) ---
# @app.post("/query")ëŠ” "ì§ˆë¬¸ì„ ë°›ì•„ì„œ ë‹µë³€ì„ ì£¼ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
# mode íŒŒë¼ë¯¸í„°ë¡œ "api" ë˜ëŠ” "local"ì„ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!
@app.post("/query",
          summary="ì§ˆë¬¸-ë‹µë³€",
          description="GraphRAGì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ì•„ìš”. modeë¡œ 'api' ë˜ëŠ” 'local'ì„ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!\n\n**ìš”ì²­ í˜•ì‹**:\n```json\n{\n  \"question\": \"ì§ˆë¬¸ ë‚´ìš©\",\n  \"mode\": \"local\"\n}\n```",
          response_description="ì§ˆë¬¸ê³¼ ë‹µë³€",
          responses={
              200: {
                  "description": "ì§ˆë¬¸ ì„±ê³µ",
                  "content": {
                      "application/json": {
                          "example": {
                              "question": "What is NVIDIA revenue?",
                              "answer": "NVIDIA's revenue is $57.0 billion in Q3 2026.",
                              "mode": "local",
                              "status": "success"
                          }
                      }
                  }
              },
              422: {
                  "description": "ìš”ì²­ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜",
                  "content": {
                      "application/json": {
                          "example": {
                              "detail": [
                                  {
                                      "type": "missing",
                                      "loc": ["body", "question"],
                                      "msg": "Field required",
                                      "input": {}
                                  }
                              ]
                          }
                      }
                  }
              }
          })
async def query(request: QueryRequest):
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    # question í•„ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if not request.question or not request.question.strip():
        raise HTTPException(
            status_code=422,
            detail="'question' í•„ë“œëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ì–´ìš”! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )
    
    # modeê°€ "api" ë˜ëŠ” "local"ì´ ì•„ë‹ˆë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if request.mode not in ["api", "local"]:
        raise HTTPException(
            status_code=400,
            detail="modeëŠ” 'api' ë˜ëŠ” 'local'ì´ì–´ì•¼ í•´ìš”! (í˜„ì¬ ê°’: '{}')".format(request.mode)
        )
    
    try:
        # #region agent log
        import json
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:325","message":"Query entry","data":{"question":request.question,"mode":request.mode},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H2,H5"})+'\n')
        # #endregion
        
        # --- Decision Layer (Router) ---
        # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ë¥˜ (GRAPH_RAG vs WEB_SEARCH)
        print(f"ğŸ¤” ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘: '{request.question}'")
        query_type = await classify_query(request.question)
        print(f"âœ… ë¶„ë¥˜ ê²°ê³¼: {query_type}")
        
        # 2ë‹¨ê³„: ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ì²˜ë¦¬
        sources_list = []
        
        if query_type == "WEB_SEARCH":
            # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬
            print(f"ğŸŒ ì›¹ ê²€ìƒ‰ ëª¨ë“œë¡œ ì²˜ë¦¬")
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = await web_search(request.question, max_results=WEB_SEARCH_MAX_RESULTS)
            
            if search_results:
                # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                sources_list = [
                    {
                        "id": idx,
                        "file": result["title"],
                        "chunk_id": result["url"],
                        "excerpt": result["snippet"],
                        "url": result["url"]
                    }
                    for idx, result in enumerate(search_results, 1)
                ]
                
                # Report í˜•ì‹ í”„ë¡¬í”„íŠ¸ ìƒì„±
                report_prompt = get_web_search_report_prompt(request.question, search_results)
                
                # LLMìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„± (ì‚¬ìš©ì ì§€ì • temperature ì‚¬ìš©)
                client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
                llm_response = await client.chat.completions.create(
                    model=ROUTER_MODEL,
                    messages=[
                        {"role": "system", "content": report_prompt},
                        {"role": "user", "content": request.question}
                    ],
                    temperature=request.temperature,
                    max_tokens=2000
                )
                response = llm_response.choices[0].message.content.strip()
            else:
                response = "ì£„ì†¡í•´ìš”, ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
            
            source = "WEB_SEARCH"
        else:
            # GraphRAGë¡œ ì²˜ë¦¬ (ì¶œì²˜ ì •ë³´ í¬í•¨)
            print(f"ğŸ“š GraphRAG ëª¨ë“œë¡œ ì²˜ë¦¬ (mode: {request.mode}, temperature: {request.temperature}, top_k: {request.top_k})")
            
            # return_context=Trueë¡œ í˜¸ì¶œí•˜ì—¬ ì¶œì²˜ ì •ë³´ ë°›ê¸° (ì‚¬ìš©ì ì§€ì • top_k ì „ë‹¬)
            result = await engine.aquery(
                request.question,
                mode=request.mode,
                return_context=True,
                top_k=request.top_k
            )
            
            if isinstance(result, dict):
                # ì¶œì²˜ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°
                base_answer = result.get("answer", "")
                sources_list = result.get("sources", [])
                
                # Executive Report í˜•ì‹ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ìƒì„±
                if sources_list:
                    # ì‹¤ì œ ì†ŒìŠ¤ ê°œìˆ˜ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì œí•œ
                    max_sources = min(len(sources_list), 10)  # ìµœëŒ€ 10ê°œ
                    sources_list = sources_list[:max_sources]
                    
                    report_prompt = get_executive_report_prompt(request.question, sources_list)
                    
                    client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
                    llm_response = await client.chat.completions.create(
                        model=ROUTER_MODEL,
                        messages=[
                            {"role": "system", "content": report_prompt},
                            {"role": "user", "content": f"Based on the sources provided, answer: {request.question}\n\nOriginal analysis: {base_answer}"}
                        ],
                        temperature=request.temperature,
                        max_tokens=2000
                    )
                    response = llm_response.choices[0].message.content.strip()
                    
                    # ì‘ë‹µì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ëœ citation ë²ˆí˜¸ ì¶”ì¶œ ë° í•„í„°ë§
                    import re
                    citation_pattern = r'\[(\d+)\]'
                    used_citations = set()
                    for match in re.finditer(citation_pattern, response):
                        citation_num = int(match.group(1))
                        if 1 <= citation_num <= len(sources_list):
                            used_citations.add(citation_num)
                    
                    # ì‚¬ìš©ëœ citationì— í•´ë‹¹í•˜ëŠ” ì†ŒìŠ¤ë§Œ ìœ ì§€
                    if used_citations:
                        sources_list = [s for s in sources_list if s['id'] in used_citations]
                        # IDë¥¼ 1ë¶€í„° ë‹¤ì‹œ ë§¤í•‘
                        for idx, source in enumerate(sources_list, 1):
                            old_id = source['id']
                            source['id'] = idx
                            # ì‘ë‹µì—ì„œ citation ë²ˆí˜¸ ì¬ë§¤í•‘
                            response = response.replace(f'[{old_id}]', f'[{idx}]')
                            # ì—¬ëŸ¬ ë²ˆí˜¸ê°€ í•¨ê»˜ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬ (ì˜ˆ: [1][2] -> [1][2])
                            response = re.sub(rf'\[{old_id}\]', f'[{idx}]', response)
                else:
                    # ì¶œì²˜ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë‹µë³€ ì‚¬ìš©
                    response = base_answer
            else:
                # í•˜ìœ„ í˜¸í™˜ì„±: ë¬¸ìì—´ ì‘ë‹µì¸ ê²½ìš°
                response = result
            
            source = "GRAPH_RAG"
        
        # #region agent log
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:338","message":"Query response","data":{"response":response[:500] if response else None,"response_type":type(response).__name__,"source":source},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H3"})+'\n')
        # #endregion
        
        # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
        return {
            "question": request.question,
            "answer": response,
            "sources": sources_list,  # Citationìš© ì¶œì²˜ ë¦¬ìŠ¤íŠ¸
            "source": source,  # ì–´ë””ì„œ ë‹µë³€ì„ ê°€ì ¸ì™”ëŠ”ì§€ ì•Œë ¤ì¤˜ìš”!
            "mode": request.mode if source == "GRAPH_RAG" else "N/A",  # GraphRAGì¼ ë•Œë§Œ ì˜ë¯¸ ìˆì–´ìš”
            "status": "success"
        }
    except Exception as e:
        # #region agent log
        import traceback
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:352","message":"Query error","data":{"error":str(e),"error_type":type(e).__name__,"traceback":traceback.format_exc()},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H4"})+'\n')
        # #endregion
        # exceptëŠ” "ë§Œì•½ ì—ëŸ¬ê°€ ìƒê¸°ë©´"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [13] ì„œë²„ ì‹¤í–‰ ---
# if __name__ == "__main__": ì´ê±´ "ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
if __name__ == "__main__":
    # uvicorn.run()ì€ "ì„œë²„ë¥¼ ì‹¤í–‰í•˜ëŠ”" ê±°ì˜ˆìš”!
    # appì€ "FastAPI ì•±"ì´ì—ìš”!
    # host="0.0.0.0"ì€ "ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ì† ê°€ëŠ¥"í•˜ë‹¤ëŠ” ëœ»ì´ì—ìš”!
    # port=8000ì€ "8000ë²ˆ í¬íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤"ëŠ” ëœ»ì´ì—ìš”!
    uvicorn.run(app, host="0.0.0.0", port=8000)

