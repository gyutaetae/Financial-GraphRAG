import streamlit as st
import requests
import sys
import os
import json
import time
import re
from typing import List, Dict

# .env íŒŒì¼ ì½ê¸°
from dotenv import load_dotenv
load_dotenv()

# Health Check ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from health_check import HealthChecker
    HEALTH_CHECK_AVAILABLE = True
except ImportError:
    HEALTH_CHECK_AVAILABLE = False
    HealthChecker = None

# í™˜ê²½ ë³€ìˆ˜ ì½ê¸° (í•˜ì´ë¸Œë¦¬ë“œ í´ë¼ìš°ë“œ ì§€ì›)
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")

# Neo4j ì—°ê²° ì²´í¬ í•¨ìˆ˜ (ë ˆê±°ì‹œ í˜¸í™˜)
def check_neo4j_connection():
    """Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
    if HEALTH_CHECK_AVAILABLE:
        checker = HealthChecker()
        return checker.check_neo4j()
    
    # Fallback: ê¸°ë³¸ ì²´í¬
    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
        driver.verify_connectivity()
        driver.close()
        return True, "âœ… Neo4j Connected"
    except Exception as e:
        error_msg = str(e)
        if "authentication" in error_msg.lower():
            return False, "âŒ Neo4j ì¸ì¦ ì‹¤íŒ¨"
        elif "dns" in error_msg.lower():
            return False, "âŒ Neo4j ì£¼ì†Œ ì˜¤ë¥˜"
        else:
            return False, f"âŒ Neo4j: {error_msg[:50]}"

# í˜„ì¬ íŒŒì¼ì˜ í´ë” ê²½ë¡œë¥¼ ì¶”ê°€í•´ìš”!
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Streamlit Cloudìš© ì§ì ‘ ì—”ì§„ ì„í¬íŠ¸
try:
    from engine import HybridGraphRAGEngine
    DIRECT_ENGINE_AVAILABLE = True
except ImportError:
    DIRECT_ENGINE_AVAILABLE = False
    HybridGraphRAGEngine = None

# í˜ì´ì§€ ì„¤ì • - Executive Dashboard
st.set_page_config(
    page_title="VIK AI: Executive Intelligence",
    page_icon="ğŸ’¼",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Dark Mode ìŠ¤íƒ€ì¼ CSS
st.markdown("""
<style>
/* ì „ì²´ ì•± ë‹¤í¬ëª¨ë“œ ìŠ¤íƒ€ì¼ */
.stApp {
    background-color: #0e1117;
    color: #ffffff;
}

/* ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë³¸ ìƒ‰ìƒ */
.stApp, .stApp p, .stApp span, .stApp div {
    color: #ffffff !important;
}

/* ë³´ê³ ì„œ ì»¨í…Œì´ë„ˆ ë‹¤í¬ëª¨ë“œ */
.report-container {
    background: #1a1d29;
    padding: 2rem;
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.4);
    margin: 1.5rem 0;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.7;
    border: 1px solid #2d3142;
}

.report-container h2 {
    color: #ffffff !important;
    font-size: 1.5rem;
    font-weight: 600;
    margin-top: 2rem;
    margin-bottom: 1rem;
    border-bottom: 2px solid #3d4461;
    padding-bottom: 0.5rem;
}

.report-container p {
    color: #e0e0e0 !important;
    margin-bottom: 1rem;
    font-size: 1rem;
}

/* ì¸ë¼ì¸ citation ìŠ¤íƒ€ì¼ - í˜¸ë²„ë§ ê°€ëŠ¥ */
.citation {
    display: inline-block;
    background: #4a9eff;
    color: #ffffff;
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 0.85em;
    font-weight: 600;
    margin: 0 2px;
    cursor: pointer;
    text-decoration: none;
    position: relative;
    transition: all 0.2s ease;
}

.citation:hover {
    background: #6bb3ff;
    transform: translateY(-1px);
    box-shadow: 0 4px 12px rgba(74,158,255,0.5);
}

/* íˆ´íŒ ë‹¤í¬ëª¨ë“œ ìŠ¤íƒ€ì¼ */
.citation-tooltip {
    visibility: hidden;
    opacity: 0;
    position: absolute;
    z-index: 1000;
    bottom: 125%;
    left: 50%;
    transform: translateX(-50%);
    min-width: 320px;
    max-width: 400px;
    background: #1e2330;
    border: 1px solid #3d4461;
    border-radius: 8px;
    padding: 12px;
    box-shadow: 0 6px 20px rgba(0,0,0,0.6);
    transition: opacity 0.3s ease, visibility 0.3s ease;
    pointer-events: none;
}

.citation:hover .citation-tooltip,
.citation-tooltip:hover {
    visibility: visible;
    opacity: 1;
    pointer-events: auto;
}

.citation-tooltip::after {
    content: "";
    position: absolute;
    top: 100%;
    left: 50%;
    margin-left: -5px;
    border-width: 5px;
    border-style: solid;
    border-color: #1e2330 transparent transparent transparent;
}

.tooltip-header {
    font-weight: 600;
    color: #4a9eff !important;
    font-size: 0.9em;
    margin-bottom: 6px;
    border-bottom: 1px solid #3d4461;
    padding-bottom: 4px;
}

.tooltip-content {
    font-size: 0.85em;
    color: #c0c0c0 !important;
    line-height: 1.4;
}

.tooltip-meta {
    font-size: 0.75em;
    color: #888888 !important;
    margin-top: 6px;
    padding-top: 6px;
    border-top: 1px solid #2d3142;
}

/* References ì„¹ì…˜ ë‹¤í¬ëª¨ë“œ */
.references {
    background: #1a1d29;
    border-left: 3px solid #4a9eff;
    padding: 1rem 1.5rem;
    margin-top: 2rem;
    border-radius: 4px;
}

.references h3 {
    color: #ffffff !important;
    font-size: 1.2rem;
    margin-bottom: 1rem;
}

.reference-item {
    margin-bottom: 0.8rem;
    padding: 0.5rem;
    background: #252936;
    border-radius: 4px;
    border: 1px solid #2d3142;
}

.reference-number {
    display: inline-block;
    background: #4a9eff;
    color: #ffffff;
    padding: 2px 8px;
    border-radius: 4px;
    font-weight: 600;
    margin-right: 0.5rem;
    font-size: 0.9em;
}

.reference-file {
    font-weight: 500;
    color: #e0e0e0 !important;
}

.reference-excerpt {
    color: #a0a0a0 !important;
    font-size: 0.9em;
    margin-top: 0.3rem;
    font-style: italic;
}

/* ì±„íŒ… ë©”ì‹œì§€ ë‹¤í¬ëª¨ë“œ */
.user-message {
    background: #1e3a5f !important;
    color: #ffffff !important;
}

.assistant-message {
    background: #1a1d29 !important;
    color: #ffffff !important;
}

/* Streamlit ê¸°ë³¸ ìš”ì†Œ ë‹¤í¬ëª¨ë“œ ì˜¤ë²„ë¼ì´ë“œ */
.stMarkdown {
    color: #ffffff !important;
}

.stTextInput input {
    background-color: #1a1d29 !important;
    color: #ffffff !important;
    border: 1px solid #3d4461 !important;
}

.stTextArea textarea {
    background-color: #1a1d29 !important;
    color: #ffffff !important;
    border: 1px solid #3d4461 !important;
}

.stButton button {
    background-color: #4a9eff !important;
    color: #ffffff !important;
    border: none !important;
}

.stButton button:hover {
    background-color: #6bb3ff !important;
}

/* íƒ­ ìŠ¤íƒ€ì¼ ë‹¤í¬ëª¨ë“œ */
.stTabs [data-baseweb="tab-list"] {
    background-color: #1a1d29;
}

.stTabs [data-baseweb="tab"] {
    color: #a0a0a0 !important;
}

.stTabs [aria-selected="true"] {
    color: #4a9eff !important;
}

/* ìµìŠ¤íŒ¬ë” ë‹¤í¬ëª¨ë“œ */
.streamlit-expanderHeader {
    background-color: #1a1d29 !important;
    color: #ffffff !important;
}

.streamlit-expanderContent {
    background-color: #0e1117 !important;
    border: 1px solid #2d3142 !important;
}

/* ìŠ¬ë¼ì´ë” ë‹¤í¬ëª¨ë“œ */
.stSlider label {
    color: #ffffff !important;
}

/* ë¼ë””ì˜¤ ë²„íŠ¼ ë‹¤í¬ëª¨ë“œ */
.stRadio label {
    color: #ffffff !important;
}

/* ì²´í¬ë°•ìŠ¤ ë‹¤í¬ëª¨ë“œ */
.stCheckbox label {
    color: #ffffff !important;
}

/* ìº¡ì…˜ ë‹¤í¬ëª¨ë“œ */
.stCaptionContainer, .stCaption {
    color: #a0a0a0 !important;
}

/* íŒŒì¼ ì—…ë¡œë” ë‹¤í¬ëª¨ë“œ */
.stFileUploader {
    background-color: #1a1d29 !important;
    border: 1px solid #3d4461 !important;
}

/* ì •ë³´/ê²½ê³  ë©”ì‹œì§€ ë‹¤í¬ëª¨ë“œ */
.stAlert {
    background-color: #1a1d29 !important;
    border: 1px solid #3d4461 !important;
}
</style>
""", unsafe_allow_html=True)

# ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬ íŒŒì¼ ê²½ë¡œ
DATA_SOURCES_FILE = os.path.join(os.path.dirname(__file__), "data_sources.json")

def load_data_sources():
    try:
        if os.path.exists(DATA_SOURCES_FILE):
            with open(DATA_SOURCES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ëˆ„ë½ëœ í‚¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if "pdf" not in data:
                    data["pdf"] = []
                if "text" not in data:
                    data["text"] = []
                if "url" not in data:
                    data["url"] = []
                return data
        return {"pdf": [], "text": [], "url": []}
    except Exception as e:
        print(f"Error loading data sources: {e}")
        return {"pdf": [], "text": [], "url": []}

def save_data_sources(data):
    with open(DATA_SOURCES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def _clean_excerpt(text: str) -> str:
    """ë ˆí¼ëŸ°ìŠ¤ì— í‘œì‹œí•  excerptë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ ì •ë¦¬"""
    if not text:
        return ""
    # ì œì–´ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x1F\x7F]', ' ', str(text))
    # ë„ˆë¬´ ê¹¨ì§„ ë¬¸ì(ï¿½) ì œê±°
    text = text.replace("ï¿½", " ")
    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    # ì²« ë¬¸ì¥ë§Œ ì‚¬ìš© (., ?, !, í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ ê¸°ì¤€)
    sentence_split = re.split(r'(?<=[\.\?\!])\s+|(?<=[ë‹¤ìš”])\s+', text)
    first = sentence_split[0] if sentence_split else text
    return first[:300]

def _strip_llm_sources_section(text: str) -> str:
    """
    LLMì´ ë‹µë³€ ë§ë¯¸ì— 'Sources:' / 'References:' ê°™ì€ ì„¹ì…˜ì„ í…ìŠ¤íŠ¸ë¡œ ë¶™ì´ëŠ” ê²½ìš°,
    UIì—ì„œ HTML Referencesë¥¼ ë³„ë„ë¡œ ë Œë”ë§í•˜ë¯€ë¡œ í•´ë‹¹ ì„¹ì…˜ì„ ì œê±°í•œë‹¤.
    """
    if not text:
        return text
    # í”í•œ íŒ¨í„´: "\n\nSources:\n..." ë˜ëŠ” "\n\nReferences:\n..."
    m = re.search(r"\n\s*\n\s*(Sources|Source|References|Reference)\s*:\s*\n", text, flags=re.IGNORECASE)
    if m:
        return text[:m.start()].rstrip()
    return text

def render_report_with_citations(answer: str, sources: List[Dict]) -> str:
    """
    ë‹µë³€ í…ìŠ¤íŠ¸ì— ì¸ë¼ì¸ citation ë²ˆí˜¸ë¥¼ ê°ì§€í•˜ê³ , 
    í˜¸ë²„ ì‹œ íˆ´íŒì„ ë³´ì—¬ì£¼ëŠ” HTMLë¡œ ë³€í™˜
    """
    # Citation íŒ¨í„´ ì°¾ê¸°: [1], [2], etc.
    citation_pattern = r'\[(\d+)\]'
    
    def replace_citation(match):
        cite_num = int(match.group(1))
        # í•´ë‹¹ ë²ˆí˜¸ì˜ source ì°¾ê¸°
        source = next((s for s in sources if s.get('id') == cite_num), None)
        
        if source:
            file_name = source.get('file', 'Unknown')
            source_type = source.get('type', 'document')
            page_num = source.get('page_number', 'N/A')
            
            # ì›ë¬¸ ì¶”ì¶œ - ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ
            original = source.get('original_sentence', source.get('excerpt', ''))
            if isinstance(original, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'report_string' ì¶”ì¶œ
                original = original.get('report_string', str(original))
            excerpt = _clean_excerpt(original)
            
            # Community Summaryì¸ ê²½ìš° í‘œì‹œ ë°©ì‹ ì¡°ì •
            if source_type == 'community':
                display_name = file_name.split(':')[1].strip() if ':' in file_name else file_name
                tooltip_meta = "Community Report"
            else:
                display_name = file_name
                tooltip_meta = f"Page {page_num}"
            
            # íˆ´íŒì´ í¬í•¨ëœ citation ë§í¬ ìƒì„±
            # NOTE: markdownì—ì„œ 4ì¹¸ ì´ìƒ ë“¤ì—¬ì“°ê¸°ëŠ” code blockìœ¼ë¡œ ì·¨ê¸‰ë  ìˆ˜ ìˆì–´
            # ì¤„ë°”ê¿ˆ/ë“¤ì—¬ì“°ê¸°ë¥¼ ìµœì†Œí™”í•œë‹¤.
            return (
                f'<a href="#source-{cite_num}" class="citation">'
                f'[{cite_num}]'
                f'<div class="citation-tooltip">'
                f'<div class="tooltip-header">{display_name}</div>'
                f'<div class="tooltip-content">{excerpt}...</div>'
                f'<div class="tooltip-meta">{tooltip_meta}</div>'
                f'</div>'
                f'</a>'
            )
        return match.group(0)
    # Citationì„ HTMLë¡œ ë³€í™˜
    html_answer = re.sub(citation_pattern, replace_citation, answer)
    
    # References ì„¹ì…˜ ìƒì„±
    references_html = '<div class="references"><h3>References</h3>'
    for source in sources:
        cite_id = source.get('id')
        file_name = source.get('file', 'Unknown')
        source_type = source.get('type', 'document')
        page_num = source.get('page_number', 'N/A')
        
        # ì›ë¬¸ ì¶”ì¶œ - ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ
        original = source.get('original_sentence', source.get('excerpt', ''))
        if isinstance(original, dict):
            original = original.get('report_string', str(original))
        excerpt = _clean_excerpt(original)
        
        # Community Summaryì¸ ê²½ìš° í‘œì‹œ ë°©ì‹ ì¡°ì •
        if source_type == 'community':
            display_name = file_name
            meta_info = "Community Report"
        else:
            display_name = file_name
            meta_info = f"Page {page_num}"
        
        references_html += (
            f'<div class="reference-item" id="source-{cite_id}">'
            f'<span class="reference-number">[{cite_id}]</span> '
            f'<span class="reference-file">{display_name}</span> ({meta_info})'
            f'<div class="reference-excerpt">"{excerpt}..."</div>'
            f'</div>'
        )
    references_html += '</div>'
    
    # ì „ì²´ HTML ì¡°í•©
    full_html = f'<div class="report-container">{html_answer}{references_html}</div>'
    
    return full_html

def render_citations_with_popover(sources: List[Dict], message_idx: int = 0):
    """
    ì¶œì²˜ ì •ë³´ë¥¼ Streamlit Popoverë¡œ ë Œë”ë§
    message_idx: ë©”ì‹œì§€ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•˜ì—¬ ê³ ìœ í•œ í‚¤ ìƒì„±
    """
    if not sources:
        return
    
    st.markdown("---")
    st.markdown("### Source Details")
    
    # ê° ì¶œì²˜ë¥¼ expander ë˜ëŠ” popoverë¡œ í‘œì‹œ
    cols = st.columns(min(len(sources), 3))
    for idx, source in enumerate(sources):
        col_idx = idx % 3
        with cols[col_idx]:
            with st.popover(f"[{source['id']}] {source.get('file', 'Source')[:25]}...", use_container_width=True):
                st.caption(f"**File**: {source.get('file', 'Unknown')}")
                st.caption(f"**Page**: {source.get('page_number', 'N/A')}")
                st.caption(f"**Chunk ID**: {source.get('chunk_id', 'N/A')}")
                
                if source.get('url'):
                    st.caption(f"**URL**: [{source['url']}]({source['url']})")
                
                # ê³ ìœ í•œ í‚¤: ë©”ì‹œì§€ ì¸ë±ìŠ¤ + ì†ŒìŠ¤ ì¸ë±ìŠ¤
                unique_key = f"excerpt_msg{message_idx}_src{idx}_{int(time.time()*1000)}"
                
                st.text_area(
                    "Original Text",
                    value=source.get('original_sentence', source.get('excerpt', ''))[:500],
                    height=150,
                    disabled=True,
                    key=unique_key
                )

# ë°ì´í„° ì†ŒìŠ¤ ì‚­ì œ í•¨ìˆ˜
def delete_data_source(source_type, index):
    data_sources = load_data_sources()
    if 0 <= index < len(data_sources[source_type]):
        del data_sources[source_type][index]
        save_data_sources(data_sources)
        return True
    return False

# API ì—”ë“œí¬ì¸íŠ¸
# Streamlit Cloudì—ì„œëŠ” STREAMLIT_SHARING_MODE í™˜ê²½ ë³€ìˆ˜ê°€ ìë™ìœ¼ë¡œ ì„¤ì •ë¨
# ë¡œì»¬ì—ì„œëŠ” 127.0.0.1:8000, Cloudì—ì„œëŠ” API ì„œë²„ ë¹„í™œì„±í™”
import socket

def is_streamlit_cloud():
    """Streamlit Cloud í™˜ê²½ ê°ì§€"""
    return os.getenv("STREAMLIT_SHARING_MODE") is not None or os.getenv("HOSTNAME", "").startswith("streamlit-")

if is_streamlit_cloud():
    # Streamlit Cloud: API ì„œë²„ ì—†ì´ ì§ì ‘ ì—”ì§„ ì‚¬ìš©
    API_BASE_URL = None
    USE_DIRECT_ENGINE = True
else:
    # ë¡œì»¬: FastAPI ì„œë²„ ì‚¬ìš©
    API_BASE_URL = os.getenv("API_BASE_URL", "http://127.0.0.1:8000")
    USE_DIRECT_ENGINE = False

# ì „ì—­ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ (Streamlit Cloudìš©)
_direct_engine = None

def get_direct_engine():
    """Streamlit Cloudì—ì„œ ì§ì ‘ ì—”ì§„ ê°€ì ¸ì˜¤ê¸°"""
    global _direct_engine
    if _direct_engine is None and DIRECT_ENGINE_AVAILABLE:
        try:
            _direct_engine = HybridGraphRAGEngine(
                working_dir="./graph_storage_hybrid",
                enable_local=False,  # Streamlit Cloudì—ì„œëŠ” Ollama ì—†ìŒ
                enable_neo4j=False   # Streamlit Cloudì—ì„œëŠ” Neo4j ì—†ìŒ
            )
        except Exception as e:
            st.error(f"ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return None
    return _direct_engine

# ìºì‹œ: ë°±ì—”ë“œ ìƒíƒœ/ì§ˆì˜ (ê·œì¹™: st.cache_dataë¡œ ë¬´ê±°ìš´ í˜¸ì¶œ ìºì‹±)
@st.cache_data(ttl=30, show_spinner=False)
def cached_health(api_base_url) -> bool:
    if USE_DIRECT_ENGINE or api_base_url is None:
        # Streamlit Cloud: ì§ì ‘ ì—”ì§„ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        return DIRECT_ENGINE_AVAILABLE
    try:
        r = requests.get(f"{api_base_url}/health", timeout=5)
        return r.status_code == 200
    except Exception:
        return False


@st.cache_data(ttl=120, show_spinner=False)
def cached_query(api_base_url: str, payload_json: str) -> Dict:
    payload = json.loads(payload_json)
    
    if USE_DIRECT_ENGINE:
        # Streamlit Cloud: ì§ì ‘ ì—”ì§„ ì‚¬ìš©
        engine = get_direct_engine()
        if engine is None:
            return {"_error": "GraphRAG ì—”ì§„ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            import asyncio
            question = payload.get("question", "")
            search_type = payload.get("search_type", "local")
            
            # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            if search_type == "global":
                response = loop.run_until_complete(engine.aglobal_search(question))
            else:
                response = loop.run_until_complete(engine.aquery(question))
            
            loop.close()
            
            return {
                "response": response,
                "sources": [],
                "confidence": 1.0,
                "search_mode": "DIRECT_ENGINE"
            }
        except Exception as e:
            return {"_error": f"ì—”ì§„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"}
    
    # ë¡œì»¬: FastAPI ì„œë²„ ì‚¬ìš©
    # Agentic Workflow ëª¨ë“œ í™•ì¸
    use_agentic = payload.get("use_agentic_workflow", False)
    endpoint = "/agentic-query" if use_agentic else "/query"
    
    r = requests.post(f"{api_base_url}{endpoint}", json=payload, timeout=180)
    if r.status_code == 200:
        return r.json()
    return {"_error": f"Error {r.status_code}: {r.text}"}

# System Status Bar (Top)
col1, col2, col3 = st.columns([2, 1, 1])

with col1:
    st.markdown("# VIK AI")
    st.markdown("*Powered by GraphRAG*")

with col2:
    server_connected = cached_health(API_BASE_URL)
    
    # Streamlit Cloud ëª¨ë“œì¼ ë•ŒëŠ” ë‹¤ë¥¸ ë©”ì‹œì§€ í‘œì‹œ
    if USE_DIRECT_ENGINE:
        status_text = "Direct Engine Mode"
        status_color = "#28a745"
    else:
        status_text = "Connected" if server_connected else "Backend Disconnected"
        status_color = "#28a745" if server_connected else "#dc3545"
    
    # Neo4j ì—°ê²° ìƒíƒœ ì²´í¬
    neo4j_connected, neo4j_msg = check_neo4j_connection()
    neo4j_color = "#28a745" if neo4j_connected else "#ffc107"
    
    status_html = f"""
    <div style="text-align: right; padding: 10px;">
        <span style="color: {status_color}; font-size: 12px;">
            â— {status_text}
        </span><br>
        <span style="color: {neo4j_color}; font-size: 10px;">
            {neo4j_msg}
        </span>
    </div>
    """
    st.markdown(status_html, unsafe_allow_html=True)

with col3:
    if st.button(" Refresh", type="secondary"):
        st.rerun()

st.markdown("---")

# Main Tabs
tab1, tab2, tab3, tab4 = st.tabs(["Query Interface", "Data Ingestion", "Data Sources", "ğŸ—ï¸ Domain Analysis"])

# Tab 1: Query Interface
with tab1:
    st.markdown("### Query Interface")
    
    # Advanced Settings Expander
    with st.expander("Advanced Settings", expanded=False):
        # Privacy Mode Toggle
        privacy_mode = st.checkbox(
            "Privacy Mode (8GB RAM Optimized)",
            value=st.session_state.get("privacy_mode", False),
            help="Enable offline-first, memory-efficient analysis with Ollama. Perfect for sensitive data."
        )
        st.session_state["privacy_mode"] = privacy_mode
        
        if privacy_mode:
            st.success("Privacy Mode: All processing happens locally with optimized memory usage")
            llm_mode = "LOCAL (qwen2.5-coder-3B)"  # Force LOCAL mode
        else:
            # LLM Mode Selection
            llm_mode = st.radio(
                "LLM Mode",
                ["API (OpenAI)", "LOCAL (qwen2.5-coder-3B)"],
                index=0,
                help="API: Use OpenAI GPT-4o-mini (faster, more accurate) | LOCAL: Use Ollama Qwen2.5-Coder-3B (private, offline)",
                horizontal=True
            )
        
        if llm_mode == "API (OpenAI)":
            st.info("ğŸŒ API Mode: Using OpenAI GPT-4o-mini for high-quality responses")
        else:
            st.info("ğŸ  LOCAL Mode: Using Ollama Qwen2.5-Coder-3B for privacy-first analysis")
        
        st.markdown("---")
        
        # Search Mode
        search_mode = st.radio(
            "Search Mode",
            ["Local (Specific)", "Global (Overview)"],
            index=0,
            help="Local: Search for specific entities and facts | Global: Get overview and common themes across all documents",
            horizontal=True
        )
        
        st.markdown("---")
        
        # ì›¹ ê²€ìƒ‰ í™œì„±í™” í† ê¸€
        enable_web_search = st.checkbox(
            "Enable Web Search",
            value=False,
            help="Check this to allow AI to search the web for real-time information. Otherwise, it will ONLY use your uploaded PDF documents."
        )
        
        # Multi-Agent ëª¨ë“œ í† ê¸€
        use_multi_agent = st.checkbox(
            "Multi-Agent Analysis Mode",
            value=False,
            help="Enable 4-agent collaboration (Master â†’ KB Collector â†’ Analyst â†’ Writer) for complex financial queries."
        )
        
        # Agentic Workflow ëª¨ë“œ í† ê¸€
        use_agentic_workflow = st.checkbox(
            "Agentic Workflow (LangGraph)",
            value=False,
            help="Enable LangGraph-based workflow: Planner â†’ Collector â†’ Analyst â†’ Writer with feedback loop and reasoning path."
        )
        st.session_state["use_agentic_workflow"] = use_agentic_workflow
        
        if enable_web_search:
            st.warning("Web search enabled: AI may search the web for LATEST/TODAY information if needed.")
        else:
            st.info("Document-only mode: AI will answer ONLY from your uploaded PDFs.")
        
        if use_multi_agent:
            st.info("Multi-Agent mode: Master â†’ KB Collector â†’ Analyst â†’ Writer pipeline will process your query.")
        
        if use_agentic_workflow:
            st.success("Agentic Workflow: LangGraph orchestration with automatic feedback loop and reasoning path tracking.")
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=0.2,
                step=0.1,
                help="Controls randomness. Lower = more focused, Higher = more creative"
            )
            st.caption(f"Current: {temperature}")
        
        with col2:
            top_k = st.slider(
                "Retrieval Chunks",
                min_value=5,
                max_value=50,
                value=30,
                step=5,
                help="Number of text chunks to retrieve from the knowledge graph"
            )
            st.caption(f"Current: {top_k} chunks")
        
        st.markdown("---")
        st.markdown("**Parameter Guide:**")
        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown("""
            **Temperature:**
            - 0.0-0.3: Precise, factual
            - 0.4-0.7: Balanced
            - 0.8-2.0: Creative, diverse
            """)
        with col_b:
            st.markdown("""
            **Retrieval Chunks:**
            - 5-15: Fast, focused
            - 20-30: Balanced (recommended)
            - 35-50: Comprehensive, slower
            """)
    
    # Store settings in session state
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.2
    if "top_k" not in st.session_state:
        st.session_state.top_k = 30
    if "enable_web_search" not in st.session_state:
        st.session_state.enable_web_search = False
    if "llm_mode" not in st.session_state:
        st.session_state.llm_mode = "api"
    
    st.session_state.temperature = temperature
    st.session_state.top_k = top_k
    st.session_state.enable_web_search = enable_web_search
    st.session_state.use_multi_agent = use_multi_agent
    st.session_state.llm_mode = "api" if "API" in llm_mode else "local"
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Chat container with dark mode styling
    st.markdown("""
    <style>
        .chat-container {
            max-height: 500px;
            overflow-y: auto;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        .user-message {
            background: #1e3a5f !important;
            color: #ffffff !important;
            padding: 1rem;
            border-radius: 12px;
            margin: 0.5rem 0 0.5rem auto;
            max-width: 70%;
            text-align: right;
            border: 1px solid #2d4a6f;
        }
        .assistant-message {
            background: #1a1d29 !important;
            color: #ffffff !important;
            padding: 1rem;
            border-radius: 12px;
            margin: 0.5rem auto 0.5rem 0;
            max-width: 70%;
            text-align: left;
            border: 1px solid #2d3142;
        }
        .message-mode {
            font-size: 0.75rem;
            color: #a0a0a0 !important;
            margin-top: 0.5rem;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # Display chat history with custom styling
    chat_container = st.container()
    with chat_container:
        for msg_idx, message in enumerate(st.session_state.messages):
            if message["role"] == "user":
                st.markdown(f"""
                <div class="user-message">
                    {message["content"]}
                </div>
                """, unsafe_allow_html=True)
            else:
                # ì¶œì²˜ ì •ë³´ê°€ ìˆìœ¼ë©´ Perplexity ìŠ¤íƒ€ì¼ë¡œ ë Œë”ë§
                sources = message.get("sources", [])
                source_type = message.get("source_type", "UNKNOWN")
                validation = message.get("validation", None)
                
                # Confidence Score í‘œì‹œ
                if validation and validation.get("confidence_score") is not None:
                    confidence = validation["confidence_score"]
                    if confidence >= 0.9:
                        st.success(f"Confidence: {confidence:.1%} - High reliability")
                    elif confidence >= 0.7:
                        st.info(f"Confidence: {confidence:.1%} - Medium reliability")
                    else:
                        st.warning(f"Confidence: {confidence:.1%} - Low reliability. Some citations may be invalid.")
                
                if sources:                    
                    # LLMì´ í…ìŠ¤íŠ¸ë¡œ 'Sources:' ì„¹ì…˜ì„ ë¶™ì´ëŠ” ê²½ìš° ì œê±° í›„ ë Œë”ë§
                    cleaned_content = _strip_llm_sources_section(message["content"])
                    # Citationê³¼ Referencesê°€ í¬í•¨ëœ ë³´ê³ ì„œ í˜•ì‹
                    report_html = render_report_with_citations(cleaned_content, sources)
                    st.markdown(report_html, unsafe_allow_html=True)
                    
                    # Popoverë¡œ ì¶”ê°€ ìƒì„¸ ì •ë³´ ì œê³µ (ì„ íƒì‚¬í•­)
                    with st.expander(f"View {len(sources)} Source(s) in Detail", expanded=False):
                        render_citations_with_popover(sources, message_idx=msg_idx)

                    # Evidence(í´ë ˆì„-ê·¼ê±°) í‘œì‹œ
                    evidence = message.get("evidence", [])
                    if evidence:
                        with st.expander(f"Evidence ({len(evidence)})", expanded=False):
                            for ev in evidence[:20]:
                                claim_id = ev.get("claim_id")
                                claim_text = ev.get("claim_text", "")
                                citation_ids = ev.get("citation_ids", [])
                                st.markdown(f"- [{claim_id}] {claim_text} " + " ".join([f"[{cid}]" for cid in citation_ids]))
                    
                    # Multi-Agent ì¶”ê°€ ì •ë³´ í‘œì‹œ
                    if message.get("mode") in ["MULTI_AGENT", "AGENTIC_WORKFLOW"]:
                        # íˆ¬ì ì œì–¸
                        recommendation = message.get("recommendation")
                        if recommendation:
                            st.success(f"Investment Recommendation: {recommendation}")
                        
                        # Agentic Workflow ì¶”ê°€ ì •ë³´
                        if message.get("mode") == "AGENTIC_WORKFLOW":
                            # ì¶”ë¡  ê²½ë¡œ
                            reasoning_path = message.get("reasoning_path", [])
                            if reasoning_path:
                                with st.expander("Reasoning Path", expanded=True):
                                    for i, step in enumerate(reasoning_path, 1):
                                        st.markdown(f"{i}. {step}")
                            
                            # ì„œë¸ŒíƒœìŠ¤í¬
                            subtasks = message.get("subtasks", [])
                            if subtasks:
                                with st.expander(f"Subtasks ({len(subtasks)})", expanded=False):
                                    for subtask in subtasks:
                                        st.markdown(f"**{subtask.get('id')}. {subtask.get('task')}** (Target: {subtask.get('target', 'N/A')})")
                                        st.markdown(f"   Priority: {subtask.get('priority', 'N/A')} | Reasoning: {subtask.get('reasoning', 'N/A')}")
                            
                            # ë°˜ë³µ íšŸìˆ˜
                            iteration_count = message.get("iteration_count", 0)
                            if iteration_count > 0:
                                st.info(f"Feedback Loop: {iteration_count} iteration(s)")
                        
                        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
                        insights = message.get("insights", [])
                        if insights:
                            with st.expander(f"Key Insights ({len(insights)})", expanded=False):
                                for insight in insights:
                                    st.markdown(f"- {insight}")
                        
                        # ì²˜ë¦¬ ë‹¨ê³„
                        processing_steps = message.get("processing_steps", [])
                        if processing_steps:
                            with st.expander("Processing Steps", expanded=False):
                                for step in processing_steps:
                                    st.markdown(f"- {step}")
                else:
                    # ì¶œì²˜ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í˜•ì‹
                    mode_text = f"<div class='message-mode'>Source: {source_type} | Mode: {message.get('mode', 'N/A')}</div>" if "mode" in message else ""
                    st.markdown(f"""
                    <div class="report-container">
                        {message["content"]}
                        {mode_text}
                    </div>
                    """, unsafe_allow_html=True)
    
    # Clear chat button at the top
    if st.session_state.messages:
        if st.button("Clear Chat History", type="secondary", key="clear_chat_top"):
            st.session_state.messages = []
            st.rerun()
    
    st.markdown("---")
    
    # Chat input at the bottom
    prompt = st.chat_input("Ask a question about your data...")
    
    if prompt:
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Get assistant response
        with st.spinner("Generating executive report..."):
            try:
                # Prepare request with advanced parameters
                search_type = "global" if "Global" in search_mode else "local"
                llm_mode_value = st.session_state.get("llm_mode", "api")
                request_data = {
                    "question": prompt,
                    "mode": llm_mode_value,
                    "temperature": st.session_state.get("temperature", 0.2),
                    "top_k": st.session_state.get("top_k", 30),
                    "search_type": search_type,
                    "enable_web_search": st.session_state.get("enable_web_search", False),
                    "use_multi_agent": st.session_state.get("use_multi_agent", False),
                    "use_agentic_workflow": st.session_state.get("use_agentic_workflow", False)
                }
                
                # ìºì‹œëœ ê²½ë¡œ ìš°ì„  (ë™ì¼ ì§ˆë¬¸/íŒŒë¼ë¯¸í„° ë°˜ë³µ ì‹œ ë¹ ë¦„)
                payload_json = json.dumps(request_data, sort_keys=True, ensure_ascii=False)
                result = cached_query(API_BASE_URL, payload_json)

                if "_error" not in result:
                    answer = result.get("answer", "No response generated.")
                    sources = result.get("sources", [])
                    source_type = result.get("source", "UNKNOWN")
                    mode = result.get('mode', 'unknown').upper()
                    validation = result.get("validation", None)
                    evidence = result.get("evidence", [])
                    
                    # Multi-Agent ì¶”ê°€ í•„ë“œ
                    recommendation = result.get("recommendation", None)
                    insights = result.get("insights", [])
                    processing_steps = result.get("processing_steps", [])
                    
                    # Agentic Workflow ì¶”ê°€ í•„ë“œ
                    reasoning_path = result.get("reasoning_path", [])
                    subtasks = result.get("subtasks", [])
                    iteration_count = result.get("iteration_count", 0)
                    
                    # Add assistant response to chat history with sources
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources,
                        "source_type": source_type,
                        "mode": mode,
                        "validation": validation,
                        "evidence": evidence,
                        "recommendation": recommendation,
                        "insights": insights,
                        "processing_steps": processing_steps,
                        "reasoning_path": reasoning_path,
                        "subtasks": subtasks,
                        "iteration_count": iteration_count
                    })
                else:
                    error_msg = result.get("_error", "Unknown error")
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
            except Exception as e:
                error_msg = f"Query failed: {str(e)}"
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })
        
        # Rerun to show new messages
        st.rerun()

# Tab 2: Data Ingestion
with tab2:
    st.markdown("### Data Ingestion")
    
    # Check if Privacy Mode is enabled
    privacy_mode_active = st.session_state.get("privacy_mode", False)
    
    if privacy_mode_active:
        input_method = st.radio(
            "Select input method",
            options=["Privacy Upload (JSON/CSV/TXT)", "PDF Upload", "URL Crawling"],
            horizontal=True,
            label_visibility="collapsed"
        )
    else:
        input_method = st.radio(
            "Select input method",
            options=["PDF Upload", "URL Crawling"],
            horizontal=True,
            label_visibility="collapsed"
        )
    
    if input_method == "Privacy Upload (JSON/CSV/TXT)":
        st.info("Privacy Mode: Upload company data files (JSON, CSV, TXT) for local processing")
        
        uploaded_file = st.file_uploader(
            "Upload Company Data",
            type=["json", "csv", "txt", "tsv"],
            help="Upload structured company data for privacy-first analysis"
        )
        
        if uploaded_file:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"{uploaded_file.name} ({uploaded_file.size / 1024:.1f} KB)")
            with col2:
                if st.button("Process File", type="primary", use_container_width=True):
                    with st.spinner("Processing with Privacy Mode..."):
                        try:
                            # Import Privacy components
                            import tempfile
                            import asyncio
                            from engine.privacy_ingestor import PrivacyIngestor
                            from engine.privacy_graph_builder import PrivacyGraphBuilder
                            from db.neo4j_db import Neo4jDatabase
                            
                            # Save uploaded file temporarily
                            with tempfile.NamedTemporaryFile(delete=False, suffix=uploaded_file.name) as tmp_file:
                                tmp_file.write(uploaded_file.getvalue())
                                tmp_file_path = tmp_file.name
                            
                            # Initialize Neo4j if configured
                            neo4j_db = None
                            if NEO4J_URI and NEO4J_PASSWORD:
                                try:
                                    neo4j_db = Neo4jDatabase(
                                        uri=NEO4J_URI,
                                        username=NEO4J_USER,
                                        password=NEO4J_PASSWORD
                                    )
                                except Exception as e:
                                    st.warning(f"Neo4j connection failed: {e}. Continuing without graph storage.")
                            
                            # Initialize components
                            ingestor = PrivacyIngestor()
                            builder = PrivacyGraphBuilder(neo4j_db=neo4j_db)
                            
                            # Create progress indicators
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            # Process file
                            status_text.text("Reading file...")
                            chunks = list(ingestor.ingest_file(tmp_file_path))
                            total_chunks = len(chunks)
                            
                            status_text.text(f"Processing {total_chunks} chunks...")
                            
                            # Build graph asynchronously
                            async def build_graph():
                                stats = {"processed": 0}
                                
                                for i, chunk in enumerate(chunks):
                                    await builder.process_chunk(chunk)
                                    stats["processed"] = i + 1
                                    progress_bar.progress((i + 1) / total_chunks)
                                    status_text.text(f"Processed {i + 1}/{total_chunks} chunks")
                                
                                return builder.stats
                            
                            # Run async function
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            stats = loop.run_until_complete(build_graph())
                            loop.close()
                            
                            # Show results
                            progress_bar.progress(1.0)
                            status_text.text("Complete!")
                            
                            st.success("File processed successfully!")
                            st.json({
                                "chunks_processed": stats["chunks_processed"],
                                "entities_extracted": stats["entities_extracted"],
                                "relationships_extracted": stats["relationships_extracted"],
                                "queries_executed": stats["queries_executed"],
                                "errors": stats["errors"]
                            })
                            
                            # Cleanup
                            import os
                            os.unlink(tmp_file_path)
                            
                        except Exception as e:
                            st.error(f"Processing error: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
    
    elif input_method == "PDF Upload":
        uploaded_file = st.file_uploader(
            "Upload PDF document",
            type=["pdf"],
            help="Upload a PDF file to extract and index its content"
        )
        
        if uploaded_file:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"{uploaded_file.name} ({uploaded_file.size / 1024:.1f} KB)")
            with col2:
                if st.button("Process PDF", type="primary", use_container_width=True):
                    with st.spinner("Processing PDF document..."):
                        try:
                            # íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
                            import tempfile
                            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                                tmp_file.write(uploaded_file.getvalue())
                                tmp_path = tmp_file.name
                            
                            # utils.pyì—ì„œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            from utils import extract_text_from_pdf
                            extracted_text = extract_text_from_pdf(tmp_path)
                            
                            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                            os.unlink(tmp_path)
                            
                            if not extracted_text or not extracted_text.strip():
                                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OCRì´ í•„ìš”í•œ ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            else:
                                # ì¸ë±ì‹± ìš”ì²­
                                if USE_DIRECT_ENGINE:
                                    # Streamlit Cloud: ì§ì ‘ ì—”ì§„ ì‚¬ìš©
                                    engine = get_direct_engine()
                                    if engine is None:
                                        st.error("GraphRAG ì—”ì§„ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    else:
                                        try:
                                            import asyncio
                                            loop = asyncio.new_event_loop()
                                            asyncio.set_event_loop(loop)
                                            loop.run_until_complete(engine.ainsert(extracted_text))
                                            loop.close()
                                            
                                            # ë°ì´í„° ì†ŒìŠ¤ ì €ì¥
                                            data_sources = load_data_sources()
                                            data_sources["pdf"].append({
                                                "filename": uploaded_file.name,
                                                "size": uploaded_file.size,
                                                "indexed_at": time.strftime("%Y-%m-%d %H:%M:%S")
                                            })
                                            save_data_sources(data_sources)
                                            
                                            st.success(f"{uploaded_file.name} successfully indexed!")
                                        except Exception as e:
                                            st.error(f"ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")
                                else:
                                    # ë¡œì»¬: FastAPI ì„œë²„ ì‚¬ìš©
                                    response = requests.post(
                                        f"{API_BASE_URL}/insert",
                                        json={"text": extracted_text},
                                        timeout=300
                                    )
                                    
                                    if response.status_code == 200:
                                        # ë°ì´í„° ì†ŒìŠ¤ ì €ì¥
                                        data_sources = load_data_sources()
                                        data_sources["pdf"].append({
                                            "filename": uploaded_file.name,
                                            "size": uploaded_file.size,
                                            "indexed_at": time.strftime("%Y-%m-%d %H:%M:%S")
                                        })
                                        save_data_sources(data_sources)
                                        
                                        st.success(f"{uploaded_file.name} successfully indexed!")
                                    else:
                                        st.error(f"Indexing failed: {response.status_code} - {response.text}")
                        except Exception as e:
                            st.error(f"Error processing PDF: {str(e)}")
    
    else:  # URL Crawling
        url_input = st.text_input(
            "Enter URL to crawl",
            placeholder="https://example.com"
        )
        
        if st.button("Crawl & Index", type="primary"):
            if url_input.strip():
                st.info("URL crawling feature coming soon!")
            else:
                st.warning("Please enter a URL.")

# Tab 3: Data Sources
with tab3:
    st.markdown("### Data Sources")
    
    data_sources = load_data_sources()
    
    # PDF Sources
    st.markdown("#### PDF Documents")
    if data_sources["pdf"]:
        for idx, source in enumerate(data_sources["pdf"]):
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.text(f"{source['filename']}")
            with col2:
                st.text(f"Size: {source['size'] / 1024:.1f} KB | Indexed: {source['indexed_at']}")
            with col3:
                if st.button("Delete", key=f"del_pdf_{idx}"):
                    if delete_data_source("pdf", idx):
                        st.rerun()
    else:
        st.info("No PDF documents indexed yet.")
    
    st.markdown("---")
    
    # Text Sources
    st.markdown("#### ğŸ“ Text Inputs")
    if data_sources["text"]:
        for idx, source in enumerate(data_sources["text"]):
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.text(f"ğŸ“ {source['preview']}")
            with col2:
                st.text(f"Length: {source['length']} chars | Indexed: {source['indexed_at']}")
            with col3:
                if st.button("Delete", key=f"del_text_{idx}"):
                    if delete_data_source("text", idx):
                        st.rerun()
    else:
        st.info("No text inputs indexed yet.")
    
    st.markdown("---")
    
    # URL Sources
    st.markdown("#### URL Sources")
    if data_sources["url"]:
        for idx, source in enumerate(data_sources["url"]):
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.text(f"{source['url']}")
            with col2:
                st.text(f"Indexed: {source['indexed_at']}")
            with col3:
                if st.button("Delete", key=f"del_url_{idx}"):
                    if delete_data_source("url", idx):
                        st.rerun()
    else:
        st.info("No URLs indexed yet.")


# Tab 4: Domain Analysis
with tab4:
    st.markdown("### ğŸ—ï¸ Domain Analysis")
    st.markdown("ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” ë¶„ì„: Event-Actor-Asset-Factor-Region ê´€ê³„ íƒìƒ‰")
    
    # ë¶„ì„ ìœ í˜• ì„ íƒ
    analysis_type = st.selectbox(
        "ë¶„ì„ ìœ í˜•",
        ["Event ì¸ê³¼ê´€ê³„", "Actor ì˜í–¥ë ¥", "Region ì´ë²¤íŠ¸", "Asset ìš”ì¸ ë¶„ì„"],
        help="ë¶„ì„í•˜ê³  ì‹¶ì€ ë„ë©”ì¸ ê´€ê³„ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    # Event ì¸ê³¼ê´€ê³„ ë¶„ì„
    if analysis_type == "Event ì¸ê³¼ê´€ê³„":
        st.markdown("#### Event â†’ Factor â†’ Asset ì¸ê³¼ê´€ê³„ ì²´ì¸")
        
        event_name = st.text_input(
            "Event ì´ë¦„",
            placeholder="ì˜ˆ: Fed ê¸ˆë¦¬ ì¸ìƒ, SVB íŒŒì‚°, ì¤‘êµ­ ë¶€ë™ì‚° ìœ„ê¸°",
            help="ë¶„ì„í•˜ê³  ì‹¶ì€ ê¸ˆìœµ ì´ë²¤íŠ¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        if st.button("ë¶„ì„", key="analyze_event"):
            if not event_name:
                st.warning("Event ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner(f"'{event_name}' ì¸ê³¼ê´€ê³„ ë¶„ì„ ì¤‘..."):
                    try:
                        response = requests.get(
                            f"{API_BASE_URL}/domain/event/{event_name}",
                            timeout=30
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            impact_chain = result.get("impact_chain", [])
                            
                            if impact_chain:
                                st.success(f"âœ… {len(impact_chain)}ê°œì˜ ì¸ê³¼ê´€ê³„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                                
                                for idx, chain in enumerate(impact_chain, 1):
                                    with st.expander(f"ì¸ê³¼ê´€ê³„ {idx}: {chain['factor']['name']} â†’ {chain['asset']['name']}", expanded=True):
                                        col1, col2, col3 = st.columns(3)
                                        
                                        with col1:
                                            st.markdown("**Event**")
                                            st.write(f"ì´ë¦„: {chain['event']['name']}")
                                            st.write(f"ë‚ ì§œ: {chain['event'].get('date', 'N/A')}")
                                            st.write(f"ì˜í–¥ ìˆ˜ì¤€: {chain['event'].get('impact_level', 'N/A')}")
                                        
                                        with col2:
                                            st.markdown("**Factor**")
                                            st.write(f"ì´ë¦„: {chain['factor']['name']}")
                                            st.write(f"íƒ€ì…: {chain['factor']['type']}")
                                        
                                        with col3:
                                            st.markdown("**Asset**")
                                            st.write(f"ì´ë¦„: {chain['asset']['name']}")
                                            st.write(f"íƒ€ì…: {chain['asset']['type']}")
                                        
                                        st.markdown("**ì˜í–¥ ë¶„ì„**")
                                        direction = chain['impact']['direction']
                                        magnitude = chain['impact']['magnitude']
                                        confidence = chain['impact']['confidence']
                                        
                                        direction_emoji = "ğŸ“ˆ" if direction == "Positive" else "ğŸ“‰"
                                        st.write(f"{direction_emoji} ë°©í–¥: {direction}")
                                        st.write(f"ğŸ“Š í¬ê¸°: {magnitude:.2f}")
                                        st.write(f"ğŸ¯ ì‹ ë¢°ë„: {confidence:.2f}")
                            else:
                                st.info(f"'{event_name}'ì— ëŒ€í•œ ì¸ê³¼ê´€ê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            st.error(f"API ì—ëŸ¬: {response.status_code}")
                    
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
    
    # Actor ì˜í–¥ë ¥ ë¶„ì„
    elif analysis_type == "Actor ì˜í–¥ë ¥":
        st.markdown("#### Actorê°€ ê´€ì—¬í•œ Eventì™€ ì˜í–¥ ë¶„ì„")
        
        actor_name = st.text_input(
            "Actor ì´ë¦„",
            placeholder="ì˜ˆ: Federal Reserve, ì¤‘êµ­ ì •ë¶€, BlackRock",
            help="ë¶„ì„í•˜ê³  ì‹¶ì€ ì£¼ì²´(ê¸°ê´€, ì •ë¶€, ê¸°ì—…) ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        if st.button("ë¶„ì„", key="analyze_actor"):
            if not actor_name:
                st.warning("Actor ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner(f"'{actor_name}' ì˜í–¥ë ¥ ë¶„ì„ ì¤‘..."):
                    try:
                        response = requests.get(
                            f"{API_BASE_URL}/domain/actor/{actor_name}",
                            timeout=30
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            influence_data = result.get("influence", [])
                            
                            if influence_data:
                                st.success(f"âœ… {len(influence_data)}ê°œì˜ ì˜í–¥ ê´€ê³„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                                
                                for idx, data in enumerate(influence_data, 1):
                                    with st.expander(f"ì˜í–¥ {idx}: {data['event']['name']}", expanded=True):
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            st.markdown("**Actor ì •ë³´**")
                                            st.write(f"ì´ë¦„: {data['actor']['name']}")
                                            st.write(f"íƒ€ì…: {data['actor']['type']}")
                                            st.write(f"ì—­í• : {data['actor'].get('role', 'N/A')}")
                                            st.write(f"ì˜í–¥ë ¥: {data['actor'].get('influence_level', 'N/A')}")
                                            
                                            st.markdown("**Event ì •ë³´**")
                                            st.write(f"ì´ë¦„: {data['event']['name']}")
                                            st.write(f"ë‚ ì§œ: {data['event'].get('date', 'N/A')}")
                                        
                                        with col2:
                                            st.markdown("**Factor â†’ Asset ì˜í–¥**")
                                            st.write(f"Factor: {data['factor']['name']} ({data['factor']['type']})")
                                            st.write(f"Asset: {data['asset']['name']} ({data['asset']['type']})")
                                            
                                            direction = data['impact']['direction']
                                            magnitude = data['impact']['magnitude']
                                            direction_emoji = "ğŸ“ˆ" if direction == "Positive" else "ğŸ“‰"
                                            st.write(f"{direction_emoji} ì˜í–¥: {direction} (í¬ê¸°: {magnitude:.2f})")
                            else:
                                st.info(f"'{actor_name}'ì— ëŒ€í•œ ì˜í–¥ ê´€ê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            st.error(f"API ì—ëŸ¬: {response.status_code}")
                    
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
    
    # Region ì´ë²¤íŠ¸ ë¶„ì„
    elif analysis_type == "Region ì´ë²¤íŠ¸":
        st.markdown("#### íŠ¹ì • ì§€ì—­ì˜ Eventì™€ ì˜í–¥ë°›ì€ Asset")
        
        region_name = st.text_input(
            "Region ì´ë¦„",
            placeholder="ì˜ˆ: ë¯¸êµ­, ì¤‘êµ­, ì•„ì‹œì•„, ì‹ í¥ì‹œì¥",
            help="ë¶„ì„í•˜ê³  ì‹¶ì€ ì§€ì—­ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        if st.button("ë¶„ì„", key="analyze_region"):
            if not region_name:
                st.warning("Region ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner(f"'{region_name}' ì´ë²¤íŠ¸ ë¶„ì„ ì¤‘..."):
                    try:
                        response = requests.get(
                            f"{API_BASE_URL}/domain/region/{region_name}",
                            timeout=30
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            regional_events = result.get("events", [])
                            
                            if regional_events:
                                st.success(f"âœ… {len(regional_events)}ê°œì˜ ì§€ì—­ ì´ë²¤íŠ¸ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                                
                                for idx, event in enumerate(regional_events, 1):
                                    with st.expander(f"ì´ë²¤íŠ¸ {idx}: {event['event']['name']}", expanded=True):
                                        col1, col2, col3 = st.columns(3)
                                        
                                        with col1:
                                            st.markdown("**Event**")
                                            st.write(f"ì´ë¦„: {event['event']['name']}")
                                            st.write(f"ë‚ ì§œ: {event['event'].get('date', 'N/A')}")
                                            st.write(f"ì˜í–¥ ìˆ˜ì¤€: {event['event'].get('impact_level', 'N/A')}")
                                        
                                        with col2:
                                            st.markdown("**Region**")
                                            st.write(f"ì´ë¦„: {event['region']['name']}")
                                            st.write(f"íƒ€ì…: {event['region']['type']}")
                                            st.write(f"ì˜í–¥ ë²”ìœ„: {event['region'].get('impact_scope', 'N/A')}")
                                        
                                        with col3:
                                            st.markdown("**Factor â†’ Asset**")
                                            st.write(f"Factor: {event['factor']['name']}")
                                            st.write(f"Asset: {event['asset']['name']}")
                                            
                                            direction = event['impact']['direction']
                                            magnitude = event['impact']['magnitude']
                                            direction_emoji = "ğŸ“ˆ" if direction == "Positive" else "ğŸ“‰"
                                            st.write(f"{direction_emoji} {direction} ({magnitude:.2f})")
                            else:
                                st.info(f"'{region_name}'ì— ëŒ€í•œ ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            st.error(f"API ì—ëŸ¬: {response.status_code}")
                    
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
    
    # Asset ìš”ì¸ ë¶„ì„
    elif analysis_type == "Asset ìš”ì¸ ë¶„ì„":
        st.markdown("#### Assetì— ì˜í–¥ì„ ì£¼ëŠ” Factor ë¶„ì„")
        
        asset_name = st.text_input(
            "Asset ì´ë¦„",
            placeholder="ì˜ˆ: ê¸ˆ, ë¯¸êµ­ ë¶€ë™ì‚°, NVDA, êµ­ì±„",
            help="ë¶„ì„í•˜ê³  ì‹¶ì€ ìì‚° ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        if st.button("ë¶„ì„", key="analyze_asset"):
            if not asset_name:
                st.warning("Asset ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner(f"'{asset_name}' ìš”ì¸ ë¶„ì„ ì¤‘..."):
                    try:
                        response = requests.get(
                            f"{API_BASE_URL}/domain/asset/{asset_name}",
                            timeout=30
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            factors = result.get("factors", [])
                            
                            if factors:
                                st.success(f"âœ… {len(factors)}ê°œì˜ ì˜í–¥ ìš”ì¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                                
                                for idx, factor_data in enumerate(factors, 1):
                                    with st.expander(f"ìš”ì¸ {idx}: {factor_data['factor']['name']}", expanded=True):
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            st.markdown("**Factor ì •ë³´**")
                                            st.write(f"ì´ë¦„: {factor_data['factor']['name']}")
                                            st.write(f"íƒ€ì…: {factor_data['factor']['type']}")
                                            value = factor_data['factor'].get('value')
                                            if value is not None:
                                                st.write(f"ê°’: {value}")
                                        
                                        with col2:
                                            st.markdown("**ì˜í–¥ ë¶„ì„**")
                                            direction = factor_data['impact']['direction']
                                            magnitude = factor_data['impact']['magnitude']
                                            confidence = factor_data['impact']['confidence']
                                            
                                            direction_emoji = "ğŸ“ˆ" if direction == "Positive" else "ğŸ“‰"
                                            st.write(f"{direction_emoji} ë°©í–¥: {direction}")
                                            st.write(f"ğŸ“Š í¬ê¸°: {magnitude:.2f}")
                                            st.write(f"ğŸ¯ ì‹ ë¢°ë„: {confidence:.2f}")
                                        
                                        # íŠ¸ë¦¬ê±° ì´ë²¤íŠ¸
                                        triggering_events = factor_data.get('triggering_events', [])
                                        if triggering_events:
                                            st.markdown("**íŠ¸ë¦¬ê±° ì´ë²¤íŠ¸**")
                                            st.write(", ".join(triggering_events))
                            else:
                                st.info(f"'{asset_name}'ì— ëŒ€í•œ ì˜í–¥ ìš”ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            st.error(f"API ì—ëŸ¬: {response.status_code}")
                    
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
    
    st.markdown("---")
    
    # ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ë²„íŠ¼
    st.markdown("### ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ê´€ë¦¬")
    
    if st.button("ğŸ”§ ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”", help="Neo4jì— ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ Constraint ë° Index ìƒì„±"):
        with st.spinner("ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì¤‘..."):
            try:
                response = requests.post(
                    f"{API_BASE_URL}/domain/schema/init",
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("status") == "success":
                        st.success(f"âœ… ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
                        st.write(f"Constraints: {result.get('constraints', 0)}ê°œ")
                        st.write(f"Indexes: {result.get('indexes', 0)}ê°œ")
                    else:
                        st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {result.get('message', 'Unknown error')}")
                else:
                    st.error(f"API ì—ëŸ¬: {response.status_code}")
            
            except Exception as e:
                st.error(f"ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
