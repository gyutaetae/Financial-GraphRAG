import streamlit as st
import requests
import sys
import os
import json
import time
import re
from typing import List, Dict

# .env íŒŒì¼ ì½ê¸°
from dotenv import load_dotenv
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì½ê¸°
try:
    NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")
except:
    pass

# í˜„ì¬ íŒŒì¼ì˜ í´ë” ê²½ë¡œë¥¼ ì¶”ê°€í•´ìš”!
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Streamlit Cloudìš© ì§ì ‘ ì—”ì§„ ì„í¬íŠ¸
try:
    from engine import HybridGraphRAGEngine
    DIRECT_ENGINE_AVAILABLE = True
except ImportError:
    DIRECT_ENGINE_AVAILABLE = False
    HybridGraphRAGEngine = None

# í˜ì´ì§€ ì„¤ì • - Executive Dashboard
st.set_page_config(
    page_title="VIK AI: Executive Intelligence",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Dark Mode ìŠ¤íƒ€ì¼ CSS
st.markdown("""
<style>
/* ì „ì²´ ì•± ë‹¤í¬ëª¨ë“œ ìŠ¤íƒ€ì¼ */
.stApp {
    background-color: #0e1117;
    color: #ffffff;
}

/* ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë³¸ ìƒ‰ìƒ */
.stApp, .stApp p, .stApp span, .stApp div {
    color: #ffffff !important;
}

/* ë³´ê³ ì„œ ì»¨í…Œì´ë„ˆ ë‹¤í¬ëª¨ë“œ */
.report-container {
    background: #1a1d29;
    padding: 2rem;
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.4);
    margin: 1.5rem 0;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.7;
    border: 1px solid #2d3142;
}

.report-container h2 {
    color: #ffffff !important;
    font-size: 1.5rem;
    font-weight: 600;
    margin-top: 2rem;
    margin-bottom: 1rem;
    border-bottom: 2px solid #3d4461;
    padding-bottom: 0.5rem;
}

.report-container p {
    color: #e0e0e0 !important;
    margin-bottom: 1rem;
    font-size: 1rem;
}

/* ì¸ë¼ì¸ citation ìŠ¤íƒ€ì¼ - í˜¸ë²„ë§ ê°€ëŠ¥ */
.citation {
    display: inline-block;
    background: #4a9eff;
    color: #ffffff;
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 0.85em;
    font-weight: 600;
    margin: 0 2px;
    cursor: pointer;
    text-decoration: none;
    position: relative;
    transition: all 0.2s ease;
}

.citation:hover {
    background: #6bb3ff;
    transform: translateY(-1px);
    box-shadow: 0 4px 12px rgba(74,158,255,0.5);
}

/* íˆ´íŒ ë‹¤í¬ëª¨ë“œ ìŠ¤íƒ€ì¼ */
.citation-tooltip {
    visibility: hidden;
    opacity: 0;
    position: absolute;
    z-index: 1000;
    bottom: 125%;
    left: 50%;
    transform: translateX(-50%);
    min-width: 320px;
    max-width: 400px;
    background: #1e2330;
    border: 1px solid #3d4461;
    border-radius: 8px;
    padding: 12px;
    box-shadow: 0 6px 20px rgba(0,0,0,0.6);
    transition: opacity 0.3s ease, visibility 0.3s ease;
    pointer-events: none;
}

.citation:hover .citation-tooltip,
.citation-tooltip:hover {
    visibility: visible;
    opacity: 1;
    pointer-events: auto;
}

.citation-tooltip::after {
    content: "";
    position: absolute;
    top: 100%;
    left: 50%;
    margin-left: -5px;
    border-width: 5px;
    border-style: solid;
    border-color: #1e2330 transparent transparent transparent;
}

.tooltip-header {
    font-weight: 600;
    color: #4a9eff !important;
    font-size: 0.9em;
    margin-bottom: 6px;
    border-bottom: 1px solid #3d4461;
    padding-bottom: 4px;
}

.tooltip-content {
    font-size: 0.85em;
    color: #c0c0c0 !important;
    line-height: 1.4;
}

.tooltip-meta {
    font-size: 0.75em;
    color: #888888 !important;
    margin-top: 6px;
    padding-top: 6px;
    border-top: 1px solid #2d3142;
}

/* References ì„¹ì…˜ ë‹¤í¬ëª¨ë“œ */
.references {
    background: #1a1d29;
    border-left: 3px solid #4a9eff;
    padding: 1rem 1.5rem;
    margin-top: 2rem;
    border-radius: 4px;
}

.references h3 {
    color: #ffffff !important;
    font-size: 1.2rem;
    margin-bottom: 1rem;
}

.reference-item {
    margin-bottom: 0.8rem;
    padding: 0.5rem;
    background: #252936;
    border-radius: 4px;
    border: 1px solid #2d3142;
}

.reference-number {
    display: inline-block;
    background: #4a9eff;
    color: #ffffff;
    padding: 2px 8px;
    border-radius: 4px;
    font-weight: 600;
    margin-right: 0.5rem;
    font-size: 0.9em;
}

.reference-file {
    font-weight: 500;
    color: #e0e0e0 !important;
}

.reference-excerpt {
    color: #a0a0a0 !important;
    font-size: 0.9em;
    margin-top: 0.3rem;
    font-style: italic;
}

/* ì±„íŒ… ë©”ì‹œì§€ ë‹¤í¬ëª¨ë“œ */
.user-message {
    background: #1e3a5f !important;
    color: #ffffff !important;
}

.assistant-message {
    background: #1a1d29 !important;
    color: #ffffff !important;
}

/* Streamlit ê¸°ë³¸ ìš”ì†Œ ë‹¤í¬ëª¨ë“œ ì˜¤ë²„ë¼ì´ë“œ */
.stMarkdown {
    color: #ffffff !important;
}

.stTextInput input {
    background-color: #1a1d29 !important;
    color: #ffffff !important;
    border: 1px solid #3d4461 !important;
}

.stTextArea textarea {
    background-color: #1a1d29 !important;
    color: #ffffff !important;
    border: 1px solid #3d4461 !important;
}

.stButton button {
    background-color: #4a9eff !important;
    color: #ffffff !important;
    border: none !important;
}

.stButton button:hover {
    background-color: #6bb3ff !important;
}

/* íƒ­ ìŠ¤íƒ€ì¼ ë‹¤í¬ëª¨ë“œ */
.stTabs [data-baseweb="tab-list"] {
    background-color: #1a1d29;
}

.stTabs [data-baseweb="tab"] {
    color: #a0a0a0 !important;
}

.stTabs [aria-selected="true"] {
    color: #4a9eff !important;
}

/* ìµìŠ¤íŒ¬ë” ë‹¤í¬ëª¨ë“œ */
.streamlit-expanderHeader {
    background-color: #1a1d29 !important;
    color: #ffffff !important;
}

.streamlit-expanderContent {
    background-color: #0e1117 !important;
    border: 1px solid #2d3142 !important;
}

/* ìŠ¬ë¼ì´ë” ë‹¤í¬ëª¨ë“œ */
.stSlider label {
    color: #ffffff !important;
}

/* ë¼ë””ì˜¤ ë²„íŠ¼ ë‹¤í¬ëª¨ë“œ */
.stRadio label {
    color: #ffffff !important;
}

/* ì²´í¬ë°•ìŠ¤ ë‹¤í¬ëª¨ë“œ */
.stCheckbox label {
    color: #ffffff !important;
}

/* ìº¡ì…˜ ë‹¤í¬ëª¨ë“œ */
.stCaptionContainer, .stCaption {
    color: #a0a0a0 !important;
}

/* íŒŒì¼ ì—…ë¡œë” ë‹¤í¬ëª¨ë“œ */
.stFileUploader {
    background-color: #1a1d29 !important;
    border: 1px solid #3d4461 !important;
}

/* ì •ë³´/ê²½ê³  ë©”ì‹œì§€ ë‹¤í¬ëª¨ë“œ */
.stAlert {
    background-color: #1a1d29 !important;
    border: 1px solid #3d4461 !important;
}
</style>
""", unsafe_allow_html=True)

# ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬ íŒŒì¼ ê²½ë¡œ
DATA_SOURCES_FILE = os.path.join(os.path.dirname(__file__), "data_sources.json")

def load_data_sources():
    try:
        if os.path.exists(DATA_SOURCES_FILE):
            with open(DATA_SOURCES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ëˆ„ë½ëœ í‚¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if "pdf" not in data:
                    data["pdf"] = []
                if "text" not in data:
                    data["text"] = []
                if "url" not in data:
                    data["url"] = []
                return data
        return {"pdf": [], "text": [], "url": []}
    except Exception as e:
        print(f"Error loading data sources: {e}")
        return {"pdf": [], "text": [], "url": []}

def save_data_sources(data):
    with open(DATA_SOURCES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def _clean_excerpt(text: str) -> str:
    """ë ˆí¼ëŸ°ìŠ¤ì— í‘œì‹œí•  excerptë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ ì •ë¦¬"""
    if not text:
        return ""
    # ì œì–´ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x1F\x7F]', ' ', str(text))
    # ë„ˆë¬´ ê¹¨ì§„ ë¬¸ì(ï¿½) ì œê±°
    text = text.replace("ï¿½", " ")
    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    # ì²« ë¬¸ì¥ë§Œ ì‚¬ìš© (., ?, !, í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ ê¸°ì¤€)
    sentence_split = re.split(r'(?<=[\.\?\!])\s+|(?<=[ë‹¤ìš”])\s+', text)
    first = sentence_split[0] if sentence_split else text
    return first[:300]

def render_report_with_citations(answer: str, sources: List[Dict]) -> str:
    """
    ë‹µë³€ í…ìŠ¤íŠ¸ì— ì¸ë¼ì¸ citation ë²ˆí˜¸ë¥¼ ê°ì§€í•˜ê³ , 
    í˜¸ë²„ ì‹œ íˆ´íŒì„ ë³´ì—¬ì£¼ëŠ” HTMLë¡œ ë³€í™˜
    """
    # Citation íŒ¨í„´ ì°¾ê¸°: [1], [2], etc.
    citation_pattern = r'\[(\d+)\]'
    
    def replace_citation(match):
        cite_num = int(match.group(1))
        # í•´ë‹¹ ë²ˆí˜¸ì˜ source ì°¾ê¸°
        source = next((s for s in sources if s.get('id') == cite_num), None)
        
        if source:
            file_name = source.get('file', 'Unknown')
            source_type = source.get('type', 'document')
            page_num = source.get('page_number', 'N/A')
            
            # ì›ë¬¸ ì¶”ì¶œ - ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ
            original = source.get('original_sentence', source.get('excerpt', ''))
            if isinstance(original, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'report_string' ì¶”ì¶œ
                original = original.get('report_string', str(original))
            excerpt = _clean_excerpt(original)
            
            # Community Summaryì¸ ê²½ìš° í‘œì‹œ ë°©ì‹ ì¡°ì •
            if source_type == 'community':
                display_name = file_name.split(':')[1].strip() if ':' in file_name else file_name
                tooltip_meta = "Community Report"
            else:
                display_name = file_name
                tooltip_meta = f"Page {page_num}"
            
            # íˆ´íŒì´ í¬í•¨ëœ citation ë§í¬ ìƒì„±
            tooltip_html = f'''
            <a href="#source-{cite_num}" class="citation">
                [{cite_num}]
                <div class="citation-tooltip">
                    <div class="tooltip-header">{display_name}</div>
                    <div class="tooltip-content">{excerpt}...</div>
                    <div class="tooltip-meta">{tooltip_meta}</div>
                </div>
            </a>
            '''
            return tooltip_html
        return match.group(0)
    
    # Citationì„ HTMLë¡œ ë³€í™˜
    html_answer = re.sub(citation_pattern, replace_citation, answer)
    
    # References ì„¹ì…˜ ìƒì„±
    references_html = '<div class="references"><h3>ğŸ“š References</h3>'
    for source in sources:
        cite_id = source.get('id')
        file_name = source.get('file', 'Unknown')
        source_type = source.get('type', 'document')
        page_num = source.get('page_number', 'N/A')
        
        # ì›ë¬¸ ì¶”ì¶œ - ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ
        original = source.get('original_sentence', source.get('excerpt', ''))
        if isinstance(original, dict):
            original = original.get('report_string', str(original))
        excerpt = _clean_excerpt(original)
        
        # Community Summaryì¸ ê²½ìš° í‘œì‹œ ë°©ì‹ ì¡°ì •
        if source_type == 'community':
            display_name = file_name
            meta_info = "Community Report"
        else:
            display_name = file_name
            meta_info = f"Page {page_num}"
        
        references_html += f'''
        <div class="reference-item" id="source-{cite_id}">
            <span class="reference-number">[{cite_id}]</span>
            <span class="reference-file">{display_name}</span> ({meta_info})
            <div class="reference-excerpt">"{excerpt}..."</div>
        </div>
        '''
    references_html += '</div>'
    
    # ì „ì²´ HTML ì¡°í•©
    full_html = f'<div class="report-container">{html_answer}{references_html}</div>'
    
    return full_html

def render_citations_with_popover(sources: List[Dict], message_idx: int = 0):
    """
    ì¶œì²˜ ì •ë³´ë¥¼ Streamlit Popoverë¡œ ë Œë”ë§
    message_idx: ë©”ì‹œì§€ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•˜ì—¬ ê³ ìœ í•œ í‚¤ ìƒì„±
    """
    if not sources:
        return
    
    st.markdown("---")
    st.markdown("### ğŸ“š Source Details")
    
    # ê° ì¶œì²˜ë¥¼ expander ë˜ëŠ” popoverë¡œ í‘œì‹œ
    cols = st.columns(min(len(sources), 3))
    for idx, source in enumerate(sources):
        col_idx = idx % 3
        with cols[col_idx]:
            with st.popover(f"[{source['id']}] {source.get('file', 'Source')[:25]}...", use_container_width=True):
                st.caption(f"**File**: {source.get('file', 'Unknown')}")
                st.caption(f"**Page**: {source.get('page_number', 'N/A')}")
                st.caption(f"**Chunk ID**: {source.get('chunk_id', 'N/A')}")
                
                if source.get('url'):
                    st.caption(f"**URL**: [{source['url']}]({source['url']})")
                
                # ê³ ìœ í•œ í‚¤: ë©”ì‹œì§€ ì¸ë±ìŠ¤ + ì†ŒìŠ¤ ì¸ë±ìŠ¤
                unique_key = f"excerpt_msg{message_idx}_src{idx}_{int(time.time()*1000)}"
                
                st.text_area(
                    "Original Text",
                    value=source.get('original_sentence', source.get('excerpt', ''))[:500],
                    height=150,
                    disabled=True,
                    key=unique_key
                )

# ë°ì´í„° ì†ŒìŠ¤ ì‚­ì œ í•¨ìˆ˜
def delete_data_source(source_type, index):
    data_sources = load_data_sources()
    if 0 <= index < len(data_sources[source_type]):
        del data_sources[source_type][index]
        save_data_sources(data_sources)
        return True
    return False

# API ì—”ë“œí¬ì¸íŠ¸
# Streamlit Cloudì—ì„œëŠ” STREAMLIT_SHARING_MODE í™˜ê²½ ë³€ìˆ˜ê°€ ìë™ìœ¼ë¡œ ì„¤ì •ë¨
# ë¡œì»¬ì—ì„œëŠ” 127.0.0.1:8000, Cloudì—ì„œëŠ” API ì„œë²„ ë¹„í™œì„±í™”
import socket

def is_streamlit_cloud():
    """Streamlit Cloud í™˜ê²½ ê°ì§€"""
    return os.getenv("STREAMLIT_SHARING_MODE") is not None or os.getenv("HOSTNAME", "").startswith("streamlit-")

if is_streamlit_cloud():
    # Streamlit Cloud: API ì„œë²„ ì—†ì´ ì§ì ‘ ì—”ì§„ ì‚¬ìš©
    API_BASE_URL = None
    USE_DIRECT_ENGINE = True
else:
    # ë¡œì»¬: FastAPI ì„œë²„ ì‚¬ìš©
    API_BASE_URL = os.getenv("API_BASE_URL", "http://127.0.0.1:8000")
    USE_DIRECT_ENGINE = False

# ì „ì—­ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ (Streamlit Cloudìš©)
_direct_engine = None

def get_direct_engine():
    """Streamlit Cloudì—ì„œ ì§ì ‘ ì—”ì§„ ê°€ì ¸ì˜¤ê¸°"""
    global _direct_engine
    if _direct_engine is None and DIRECT_ENGINE_AVAILABLE:
        try:
            _direct_engine = HybridGraphRAGEngine(
                working_dir="./graph_storage_hybrid",
                enable_local=False,  # Streamlit Cloudì—ì„œëŠ” Ollama ì—†ìŒ
                enable_neo4j=False   # Streamlit Cloudì—ì„œëŠ” Neo4j ì—†ìŒ
            )
        except Exception as e:
            st.error(f"ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return None
    return _direct_engine

# ìºì‹œ: ë°±ì—”ë“œ ìƒíƒœ/ì§ˆì˜ (ê·œì¹™: st.cache_dataë¡œ ë¬´ê±°ìš´ í˜¸ì¶œ ìºì‹±)
@st.cache_data(ttl=30, show_spinner=False)
def cached_health(api_base_url) -> bool:
    if USE_DIRECT_ENGINE or api_base_url is None:
        # Streamlit Cloud: ì§ì ‘ ì—”ì§„ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        return DIRECT_ENGINE_AVAILABLE
    try:
        r = requests.get(f"{api_base_url}/health", timeout=5)
        return r.status_code == 200
    except Exception:
        return False


@st.cache_data(ttl=120, show_spinner=False)
def cached_query(api_base_url: str, payload_json: str) -> Dict:
    payload = json.loads(payload_json)
    
    if USE_DIRECT_ENGINE:
        # Streamlit Cloud: ì§ì ‘ ì—”ì§„ ì‚¬ìš©
        engine = get_direct_engine()
        if engine is None:
            return {"_error": "GraphRAG ì—”ì§„ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            import asyncio
            question = payload.get("question", "")
            search_type = payload.get("search_type", "local")
            
            # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            if search_type == "global":
                response = loop.run_until_complete(engine.aglobal_search(question))
            else:
                response = loop.run_until_complete(engine.aquery(question))
            
            loop.close()
            
            return {
                "response": response,
                "sources": [],
                "confidence": 1.0,
                "search_mode": "DIRECT_ENGINE"
            }
        except Exception as e:
            return {"_error": f"ì—”ì§„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"}
    
    # ë¡œì»¬: FastAPI ì„œë²„ ì‚¬ìš©
    r = requests.post(f"{api_base_url}/query", json=payload, timeout=120)
    if r.status_code == 200:
        return r.json()
    return {"_error": f"Error {r.status_code}: {r.text}"}

# System Status Bar (Top)
col1, col2, col3 = st.columns([2, 1, 1])

with col1:
    st.markdown("# ğŸ“Š VIK AI: Executive Intelligence")
    st.markdown("*Powered by Hybrid GraphRAG*")

with col2:
    server_connected = cached_health(API_BASE_URL)
    
    # Streamlit Cloud ëª¨ë“œì¼ ë•ŒëŠ” ë‹¤ë¥¸ ë©”ì‹œì§€ í‘œì‹œ
    if USE_DIRECT_ENGINE:
        status_text = "Direct Engine Mode"
        status_color = "#28a745"
    else:
        status_text = "Backend Connected" if server_connected else "Backend Disconnected"
        status_color = "#28a745" if server_connected else "#dc3545"
    
    status_html = f"""
    <div style="text-align: right; padding: 10px;">
        <span style="color: {status_color}; font-size: 12px;">
            â— {status_text}
        </span>
    </div>
    """
    st.markdown(status_html, unsafe_allow_html=True)

with col3:
    if st.button("ğŸ”„ Refresh", type="secondary"):
        st.rerun()

st.markdown("---")

# Main Tabs
tab1, tab2, tab3 = st.tabs(["ğŸ’¬ Query Interface", "ğŸ“¤ Data Ingestion", "ğŸ“Š Data Sources"])

# Tab 1: Query Interface
with tab1:
    st.markdown("### Query Interface")
    
    # Advanced Settings Expander
    with st.expander("âš™ï¸ Advanced Settings", expanded=False):
        # Search Mode
        search_mode = st.radio(
            "Search Mode",
            ["Local (Specific)", "Global (Overview)"],
            index=0,
            help="Local: Search for specific entities and facts | Global: Get overview and common themes across all documents",
            horizontal=True
        )
        
        st.markdown("---")
        
        # ì›¹ ê²€ìƒ‰ í™œì„±í™” í† ê¸€
        enable_web_search = st.checkbox(
            "ğŸŒ Enable Web Search",
            value=False,
            help="Check this to allow AI to search the web for real-time information. Otherwise, it will ONLY use your uploaded PDF documents."
        )
        
        if enable_web_search:
            st.warning("âš ï¸ Web search enabled: AI may search the web for LATEST/TODAY information if needed.")
        else:
            st.success("âœ… Document-only mode: AI will answer ONLY from your uploaded PDFs.")
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=0.2,
                step=0.1,
                help="Controls randomness. Lower = more focused, Higher = more creative"
            )
            st.caption(f"Current: {temperature}")
        
        with col2:
            top_k = st.slider(
                "Retrieval Chunks",
                min_value=5,
                max_value=50,
                value=30,
                step=5,
                help="Number of text chunks to retrieve from the knowledge graph"
            )
            st.caption(f"Current: {top_k} chunks")
        
        st.markdown("---")
        st.markdown("**ğŸ“Š Parameter Guide:**")
        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown("""
            **Temperature:**
            - 0.0-0.3: Precise, factual
            - 0.4-0.7: Balanced
            - 0.8-2.0: Creative, diverse
            """)
        with col_b:
            st.markdown("""
            **Retrieval Chunks:**
            - 5-15: Fast, focused
            - 20-30: Balanced (recommended)
            - 35-50: Comprehensive, slower
            """)
    
    # Store settings in session state
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.2
    if "top_k" not in st.session_state:
        st.session_state.top_k = 30
    if "enable_web_search" not in st.session_state:
        st.session_state.enable_web_search = False
    
    st.session_state.temperature = temperature
    st.session_state.top_k = top_k
    st.session_state.enable_web_search = enable_web_search
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Chat container with dark mode styling
    st.markdown("""
    <style>
        .chat-container {
            max-height: 500px;
            overflow-y: auto;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        .user-message {
            background: #1e3a5f !important;
            color: #ffffff !important;
            padding: 1rem;
            border-radius: 12px;
            margin: 0.5rem 0 0.5rem auto;
            max-width: 70%;
            text-align: right;
            border: 1px solid #2d4a6f;
        }
        .assistant-message {
            background: #1a1d29 !important;
            color: #ffffff !important;
            padding: 1rem;
            border-radius: 12px;
            margin: 0.5rem auto 0.5rem 0;
            max-width: 70%;
            text-align: left;
            border: 1px solid #2d3142;
        }
        .message-mode {
            font-size: 0.75rem;
            color: #a0a0a0 !important;
            margin-top: 0.5rem;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # Display chat history with custom styling
    chat_container = st.container()
    with chat_container:
        for msg_idx, message in enumerate(st.session_state.messages):
            if message["role"] == "user":
                st.markdown(f"""
                <div class="user-message">
                    {message["content"]}
                </div>
                """, unsafe_allow_html=True)
            else:
                # ì¶œì²˜ ì •ë³´ê°€ ìˆìœ¼ë©´ Perplexity ìŠ¤íƒ€ì¼ë¡œ ë Œë”ë§
                sources = message.get("sources", [])
                source_type = message.get("source_type", "UNKNOWN")
                validation = message.get("validation", None)
                
                # Confidence Score í‘œì‹œ
                if validation and validation.get("confidence_score") is not None:
                    confidence = validation["confidence_score"]
                    if confidence >= 0.9:
                        st.success(f"Confidence: {confidence:.1%} - High reliability")
                    elif confidence >= 0.7:
                        st.info(f"Confidence: {confidence:.1%} - Medium reliability")
                    else:
                        st.warning(f"Confidence: {confidence:.1%} - Low reliability. Some citations may be invalid.")
                
                if sources:
                    # Citationê³¼ Referencesê°€ í¬í•¨ëœ ë³´ê³ ì„œ í˜•ì‹
                    report_html = render_report_with_citations(message["content"], sources)
                    st.markdown(report_html, unsafe_allow_html=True)
                    
                    # Popoverë¡œ ì¶”ê°€ ìƒì„¸ ì •ë³´ ì œê³µ (ì„ íƒì‚¬í•­)
                    with st.expander(f"ğŸ“ View {len(sources)} Source(s) in Detail", expanded=False):
                        render_citations_with_popover(sources, message_idx=msg_idx)

                    # Evidence(í´ë ˆì„-ê·¼ê±°) í‘œì‹œ
                    evidence = message.get("evidence", [])
                    if evidence:
                        with st.expander(f"Evidence ({len(evidence)})", expanded=False):
                            for ev in evidence[:20]:
                                claim_id = ev.get("claim_id")
                                claim_text = ev.get("claim_text", "")
                                citation_ids = ev.get("citation_ids", [])
                                st.markdown(f"- [{claim_id}] {claim_text} " + " ".join([f"[{cid}]" for cid in citation_ids]))
                else:
                    # ì¶œì²˜ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í˜•ì‹
                    mode_text = f"<div class='message-mode'>Source: {source_type} | Mode: {message.get('mode', 'N/A')}</div>" if "mode" in message else ""
                    st.markdown(f"""
                    <div class="report-container">
                        {message["content"]}
                        {mode_text}
                    </div>
                    """, unsafe_allow_html=True)
    
    # Clear chat button at the top
    if st.session_state.messages:
        if st.button("Clear Chat History", type="secondary", key="clear_chat_top"):
            st.session_state.messages = []
            st.rerun()
    
    st.markdown("---")
    
    # Chat input at the bottom
    prompt = st.chat_input("Ask a question about your data...")
    
    if prompt:
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Get assistant response
        with st.spinner("Generating executive report..."):
            try:
                # Prepare request with advanced parameters
                search_type = "global" if "Global" in search_mode else "local"
                request_data = {
                    "question": prompt,
                    "mode": "api",
                    "temperature": st.session_state.get("temperature", 0.2),
                    "top_k": st.session_state.get("top_k", 30),
                    "search_type": search_type,
                    "enable_web_search": st.session_state.get("enable_web_search", False)
                }
                
                # ìºì‹œëœ ê²½ë¡œ ìš°ì„  (ë™ì¼ ì§ˆë¬¸/íŒŒë¼ë¯¸í„° ë°˜ë³µ ì‹œ ë¹ ë¦„)
                payload_json = json.dumps(request_data, sort_keys=True, ensure_ascii=False)
                result = cached_query(API_BASE_URL, payload_json)

                if "_error" not in result:
                    answer = result.get("answer", "No response generated.")
                    sources = result.get("sources", [])
                    source_type = result.get("source", "UNKNOWN")
                    mode = result.get('mode', 'unknown').upper()
                    validation = result.get("validation", None)
                    evidence = result.get("evidence", [])
                    
                    # Add assistant response to chat history with sources
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources,
                        "source_type": source_type,
                        "mode": mode,
                        "validation": validation,
                        "evidence": evidence
                    })
                else:
                    error_msg = result.get("_error", "Unknown error")
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
            except Exception as e:
                error_msg = f"Query failed: {str(e)}"
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })
        
        # Rerun to show new messages
        st.rerun()

# Tab 2: Data Ingestion
with tab2:
    st.markdown("### Data Ingestion")
    
    input_method = st.radio(
        "Select input method",
        options=["PDF Upload", "URL Crawling"],
        horizontal=True,
        label_visibility="collapsed"
    )
    
    if input_method == "PDF Upload":
        uploaded_file = st.file_uploader(
            "Upload PDF document",
            type=["pdf"],
            help="Upload a PDF file to extract and index its content"
        )
        
        if uploaded_file:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"ğŸ“„ {uploaded_file.name} ({uploaded_file.size / 1024:.1f} KB)")
            with col2:
                if st.button("ğŸš€ Process PDF", type="primary", use_container_width=True):
                    with st.spinner("Processing PDF document..."):
                        try:
                            # íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
                            import tempfile
                            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                                tmp_file.write(uploaded_file.getvalue())
                                tmp_path = tmp_file.name
                            
                            # utils.pyì—ì„œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            from utils import extract_text_from_pdf
                            extracted_text = extract_text_from_pdf(tmp_path)
                            
                            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                            os.unlink(tmp_path)
                            
                            if not extracted_text or not extracted_text.strip():
                                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OCRì´ í•„ìš”í•œ ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            else:
                                # ì¸ë±ì‹± ìš”ì²­
                                if USE_DIRECT_ENGINE:
                                    # Streamlit Cloud: ì§ì ‘ ì—”ì§„ ì‚¬ìš©
                                    engine = get_direct_engine()
                                    if engine is None:
                                        st.error("GraphRAG ì—”ì§„ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    else:
                                        try:
                                            import asyncio
                                            loop = asyncio.new_event_loop()
                                            asyncio.set_event_loop(loop)
                                            loop.run_until_complete(engine.ainsert(extracted_text))
                                            loop.close()
                                            
                                            # ë°ì´í„° ì†ŒìŠ¤ ì €ì¥
                                            data_sources = load_data_sources()
                                            data_sources["pdf"].append({
                                                "filename": uploaded_file.name,
                                                "size": uploaded_file.size,
                                                "indexed_at": time.strftime("%Y-%m-%d %H:%M:%S")
                                            })
                                            save_data_sources(data_sources)
                                            
                                            st.success(f"âœ… {uploaded_file.name} successfully indexed!")
                                        except Exception as e:
                                            st.error(f"ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")
                                else:
                                    # ë¡œì»¬: FastAPI ì„œë²„ ì‚¬ìš©
                                    response = requests.post(
                                        f"{API_BASE_URL}/insert",
                                        json={"text": extracted_text},
                                        timeout=300
                                    )
                                    
                                    if response.status_code == 200:
                                        # ë°ì´í„° ì†ŒìŠ¤ ì €ì¥
                                        data_sources = load_data_sources()
                                        data_sources["pdf"].append({
                                            "filename": uploaded_file.name,
                                            "size": uploaded_file.size,
                                            "indexed_at": time.strftime("%Y-%m-%d %H:%M:%S")
                                        })
                                        save_data_sources(data_sources)
                                        
                                        st.success(f"âœ… {uploaded_file.name} successfully indexed!")
                                    else:
                                        st.error(f"Indexing failed: {response.status_code} - {response.text}")
                        except Exception as e:
                            st.error(f"Error processing PDF: {str(e)}")
    
    else:  # URL Crawling
        url_input = st.text_input(
            "Enter URL to crawl",
            placeholder="https://example.com"
        )
        
        if st.button("ğŸš€ Crawl & Index", type="primary"):
            if url_input.strip():
                st.info("URL crawling feature coming soon!")
            else:
                st.warning("Please enter a URL.")

# Tab 3: Data Sources
with tab3:
    st.markdown("### Data Sources")
    
    data_sources = load_data_sources()
    
    # PDF Sources
    st.markdown("#### ğŸ“„ PDF Documents")
    if data_sources["pdf"]:
        for idx, source in enumerate(data_sources["pdf"]):
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.text(f"ğŸ“„ {source['filename']}")
            with col2:
                st.text(f"Size: {source['size'] / 1024:.1f} KB | Indexed: {source['indexed_at']}")
            with col3:
                if st.button("ğŸ—‘ï¸", key=f"del_pdf_{idx}"):
                    if delete_data_source("pdf", idx):
                        st.rerun()
    else:
        st.info("No PDF documents indexed yet.")
    
    st.markdown("---")
    
    # Text Sources
    st.markdown("#### ğŸ“ Text Inputs")
    if data_sources["text"]:
        for idx, source in enumerate(data_sources["text"]):
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.text(f"ğŸ“ {source['preview']}")
            with col2:
                st.text(f"Length: {source['length']} chars | Indexed: {source['indexed_at']}")
            with col3:
                if st.button("ğŸ—‘ï¸", key=f"del_text_{idx}"):
                    if delete_data_source("text", idx):
                        st.rerun()
    else:
        st.info("No text inputs indexed yet.")
    
    st.markdown("---")
    
    # URL Sources
    st.markdown("#### ğŸŒ URL Sources")
    if data_sources["url"]:
        for idx, source in enumerate(data_sources["url"]):
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.text(f"ğŸŒ {source['url']}")
            with col2:
                st.text(f"Indexed: {source['indexed_at']}")
            with col3:
                if st.button("ğŸ—‘ï¸", key=f"del_url_{idx}"):
                    if delete_data_source("url", idx):
                        st.rerun()
    else:
        st.info("No URLs indexed yet.")
