"""
GraphRAG í•µì‹¬ ë¡œì§
ì¸ë±ì‹± ë° ê²€ìƒ‰ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼ì´ì—ìš”!
"""

import os
import asyncio
import sys
from typing import Optional, Literal, Dict, List

from nano_graphrag import GraphRAG
from nano_graphrag.base import QueryParam

# graspologic íŒ¨í‚¤ì§€ê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸ ëª¨ë“ˆ
try:
    import graspologic
    import graspologic.utils
except ImportError:
    class DummyGraspologic:
        class partition:
            @staticmethod
            def hierarchical_leiden(*args, **kwargs):
                return {}
        
        class utils:
            @staticmethod
            def largest_connected_component(graph):
                return graph
    
    sys.modules['graspologic'] = DummyGraspologic()
    sys.modules['graspologic.partition'] = DummyGraspologic.partition
    sys.modules['graspologic.utils'] = DummyGraspologic.utils
    print("âš ï¸  graspologicê°€ ì—†ì–´ì„œ ë”ë¯¸ ëª¨ë“ˆì„ ì‚¬ìš©í•´ìš”. í´ëŸ¬ìŠ¤í„°ë§ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆì–´ìš”.")

# src ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import (
    WORKING_DIR,
    DEV_MODE,
    DEV_MODE_MAX_CHARS,
    validate_config,
    NEO4J_AUTO_EXPORT,
    ENABLE_DOMAIN_SCHEMA,
)

from utils import (
    openai_model_if,
    openai_embedding_if,
    ollama_model_if,
    ollama_embedding_if,
    preprocess_text,
    chunk_text,
    get_financial_entity_prompt,
)

from engine.planner import QueryPlanner
from models.neo4j_models import GraphStats
from engine.neo4j_retriever import Neo4jRetriever
from engine.entity_classifier import EntityClassifier
from engine.relationship_inferencer import RelationshipInferencer
from db.neo4j_db import Neo4jDatabase


class HybridGraphRAGEngine:
    """
    í•˜ì´ë¸Œë¦¬ë“œ GraphRAG ì—”ì§„ í´ë˜ìŠ¤
    ì¸ë±ì‹±ì€ OpenAI APIë¥¼ ì‚¬ìš©í•˜ê³ , ì§ˆë¬¸ì€ API/LOCAL ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!
    """
    
    def __init__(self, working_dir: Optional[str] = None) -> None:
        """
        GraphRAG ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            working_dir: ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: config.WORKING_DIR)
        """
        validate_config()
        
        self.working_dir = working_dir or WORKING_DIR
        os.makedirs(self.working_dir, exist_ok=True)
        
        # ì¸ë±ì‹±ìš© GraphRAG ì¸ìŠ¤í„´ìŠ¤ (í•­ìƒ OpenAI API ì‚¬ìš©)
        self.indexing_rag = GraphRAG(
            working_dir=self.working_dir,
            best_model_func=openai_model_if,
            cheap_model_func=openai_model_if,
            embedding_func=openai_embedding_if,
            chunk_token_size=2000,  # 1200 -> 2000 (API í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œ)
            addon_params={
                "entity_extract_max_gleaning": 0,  # 1 -> 0 (ì¬ì¶”ì¶œ ë¹„í™œì„±í™”ë¡œ 2ë°° ì†ë„ í–¥ìƒ)
                "entity_summary_to_max_tokens": 300,  # ìš”ì•½ ê¸¸ì´ ì œí•œ
            }
        )
        
        # ì§ˆë¬¸ìš© GraphRAG ì¸ìŠ¤í„´ìŠ¤ë“¤ (API/LOCAL ì„ íƒ ê°€ëŠ¥)
        self.query_rag_api = GraphRAG(
            working_dir=self.working_dir,
            best_model_func=openai_model_if,
            cheap_model_func=openai_model_if,
            embedding_func=openai_embedding_if,
        )
        
        self.query_rag_local = GraphRAG(
            working_dir=self.working_dir,
            best_model_func=ollama_model_if,
            cheap_model_func=ollama_model_if,
            embedding_func=openai_embedding_if,  # ì¸ë±ì‹±ê³¼ ê°™ì€ embedding ì‚¬ìš©!
        )

        # Neo4j ê¸°ë°˜ ì •ë°€ Retriever (ê·¼ê±°/ì¶œì²˜ ìƒì„±ìš©)
        # Neo4j ì—°ê²°ì´ ì—†ìœ¼ë©´ QueryExecutorì—ì„œ ì˜ˆì™¸ê°€ ë‚  ìˆ˜ ìˆìœ¼ë‹ˆ lazy í•˜ê²Œ ì‚¬ìš©
        self._neo4j_retriever: Neo4jRetriever | None = None
        
        # ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ì»´í¬ë„ŒíŠ¸ (lazy loading)
        self._entity_classifier: EntityClassifier | None = None
        self._relationship_inferencer: RelationshipInferencer | None = None
        self._neo4j_db: Neo4jDatabase | None = None
        self.enable_domain_schema = ENABLE_DOMAIN_SCHEMA
        
        print(f"HybridGraphRAGEngine ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {self.working_dir}")
        print(f"ì¸ë±ì‹± ëª¨ë“œ: OpenAI API")
        print(f"ì§ˆë¬¸ ëª¨ë“œ: API ë˜ëŠ” LOCAL ì„ íƒ ê°€ëŠ¥")
        print(f"ë„ë©”ì¸ ìŠ¤í‚¤ë§ˆ: {'í™œì„±í™”' if self.enable_domain_schema else 'ë¹„í™œì„±í™”'}")
    
    async def ainsert(self, text: str) -> None:
        """
        ë¹„ë™ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë˜í”„ì— ì¸ë±ì‹±í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            text: ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸
        """
        # ê°œë°œ ëª¨ë“œì¼ ë•ŒëŠ” í…ìŠ¤íŠ¸ë¥¼ ì§§ê²Œ ìë¦„
        if DEV_MODE:
            text = text[:DEV_MODE_MAX_CHARS]
            print(f"[DEV_MODE] í…ìŠ¤íŠ¸ë¥¼ {DEV_MODE_MAX_CHARS}ìë¡œ ì œí•œí–ˆì–´ìš”!")
        
        # 1) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = preprocess_text(text)
        
        # 2) ì²­í¬ ë¶„í• 
        chunks = chunk_text(processed_text, max_tokens=1200)
        print(f"[DEBUG] ì¸ë±ì‹±ìš© ì²­í¬ ê°œìˆ˜: {len(chunks)}")
        
        # 3) ë¹„ë™ê¸° ë³‘ë ¬ ì¸ë±ì‹± (ìµœëŒ€ ë™ì‹œ 15ê°œë¡œ ì¦ê°€)
        semaphore = asyncio.Semaphore(15)  # 10 -> 15 (ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€)
        
        async def insert_one(chunk_text: str, idx: int) -> None:
            async with semaphore:
                print(f"[DEBUG] ì²­í¬ {idx+1}/{len(chunks)} ì¸ë±ì‹± ì‹œì‘")
                await self.indexing_rag.ainsert(chunk_text)
                print(f"[DEBUG] ì²­í¬ {idx+1}/{len(chunks)} ì¸ë±ì‹± ì™„ë£Œ")
        
        # 4) ëª¨ë“  ì²­í¬ë¥¼ ë™ì‹œì— ì¸ë±ì‹±
        tasks = [insert_one(chunk, i) for i, chunk in enumerate(chunks)]
        await asyncio.gather(*tasks)
        
        print("ì¸ë±ì‹± ì™„ë£Œ! (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ + í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©)")
        
        # 5) ë„ë©”ì¸ ë…¸ë“œ ë³€í™˜ (ì˜µì…˜)
        if self.enable_domain_schema:
            print("ğŸ”„ ë„ë©”ì¸ ë…¸ë“œ ë³€í™˜ ì‹œì‘...")
            await self._convert_to_domain_nodes()
        
        # 6) Neo4jë¡œ ìë™ ì—…ë¡œë“œ (ì„¤ì •ë˜ì–´ ìˆì„ ê²½ìš°)
        if NEO4J_AUTO_EXPORT:
            print("Neo4jë¡œ ìë™ ì—…ë¡œë“œ ì‹œì‘...")
            try:
                from ..db.neo4j_db import Neo4jDatabase
                graphml_path = os.path.join(self.working_dir, "graph_chunk_entity_relation.graphml")
                
                if os.path.exists(graphml_path):
                    db = Neo4jDatabase()
                    result = await asyncio.to_thread(
                        db.upload_graphml,
                        graphml_path,
                        clear_before=False
                    )
                    if result["status"] == "success":
                        print(f"Neo4j ì—…ë¡œë“œ ì™„ë£Œ! ë…¸ë“œ: {result['nodes']}ê°œ, ê´€ê³„: {result['edges']}ê°œ")
                    else:
                        print(f"Neo4j ì—…ë¡œë“œ ì‹¤íŒ¨: {result['message']}")
                else:
                    print(f"GraphML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”: {graphml_path}")
            except Exception as e:
                print(f"Neo4j ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                print("NEO4J_URI, NEO4J_PASSWORDê°€ .envì— ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”!")
        else:
            print("Neo4j ìë™ ì—…ë¡œë“œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ìš”. NEO4J_AUTO_EXPORT=trueë¡œ ì„¤ì •í•˜ë©´ ìë™ ì—…ë¡œë“œë©ë‹ˆë‹¤!")
    
    async def aquery(
        self,
        question: str,
        mode: Literal["api", "local"] | None = None,
        auto_plan: bool = True,
        return_context: bool = False,
        top_k: int = 30
    ) -> str | dict:
        """
        ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ ì°¾ëŠ” í•¨ìˆ˜
        
        ê·œì¹™: Planner-Executor íŒ¨í„´ ì‚¬ìš©
        - auto_plan=True: Plannerê°€ ìë™ìœ¼ë¡œ ëª¨ë“œ ê²°ì •
        - auto_plan=False: mode íŒŒë¼ë¯¸í„° ì‚¬ìš©
        
        Args:
            question: ì§ˆë¬¸ ë‚´ìš©
            mode: "api" (OpenAI API) ë˜ëŠ” "local" (Ollama) - auto_plan=Falseì¼ ë•Œë§Œ ì‚¬ìš©
            auto_plan: Plannerë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ëª¨ë“œ ê²°ì • (ê¸°ë³¸ê°’: True)
            return_context: Trueì¼ ê²½ìš° ë‹µë³€ê³¼ í•¨ê»˜ ì¶œì²˜ ì •ë³´ ë°˜í™˜ (ê¸°ë³¸ê°’: False)
            top_k: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì²­í¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 30)
            
        Returns:
            return_context=False: ë‹µë³€ í…ìŠ¤íŠ¸ (str)
            return_context=True: {"answer": str, "sources": List[dict]} (dict)
        """
        # #region agent log
        with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(__import__('json').dumps({"location":"graphrag_engine.py:171","message":"aquery() entry","data":{"question":question,"mode":mode,"return_context":return_context,"top_k":top_k},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1"})+'\n')
        # #endregion
        # Plannerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“œ ìë™ ê²°ì •
        if auto_plan and mode is None:
            planner = QueryPlanner()
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë³µì¡ë„ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„ì„ í•„ìš”)
            mode, complexity, privacy = planner.analyze_query(
                question=question,
                entity_count=1,  # ì‹¤ì œë¡œëŠ” ê·¸ë˜í”„ì—ì„œ ì¶”ì •
                relationship_depth=2,  # ê¸°ë³¸ê°’
                has_pii=False,  # ì‹¤ì œë¡œëŠ” ë°ì´í„° ë¶„ì„ í•„ìš”
                needs_synthesis="cross" in question.lower() or "compare" in question.lower()
            )
            print(f"[Planner] ëª¨ë“œ: {mode}, ë³µì¡ë„: {complexity}, í”„ë¼ì´ë²„ì‹œ: {privacy}")
        
        # modeê°€ Noneì´ë©´ ê¸°ë³¸ê°’ local ì‚¬ìš©
        if mode is None:
            mode = "local"
        
        print(f"[DEBUG] ì§ˆë¬¸: {question}")
        print(f"[DEBUG] ëª¨ë“œ: {mode}")
        print(f"[DEBUG] ì‘ì—… ë””ë ‰í† ë¦¬: {self.working_dir}")
        
        # ê·¸ë˜í”„ íŒŒì¼ í™•ì¸
        graphml_path = os.path.join(self.working_dir, "graph_chunk_entity_relation.graphml")
        if os.path.exists(graphml_path):
            import networkx as nx
            G = nx.read_graphml(graphml_path)
            # #region agent log
            with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                revenue_nodes = [n for n,d in G.nodes(data=True) if 'revenue' in str(d).lower() or 'revenue' in str(n).lower()]
                f.write(__import__('json').dumps({"location":"graphrag_engine.py:232","message":"graph stats","data":{"nodes":G.number_of_nodes(),"edges":G.number_of_edges(),"revenue_nodes_count":len(revenue_nodes),"revenue_nodes_sample":revenue_nodes[:5]},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H6"})+'\n')
            # #endregion
            print(f"[DEBUG] ê·¸ë˜í”„ ë…¸ë“œ ìˆ˜: {G.number_of_nodes()}, ì—£ì§€ ìˆ˜: {G.number_of_edges()}")
        else:
            print(f"[DEBUG] ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ì–´ìš”: {graphml_path}")
        
        if mode == "api":
            print(f"ì§ˆë¬¸ ëª¨ë“œ: OpenAI API (top_k: {top_k})")
            try:
                # Global ëª¨ë“œ: ì „ì²´ ê·¸ë˜í”„ì—ì„œ ì»¤ë®¤ë‹ˆí‹° ë¦¬í¬íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (ë„“ì€ ë²”ìœ„, revenue ê°™ì€ ì§ˆë¬¸ì— ì í•©)
                query_param = QueryParam(
                    mode='global',  # local -> global (ì „ì²´ ê·¸ë˜í”„ ê²€ìƒ‰)
                    top_k=top_k,  # ì‚¬ìš©ì ì§€ì • top_k ì‚¬ìš©
                )
                response = await self.query_rag_api.aquery(question, param=query_param)
                # #region agent log
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"graphrag_engine.py:236","message":"query_rag_api response","data":{"response_length":len(response) if response else 0,"response_preview":response[:200] if response else None},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1"})+'\n')
                # #endregion
                print(f"ğŸ” [DEBUG] query_rag_api.aquery() ì™„ë£Œ! (top_k: {top_k})")
            except Exception as e:
                print(f"âŒ [DEBUG] query_rag_api.aquery() ì—ëŸ¬: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                raise
        else:
            print(f"ğŸ’¬ ì§ˆë¬¸ ëª¨ë“œ: Ollama (ë¡œì»¬, top_k: {top_k})")
            # Ollama ì„œë²„ í™•ì¸
            try:
                import requests
                ollama_check = requests.get("http://localhost:11434/api/tags", timeout=2)
                if ollama_check.status_code != 200:
                    return "âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì–´ìš”! 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”!"
            except:
                return "âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ì–´ìš”! 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ê±°ë‚˜, 'api' ëª¨ë“œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”!"
            
            try:
                # Global ëª¨ë“œë¡œ ê²€ìƒ‰
                query_param = QueryParam(
                    mode='global',  # ì „ì²´ ê·¸ë˜í”„ ê²€ìƒ‰
                    top_k=top_k,  # ì‚¬ìš©ì ì§€ì • top_k ì‚¬ìš©
                )
                response = await self.query_rag_local.aquery(question, param=query_param)
                print(f"ğŸ” [DEBUG] query_rag_local.aquery() ì™„ë£Œ! (top_k: {top_k})")
            except Exception as e:
                print(f"âŒ [DEBUG] query_rag_local.aquery() ì—ëŸ¬: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                raise
        
        # ë‹µë³€ì´ ë¹„ì–´ìˆê±°ë‚˜ "Sorry"ë¡œ ì‹œì‘í•˜ë©´ ê²½ê³ 
        if not response or response.strip().startswith("Sorry"):
            print("âš ï¸  ê·¸ë˜í”„ì— ë°ì´í„°ê°€ ìˆì§€ë§Œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”.")
            print("ğŸ’¡ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”!")
        
        # return_context=Trueì¼ ê²½ìš° ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
        if return_context:
            ctx = await self._aretrieve_context_from_neo4j(question=question, top_sources=min(top_k, 10))
            sources = ctx.get("sources", [])
            # #region agent log
            with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                f.write(__import__('json').dumps({"location":"graphrag_engine.py:288","message":"context retrieval result","data":{"sources_count":len(sources),"first_source_excerpt":sources[0].get('excerpt','')[:100] if sources else None,"retrieval_backend":ctx.get("retrieval_backend")},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H2,H3"})+'\n')
            # #endregion
            return {
                "answer": response,
                "sources": sources,
                "context": ctx.get("context", ""),
                "retrieval_backend": ctx.get("retrieval_backend", "neo4j"),
            }
        
        return response
    
    async def _aretrieve_context_from_neo4j(self, question: str, top_sources: int = 10) -> Dict:
        """
        Neo4jì—ì„œ ì •ë°€ ê·¼ê±°ë¥¼ ì¶”ì¶œí•´ sources/context ìƒì„±.
        ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ KV ê¸°ë°˜ _extract_sourcesë¡œ í´ë°±.
        """
        try:
            if self._neo4j_retriever is None:
                self._neo4j_retriever = Neo4jRetriever()
            # #region agent log
            with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                f.write(__import__('json').dumps({"location":"graphrag_engine.py:314","message":"before neo4j retrieval","data":{"question":question,"top_sources":top_sources},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H2"})+'\n')
            # #endregion
            result = await asyncio.to_thread(
                self._neo4j_retriever.retrieve,
                question,
                2,   # depth=2 (2-hop+)
                50,  # limit=50 (hard LIMIT)
                top_sources,
            )
            sources = result.get("sources", [])
            # #region agent log
            with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                f.write(__import__('json').dumps({"location":"graphrag_engine.py:326","message":"neo4j retrieval result","data":{"sources_count":len(sources),"context_length":len(result.get("context",""))},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H2"})+'\n')
            # #endregion
            
            # Neo4jì—ì„œ ì†ŒìŠ¤ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ KV í´ë°± ì‹¤í–‰
            if not sources or len(sources) == 0:
                # #region agent log
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"graphrag_engine.py:333","message":"neo4j returned empty sources, falling back to KV","data":{},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H2,H3"})+'\n')
                # #endregion
                print(f"[Neo4jRetriever] Neo4j returned 0 sources, falling back to KV store")
                try:
                    sources = await self._extract_sources(question=question)
                    # #region agent log
                    with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                        f.write(__import__('json').dumps({"location":"graphrag_engine.py:340","message":"kv fallback success","data":{"sources_count":len(sources)},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H3"})+'\n')
                    # #endregion
                    return {
                        "context": "",
                        "sources": sources,
                        "retrieval_backend": "kv_fallback",
                    }
                except Exception as e2:
                    # #region agent log
                    with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                        f.write(__import__('json').dumps({"location":"graphrag_engine.py:350","message":"kv fallback also failed","data":{"error":str(e2),"error_type":type(e2).__name__},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H3"})+'\n')
                    # #endregion
                    sources = []
                    return {
                        "context": "",
                        "sources": sources,
                        "retrieval_backend": "kv_fallback",
                    }
            
            return {
                "context": result.get("context", ""),
                "sources": sources,
                "retrieval_backend": "neo4j",
            }
        except Exception as e:
            # #region agent log
            with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                import traceback
                f.write(__import__('json').dumps({"location":"graphrag_engine.py:337","message":"neo4j retrieval failed, trying fallback","data":{"error":str(e),"error_type":type(e).__name__,"traceback":traceback.format_exc()[:500]},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H2,H3"})+'\n')
            # #endregion
            print(f"[Neo4jRetriever] fallback to kv sources: {type(e).__name__}: {e}")
            try:
                sources = await self._extract_sources(question=question)
                # #region agent log
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"graphrag_engine.py:347","message":"kv fallback success","data":{"sources_count":len(sources)},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H3"})+'\n')
                # #endregion
            except Exception as e2:
                # #region agent log
                with open('/Users/gyuteoi/Desktop/graphrag/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"graphrag_engine.py:353","message":"kv fallback also failed","data":{"error":str(e2),"error_type":type(e2).__name__},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run2","hypothesisId":"H3"})+'\n')
                # #endregion
                sources = []
            return {
                "context": "",
                "sources": sources,
                "retrieval_backend": "kv_fallback",
            }
    
    async def _extract_sources(self, question: str = "") -> list[dict]:
        """
        text_chunks KV storeì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
        ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì²­í¬ë¥¼ ìš°ì„  ì„ íƒ (ìµœëŒ€ 10ê°œ)
        
        Args:
            question: ì§ˆë¬¸ ë‚´ìš© (ê´€ë ¨ ì²­í¬ë¥¼ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©)
        
        Returns:
            List of source dicts with id, file, chunk_id, excerpt
        """
        import json
        import re
        
        sources = []
        text_chunks_path = os.path.join(self.working_dir, "kv_store_text_chunks.json")
        
        if not os.path.exists(text_chunks_path):
            print("[DEBUG] text_chunks íŒŒì¼ì´ ì—†ì–´ìš”")
            return sources
        
        # data_sources.jsonì—ì„œ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
        data_sources_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data_sources.json")
        pdf_files = []
        if os.path.exists(data_sources_file):
            try:
                with open(data_sources_file, 'r', encoding='utf-8') as f:
                    data_sources = json.load(f)
                    pdf_files = [pdf.get('name', 'uploaded_document.pdf') for pdf in data_sources.get('pdfs', [])]
            except:
                pass
        
        try:
            with open(text_chunks_path, 'r', encoding='utf-8') as f:
                chunks_data = json.load(f)
            
            # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€/ì˜ë¬¸)
            question_lower = question.lower()
            keywords = []
            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            if "ì—”ë¹„ë””ì•„" in question or "nvidia" in question_lower:
                keywords.extend(["nvidia", "ì—”ë¹„ë””ì•„", "NVIDIA"])
            if "ìˆ˜ìµ" in question or "revenue" in question_lower:
                keywords.extend(["revenue", "ìˆ˜ìµ", "Revenue", "REVENUE"])
            if "ì˜¬í•´" in question or "2024" in question or "fiscal" in question_lower:
                keywords.extend(["2024", "FY2024", "fiscal"])
            
            # ëª¨ë“  ì²­í¬ë¥¼ ìˆœíšŒí•˜ë©° ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            scored_chunks = []
            for chunk_id, chunk_info in chunks_data.items():
                content = chunk_info.get('content', '').lower()
                score = 0
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                for keyword in keywords:
                    if keyword.lower() in content:
                        score += 10
                
                # ì§ˆë¬¸ì˜ ì£¼ìš” ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš° ì¶”ê°€ ì ìˆ˜
                question_words = re.findall(r'\b\w+\b', question_lower)
                for word in question_words:
                    if len(word) > 2 and word in content:
                        score += 1
                
                scored_chunks.append((score, chunk_id, chunk_info))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìš°ì„ )
            scored_chunks.sort(key=lambda x: x[0], reverse=True)
            
            # ìƒìœ„ 10ê°œ ì„ íƒ
            top_chunks = scored_chunks[:10]
            
            for idx, (score, chunk_id, chunk_info) in enumerate(top_chunks, 1):
                excerpt = chunk_info.get('content', '')[:300]  # ì²˜ìŒ 300ìë§Œ
                
                # íŒŒì¼ëª… ê²°ì •: data_sourcesì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
                file_name = "uploaded_document.pdf"
                if pdf_files:
                    # ì—¬ëŸ¬ íŒŒì¼ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš© (ë˜ëŠ” ì²­í¬ ID ê¸°ë°˜ìœ¼ë¡œ ë§¤í•‘)
                    file_name = pdf_files[0] if len(pdf_files) == 1 else pdf_files[idx % len(pdf_files)]
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
                page_number = chunk_info.get('page_number', 0)
                original_sentence = chunk_info.get('original_sentence', excerpt)
                
                sources.append({
                    "id": idx,
                    "file": file_name,
                    "chunk_id": chunk_id,
                    "excerpt": excerpt,
                    "tokens": chunk_info.get('tokens', 0),
                    "page_number": page_number,
                    "original_sentence": original_sentence
                })
            
            print(f"[DEBUG] {len(sources)}ê°œì˜ ì¶œì²˜ ì¶”ì¶œ ì™„ë£Œ (ì§ˆë¬¸: {question[:50]}...)")
            
        except Exception as e:
            print(f"[DEBUG] ì¶œì²˜ ì¶”ì¶œ ì¤‘ ì—ëŸ¬: {e}")
        
        return sources
    
    async def aglobal_search(self, question: str, top_k: int = 5, temperature: float = 0.2) -> Dict:
        """
        ì „ì²´ ê·¸ë˜í”„ì˜ Community Summaryë¥¼ í™œìš©í•œ ì „ì—­ ê²€ìƒ‰
        
        "ì´ ëª¨ë“  ë¬¸ì„œë“¤ì˜ ê³µí†µ ë¦¬ìŠ¤í¬ëŠ”?" ê°™ì€ ì§ˆë¬¸ì— ëŒ€ì‘
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ì»¤ë®¤ë‹ˆí‹° ìˆ˜
            temperature: LLM temperature
            
        Returns:
            {
                "answer": str,
                "sources": List[dict],
                "search_type": "global"
            }
        """
        print(f"[GLOBAL SEARCH] ì „ì—­ ê²€ìƒ‰ ì‹œì‘: {question}")
        
        # nano-graphragì˜ global search mode í™œìš©
        query_param = QueryParam(
            mode="global",  # global mode
            only_need_context=False,
            top_k=top_k
        )
        
        # Community reports ë¡œë“œ
        community_reports = self._load_community_reports()
        
        # LLMìœ¼ë¡œ ì „ì²´ ìš”ì•½ ìƒì„±
        response = await self.query_rag_api.aquery(
            question,
            param=query_param
        )
        
        # ì»¤ë®¤ë‹ˆí‹° ì†ŒìŠ¤ ì¶”ì¶œ
        sources = self._extract_community_sources(community_reports, top_k)
        
        print(f"[GLOBAL SEARCH] ì™„ë£Œ: {len(sources)}ê°œ ì»¤ë®¤ë‹ˆí‹° ì°¸ì¡°")
        
        return {
            "answer": response,
            "sources": sources,
            "search_type": "global"
        }
    
    def _load_community_reports(self) -> Dict:
        """kv_store_community_reports.json ë¡œë“œ"""
        import json
        reports_path = os.path.join(self.working_dir, "kv_store_community_reports.json")
        
        if not os.path.exists(reports_path):
            print("[DEBUG] community_reports íŒŒì¼ì´ ì—†ì–´ìš”")
            return {}
        
        try:
            with open(reports_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[DEBUG] community_reports ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")
            return {}
    
    def _extract_community_sources(self, community_reports: Dict, top_k: int = 5) -> List[Dict]:
        """ì»¤ë®¤ë‹ˆí‹° ë¦¬í¬íŠ¸ì—ì„œ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        sources = []
        
        # ì»¤ë®¤ë‹ˆí‹° ë¦¬í¬íŠ¸ë¥¼ ì†ŒìŠ¤ë¡œ ë³€í™˜
        for idx, (community_id, report_data) in enumerate(list(community_reports.items())[:top_k], 1):
            # report_dataì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if isinstance(report_data, dict):
                # 'report_string' í‚¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                if 'report_string' in report_data:
                    content = report_data['report_string']
                # 'content' í‚¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                elif 'content' in report_data:
                    content = report_data['content']
                # ê·¸ ì™¸ì˜ ê²½ìš° ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ
                else:
                    content = str(report_data)
            else:
                content = str(report_data)
            
            # ì»¤ë®¤ë‹ˆí‹° ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ ì¤„ì˜ # ì œê±°)
            lines = content.split('\n')
            title = lines[0].replace('#', '').strip() if lines else "Community Summary"
            
            # ë‚´ìš© ìš”ì•½ (ì²« 3ì¤„ ì •ë„)
            excerpt = '\n'.join(lines[:3]) if len(lines) > 1 else content[:300]
            
            sources.append({
                "id": idx,
                "file": f"Community {community_id}: {title[:50]}",
                "chunk_id": community_id,
                "excerpt": excerpt[:400],
                "page_number": 0,
                "original_sentence": content[:1000],  # ì „ì²´ ë‚´ìš© (ìµœëŒ€ 1000ì)
                "type": "community"
            })
        
        return sources
    
    def get_graph_stats(self) -> GraphStats:
        """
        ê·¸ë˜í”„ í†µê³„ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        
        Returns:
            ê·¸ë˜í”„ í†µê³„ ë”•ì…”ë„ˆë¦¬ (nodes, edges, status)
        """
        import networkx as nx
        
        graphml_path = os.path.join(self.working_dir, "graph_chunk_entity_relation.graphml")
        
        if not os.path.exists(graphml_path):
            return GraphStats(nodes=0, edges=0, relationships=0, status="no_file")
        
        G = nx.read_graphml(graphml_path)
        
        return GraphStats(
            nodes=G.number_of_nodes(),
            edges=G.number_of_edges(),
            relationships=G.number_of_edges(),
            status="success"
        )
    
    async def _convert_to_domain_nodes(self) -> None:
        """
        Entityë¥¼ Event/Actor/Asset/Factor/Regionìœ¼ë¡œ ë³€í™˜
        
        ë‹¨ê³„:
        1. GraphMLì—ì„œ Entity ì½ê¸°
        2. EntityClassifierë¡œ ë¶„ë¥˜
        3. Neo4jì— ë„ë©”ì¸ ë…¸ë“œ ìƒì„±
        4. ê´€ê³„ ì¶”ë¡  (TRIGGERS, IMPACTS, INVOLVED_IN, LOCATED_IN)
        """
        try:
            import networkx as nx
            
            # Lazy loading
            if self._entity_classifier is None:
                self._entity_classifier = EntityClassifier()
            if self._relationship_inferencer is None:
                self._relationship_inferencer = RelationshipInferencer()
            if self._neo4j_db is None:
                self._neo4j_db = Neo4jDatabase()
            
            # 1. GraphMLì—ì„œ Entity ì½ê¸°
            graphml_path = os.path.join(self.working_dir, "graph_chunk_entity_relation.graphml")
            if not os.path.exists(graphml_path):
                print("âš ï¸  GraphML íŒŒì¼ì´ ì—†ì–´ì„œ ë„ë©”ì¸ ë…¸ë“œ ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            G = nx.read_graphml(graphml_path)
            
            # Entity ë…¸ë“œë§Œ í•„í„°ë§
            entity_nodes = [
                (node_id, data)
                for node_id, data in G.nodes(data=True)
                if data.get('entity_type') == 'entity' or 'entity_name' in data
            ]
            
            if not entity_nodes:
                print("âš ï¸  Entity ë…¸ë“œê°€ ì—†ì–´ì„œ ë„ë©”ì¸ ë…¸ë“œ ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            print(f"ğŸ“Š {len(entity_nodes)}ê°œì˜ Entity ë…¸ë“œ ë°œê²¬")
            
            # 2. Entity ë¶„ë¥˜
            entities_to_classify = []
            for node_id, data in entity_nodes[:50]:  # ìµœëŒ€ 50ê°œë§Œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
                entities_to_classify.append({
                    "id": node_id,
                    "name": data.get('entity_name', node_id),
                    "type": data.get('entity_type', 'unknown'),
                    "description": data.get('description', '')
                })
            
            print(f"ğŸ” {len(entities_to_classify)}ê°œì˜ Entity ë¶„ë¥˜ ì‹œì‘...")
            classifications = await self._entity_classifier.classify_batch(entities_to_classify)
            
            # 3. Neo4jì— ë„ë©”ì¸ ë…¸ë“œ ìƒì„±
            domain_nodes = {
                "Event": [],
                "Actor": [],
                "Asset": [],
                "Factor": [],
                "Region": []
            }
            
            for entity, classification in zip(entities_to_classify, classifications):
                category = classification.get("category", "None")
                confidence = classification.get("confidence", 0.0)
                
                # ì‹ ë¢°ë„ê°€ 0.6 ì´ìƒì¸ ê²½ìš°ë§Œ ìƒì„±
                if confidence < 0.6 or category == "None":
                    continue
                
                # ë…¸ë“œ ì†ì„± ì¶”ë¡ 
                node_properties = self._entity_classifier.infer_node_properties(
                    entity_name=entity["name"],
                    category=category,
                    entity_data=entity
                )
                
                # Neo4jì— ë…¸ë“œ ìƒì„±
                try:
                    await asyncio.to_thread(
                        self._neo4j_db.create_domain_node,
                        node_type=category,
                        node_id=entity["id"],
                        node_data=node_properties
                    )
                    
                    # ë…¸ë“œ ì •ë³´ ì €ì¥ (ê´€ê³„ ì¶”ë¡ ìš©)
                    domain_nodes[category].append({
                        "id": entity["id"],
                        **node_properties
                    })
                    
                    print(f"  âœ… {category} ë…¸ë“œ ìƒì„±: {entity['name']} (ì‹ ë¢°ë„: {confidence:.2f})")
                    
                except Exception as e:
                    print(f"  âš ï¸  {category} ë…¸ë“œ ìƒì„± ì‹¤íŒ¨: {entity['name']} - {e}")
            
            # í†µê³„ ì¶œë ¥
            total_nodes = sum(len(nodes) for nodes in domain_nodes.values())
            print(f"\nğŸ“Š ë„ë©”ì¸ ë…¸ë“œ ìƒì„± ì™„ë£Œ: ì´ {total_nodes}ê°œ")
            for category, nodes in domain_nodes.items():
                if nodes:
                    print(f"  - {category}: {len(nodes)}ê°œ")
            
            # 4. ê´€ê³„ ì¶”ë¡  (Event, Factor, Assetì´ ìˆì„ ë•Œë§Œ)
            if domain_nodes["Event"] and domain_nodes["Factor"]:
                print("\nğŸ”— ê´€ê³„ ì¶”ë¡  ì‹œì‘...")
                
                # Event â†’ Factor (TRIGGERS)
                for event in domain_nodes["Event"][:10]:  # ìµœëŒ€ 10ê°œë§Œ
                    triggers = await self._relationship_inferencer.infer_triggers(
                        event=event,
                        factors=domain_nodes["Factor"]
                    )
                    
                    for rel in triggers:
                        try:
                            await asyncio.to_thread(
                                self._neo4j_db.create_domain_relationship,
                                rel_type=rel["type"],
                                source_id=rel["source_id"],
                                target_id=rel["target_id"],
                                source_label=rel["source_label"],
                                target_label=rel["target_label"],
                                rel_data=rel
                            )
                            print(f"  âœ… TRIGGERS ê´€ê³„ ìƒì„±: {event['name']} â†’ Factor")
                        except Exception as e:
                            print(f"  âš ï¸  TRIGGERS ê´€ê³„ ìƒì„± ì‹¤íŒ¨: {e}")
                
                # Factor â†’ Asset (IMPACTS)
                if domain_nodes["Asset"]:
                    for factor in domain_nodes["Factor"][:10]:  # ìµœëŒ€ 10ê°œë§Œ
                        impacts = await self._relationship_inferencer.infer_impacts(
                            factor=factor,
                            assets=domain_nodes["Asset"]
                        )
                        
                        for rel in impacts:
                            try:
                                await asyncio.to_thread(
                                    self._neo4j_db.create_domain_relationship,
                                    rel_type=rel["type"],
                                    source_id=rel["source_id"],
                                    target_id=rel["target_id"],
                                    source_label=rel["source_label"],
                                    target_label=rel["target_label"],
                                    rel_data=rel
                                )
                                print(f"  âœ… IMPACTS ê´€ê³„ ìƒì„±: Factor â†’ {rel.get('direction', 'Unknown')} Asset")
                            except Exception as e:
                                print(f"  âš ï¸  IMPACTS ê´€ê³„ ìƒì„± ì‹¤íŒ¨: {e}")
                
                print("âœ… ë„ë©”ì¸ ë…¸ë“œ ë³€í™˜ ì™„ë£Œ!")
            else:
                print("âš ï¸  Event ë˜ëŠ” Factorê°€ ì—†ì–´ì„œ ê´€ê³„ ì¶”ë¡ ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        except Exception as e:
            print(f"âŒ ë„ë©”ì¸ ë…¸ë“œ ë³€í™˜ ì¤‘ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()

