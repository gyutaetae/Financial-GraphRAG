"""
GraphRAG í•µì‹¬ ë¡œì§
ì¸ë±ì‹± ë° ê²€ìƒ‰ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” íŒŒì¼ì´ì—ìš”!
"""

import os
import asyncio
import sys
from typing import Optional, Literal

from nano_graphrag import GraphRAG
from nano_graphrag.base import QueryParam

# graspologic íŒ¨í‚¤ì§€ê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸ ëª¨ë“ˆ
try:
    import graspologic
    import graspologic.utils
except ImportError:
    class DummyGraspologic:
        class partition:
            @staticmethod
            def hierarchical_leiden(*args, **kwargs):
                return {}
        
        class utils:
            @staticmethod
            def largest_connected_component(graph):
                return graph
    
    sys.modules['graspologic'] = DummyGraspologic()
    sys.modules['graspologic.partition'] = DummyGraspologic.partition
    sys.modules['graspologic.utils'] = DummyGraspologic.utils
    print("âš ï¸  graspologicê°€ ì—†ì–´ì„œ ë”ë¯¸ ëª¨ë“ˆì„ ì‚¬ìš©í•´ìš”. í´ëŸ¬ìŠ¤í„°ë§ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìžˆì–´ìš”.")

# src ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import (
    WORKING_DIR,
    DEV_MODE,
    DEV_MODE_MAX_CHARS,
    validate_config,
    NEO4J_AUTO_EXPORT,
)

from utils import (
    openai_model_if,
    openai_embedding_if,
    ollama_model_if,
    ollama_embedding_if,
    preprocess_text,
    chunk_text,
    get_financial_entity_prompt,
)

from engine.planner import QueryPlanner
from models.neo4j_models import GraphStats


class HybridGraphRAGEngine:
    """
    í•˜ì´ë¸Œë¦¬ë“œ GraphRAG ì—”ì§„ í´ëž˜ìŠ¤
    ì¸ë±ì‹±ì€ OpenAI APIë¥¼ ì‚¬ìš©í•˜ê³ , ì§ˆë¬¸ì€ API/LOCAL ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìžˆì–´ìš”!
    """
    
    def __init__(self, working_dir: Optional[str] = None) -> None:
        """
        GraphRAG ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            working_dir: ê·¸ëž˜í”„ ë°ì´í„°ë¥¼ ì €ìž¥í•  í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: config.WORKING_DIR)
        """
        validate_config()
        
        self.working_dir = working_dir or WORKING_DIR
        os.makedirs(self.working_dir, exist_ok=True)
        
        # ì¸ë±ì‹±ìš© GraphRAG ì¸ìŠ¤í„´ìŠ¤ (í•­ìƒ OpenAI API ì‚¬ìš©)
        self.indexing_rag = GraphRAG(
            working_dir=self.working_dir,
            best_model_func=openai_model_if,
            cheap_model_func=openai_model_if,
            embedding_func=openai_embedding_if,
            chunk_token_size=2000,  # 1200 -> 2000 (API í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œ)
            addon_params={
                "entity_extract_max_gleaning": 0,  # 1 -> 0 (ìž¬ì¶”ì¶œ ë¹„í™œì„±í™”ë¡œ 2ë°° ì†ë„ í–¥ìƒ)
                "entity_summary_to_max_tokens": 300,  # ìš”ì•½ ê¸¸ì´ ì œí•œ
            }
        )
        
        # ì§ˆë¬¸ìš© GraphRAG ì¸ìŠ¤í„´ìŠ¤ë“¤ (API/LOCAL ì„ íƒ ê°€ëŠ¥)
        self.query_rag_api = GraphRAG(
            working_dir=self.working_dir,
            best_model_func=openai_model_if,
            cheap_model_func=openai_model_if,
            embedding_func=openai_embedding_if,
        )
        
        self.query_rag_local = GraphRAG(
            working_dir=self.working_dir,
            best_model_func=ollama_model_if,
            cheap_model_func=ollama_model_if,
            embedding_func=openai_embedding_if,  # ì¸ë±ì‹±ê³¼ ê°™ì€ embedding ì‚¬ìš©!
        )
        
        print(f"HybridGraphRAGEngine ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ìž‘ì—… ë””ë ‰í† ë¦¬: {self.working_dir}")
        print(f"ì¸ë±ì‹± ëª¨ë“œ: OpenAI API")
        print(f"ì§ˆë¬¸ ëª¨ë“œ: API ë˜ëŠ” LOCAL ì„ íƒ ê°€ëŠ¥")
    
    async def ainsert(self, text: str) -> None:
        """
        ë¹„ë™ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëž˜í”„ì— ì¸ë±ì‹±í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            text: ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸
        """
        # ê°œë°œ ëª¨ë“œì¼ ë•ŒëŠ” í…ìŠ¤íŠ¸ë¥¼ ì§§ê²Œ ìžë¦„
        if DEV_MODE:
            text = text[:DEV_MODE_MAX_CHARS]
            print(f"[DEV_MODE] í…ìŠ¤íŠ¸ë¥¼ {DEV_MODE_MAX_CHARS}ìžë¡œ ì œí•œí–ˆì–´ìš”!")
        
        # 1) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = preprocess_text(text)
        
        # 2) ì²­í¬ ë¶„í• 
        chunks = chunk_text(processed_text, max_tokens=1200)
        print(f"[DEBUG] ì¸ë±ì‹±ìš© ì²­í¬ ê°œìˆ˜: {len(chunks)}")
        
        # 3) ë¹„ë™ê¸° ë³‘ë ¬ ì¸ë±ì‹± (ìµœëŒ€ ë™ì‹œ 15ê°œë¡œ ì¦ê°€)
        semaphore = asyncio.Semaphore(15)  # 10 -> 15 (ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€)
        
        async def insert_one(chunk_text: str, idx: int) -> None:
            async with semaphore:
                print(f"[DEBUG] ì²­í¬ {idx+1}/{len(chunks)} ì¸ë±ì‹± ì‹œìž‘")
                await self.indexing_rag.ainsert(chunk_text)
                print(f"[DEBUG] ì²­í¬ {idx+1}/{len(chunks)} ì¸ë±ì‹± ì™„ë£Œ")
        
        # 4) ëª¨ë“  ì²­í¬ë¥¼ ë™ì‹œì— ì¸ë±ì‹±
        tasks = [insert_one(chunk, i) for i, chunk in enumerate(chunks)]
        await asyncio.gather(*tasks)
        
        print("ì¸ë±ì‹± ì™„ë£Œ! (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ + í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©)")
        
        # 5) Neo4jë¡œ ìžë™ ì—…ë¡œë“œ (ì„¤ì •ë˜ì–´ ìžˆì„ ê²½ìš°)
        if NEO4J_AUTO_EXPORT:
            print("Neo4jë¡œ ìžë™ ì—…ë¡œë“œ ì‹œìž‘...")
            try:
                from ..db.neo4j_db import Neo4jDatabase
                graphml_path = os.path.join(self.working_dir, "graph_chunk_entity_relation.graphml")
                
                if os.path.exists(graphml_path):
                    db = Neo4jDatabase()
                    result = await asyncio.to_thread(
                        db.upload_graphml,
                        graphml_path,
                        clear_before=False
                    )
                    if result["status"] == "success":
                        print(f"Neo4j ì—…ë¡œë“œ ì™„ë£Œ! ë…¸ë“œ: {result['nodes']}ê°œ, ê´€ê³„: {result['edges']}ê°œ")
                    else:
                        print(f"Neo4j ì—…ë¡œë“œ ì‹¤íŒ¨: {result['message']}")
                else:
                    print(f"GraphML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”: {graphml_path}")
            except Exception as e:
                print(f"Neo4j ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                print("NEO4J_URI, NEO4J_PASSWORDê°€ .envì— ì„¤ì •ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”!")
        else:
            print("Neo4j ìžë™ ì—…ë¡œë“œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìžˆì–´ìš”. NEO4J_AUTO_EXPORT=trueë¡œ ì„¤ì •í•˜ë©´ ìžë™ ì—…ë¡œë“œë©ë‹ˆë‹¤!")
    
    async def aquery(
        self,
        question: str,
        mode: Literal["api", "local"] | None = None,
        auto_plan: bool = True
    ) -> str:
        """
        ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸ì— ë‹µë³€ì„ ì°¾ëŠ” í•¨ìˆ˜
        
        ê·œì¹™: Planner-Executor íŒ¨í„´ ì‚¬ìš©
        - auto_plan=True: Plannerê°€ ìžë™ìœ¼ë¡œ ëª¨ë“œ ê²°ì •
        - auto_plan=False: mode íŒŒë¼ë¯¸í„° ì‚¬ìš©
        
        Args:
            question: ì§ˆë¬¸ ë‚´ìš©
            mode: "api" (OpenAI API) ë˜ëŠ” "local" (Ollama) - auto_plan=Falseì¼ ë•Œë§Œ ì‚¬ìš©
            auto_plan: Plannerë¥¼ ì‚¬ìš©í•˜ì—¬ ìžë™ìœ¼ë¡œ ëª¨ë“œ ê²°ì • (ê¸°ë³¸ê°’: True)
            
        Returns:
            ë‹µë³€ í…ìŠ¤íŠ¸
        """
        # Plannerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“œ ìžë™ ê²°ì •
        if auto_plan and mode is None:
            planner = QueryPlanner()
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë³µìž¡ë„ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„ì„ í•„ìš”)
            mode, complexity, privacy = planner.analyze_query(
                question=question,
                entity_count=1,  # ì‹¤ì œë¡œëŠ” ê·¸ëž˜í”„ì—ì„œ ì¶”ì •
                relationship_depth=2,  # ê¸°ë³¸ê°’
                has_pii=False,  # ì‹¤ì œë¡œëŠ” ë°ì´í„° ë¶„ì„ í•„ìš”
                needs_synthesis="cross" in question.lower() or "compare" in question.lower()
            )
            print(f"[Planner] ëª¨ë“œ: {mode}, ë³µìž¡ë„: {complexity}, í”„ë¼ì´ë²„ì‹œ: {privacy}")
        
        # modeê°€ Noneì´ë©´ ê¸°ë³¸ê°’ local ì‚¬ìš©
        if mode is None:
            mode = "local"
        
        print(f"[DEBUG] ì§ˆë¬¸: {question}")
        print(f"[DEBUG] ëª¨ë“œ: {mode}")
        print(f"[DEBUG] ìž‘ì—… ë””ë ‰í† ë¦¬: {self.working_dir}")
        
        # ê·¸ëž˜í”„ íŒŒì¼ í™•ì¸
        graphml_path = os.path.join(self.working_dir, "graph_chunk_entity_relation.graphml")
        if os.path.exists(graphml_path):
            import networkx as nx
            G = nx.read_graphml(graphml_path)
            print(f"[DEBUG] ê·¸ëž˜í”„ ë…¸ë“œ ìˆ˜: {G.number_of_nodes()}, ì—£ì§€ ìˆ˜: {G.number_of_edges()}")
        else:
            print(f"[DEBUG] ê·¸ëž˜í”„ íŒŒì¼ì´ ì—†ì–´ìš”: {graphml_path}")
        
        if mode == "api":
            print(f"ì§ˆë¬¸ ëª¨ë“œ: OpenAI API")
            try:
                # Global ëª¨ë“œ: ì „ì²´ ê·¸ëž˜í”„ì—ì„œ ì»¤ë®¤ë‹ˆí‹° ë¦¬í¬íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (ë„“ì€ ë²”ìœ„, revenue ê°™ì€ ì§ˆë¬¸ì— ì í•©)
                query_param = QueryParam(
                    mode='global',  # local -> global (ì „ì²´ ê·¸ëž˜í”„ ê²€ìƒ‰)
                    top_k=30,  # 20 -> 30 (ë” ë§Žì€ ì»¨í…ìŠ¤íŠ¸)
                )
                response = await self.query_rag_api.aquery(question, param=query_param)
                print(f"ðŸ” [DEBUG] query_rag_api.aquery() ì™„ë£Œ!")
            except Exception as e:
                print(f"âŒ [DEBUG] query_rag_api.aquery() ì—ëŸ¬: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                raise
        else:
            print(f"ðŸ’¬ ì§ˆë¬¸ ëª¨ë“œ: Ollama (ë¡œì»¬)")
            # Ollama ì„œë²„ í™•ì¸
            try:
                import requests
                ollama_check = requests.get("http://localhost:11434/api/tags", timeout=2)
                if ollama_check.status_code != 200:
                    return "âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì–´ìš”! 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œìž‘í•´ì£¼ì„¸ìš”!"
            except:
                return "âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ì–´ìš”! 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œìž‘í•˜ê±°ë‚˜, 'api' ëª¨ë“œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”!"
            
            try:
                # Global ëª¨ë“œë¡œ ê²€ìƒ‰
                query_param = QueryParam(
                    mode='global',  # ì „ì²´ ê·¸ëž˜í”„ ê²€ìƒ‰
                    top_k=30,
                )
                response = await self.query_rag_local.aquery(question, param=query_param)
                print(f"ðŸ” [DEBUG] query_rag_local.aquery() ì™„ë£Œ!")
            except Exception as e:
                print(f"âŒ [DEBUG] query_rag_local.aquery() ì—ëŸ¬: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                raise
        
        # ë‹µë³€ì´ ë¹„ì–´ìžˆê±°ë‚˜ "Sorry"ë¡œ ì‹œìž‘í•˜ë©´ ê²½ê³ 
        if not response or response.strip().startswith("Sorry"):
            print("âš ï¸  ê·¸ëž˜í”„ì— ë°ì´í„°ê°€ ìžˆì§€ë§Œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”.")
            print("ðŸ’¡ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”!")
        
        return response
    
    def get_graph_stats(self) -> GraphStats:
        """
        ê·¸ëž˜í”„ í†µê³„ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        
        Returns:
            ê·¸ëž˜í”„ í†µê³„ ë”•ì…”ë„ˆë¦¬ (nodes, edges, status)
        """
        import networkx as nx
        
        graphml_path = os.path.join(self.working_dir, "graph_chunk_entity_relation.graphml")
        
        if not os.path.exists(graphml_path):
            return GraphStats(nodes=0, edges=0, relationships=0, status="no_file")
        
        G = nx.read_graphml(graphml_path)
        
        return GraphStats(
            nodes=G.number_of_nodes(),
            edges=G.number_of_edges(),
            relationships=G.number_of_edges(),
            status="success"
        )

