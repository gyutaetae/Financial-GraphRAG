services:
  # Neo4j Graph Database
  neo4j:
    image: neo4j:5.15.0
    container_name: finance-graphrag-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/your_password_here
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=512m
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Ollama (Local LLM Server)
  ollama:
    image: ollama/ollama:latest
    container_name: finance-graphrag-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - graphrag-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
  
  # Ollama Model Loader (Qwen2.5-Coder-3B 자동 다운로드)
  ollama-loader:
    image: curlimages/curl:latest
    container_name: finance-graphrag-ollama-loader
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - graphrag-network
    entrypoint: |
      sh -c "
        echo 'Waiting for Ollama to be ready...'
        sleep 5
        echo 'Downloading Qwen2.5-Coder-3B model...'
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"qwen2.5-coder:3b\"}'
        echo 'Downloading nomic-embed-text model...'
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"nomic-embed-text\"}'
        echo 'Models downloaded successfully!'
      "
    restart: "no"

  # FastAPI Backend
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: finance-graphrag-backend
    ports:
      - "8000:8000"
    environment:
      # Run Mode: API (OpenAI) or LOCAL (Ollama)
      - RUN_MODE=${RUN_MODE:-API}
      
      # OpenAI Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      
      # Neo4j Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-your_password_here}
      - NEO4J_AUTO_EXPORT=${NEO4J_AUTO_EXPORT:-false}
      
      # MCP Configuration
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - MCP_CONFIG_PATH=mcp-config.json
      - MCP_LAZY_LOAD=true
      
      # Domain Schema Configuration
      - ENABLE_DOMAIN_SCHEMA=${ENABLE_DOMAIN_SCHEMA:-true}
      - DOMAIN_CLASSIFICATION_MODEL=${DOMAIN_CLASSIFICATION_MODEL:-gpt-4o-mini}
      
      # Router Configuration
      - ROUTER_MODEL=${ROUTER_MODEL:-gpt-4o-mini}
      - ROUTER_TEMPERATURE=${ROUTER_TEMPERATURE:-0.0}
      
      # Ollama Configuration
      - OLLAMA_BASE_URL=http://ollama:11434
      
      # Application Settings
      - GRAPH_STORAGE_PATH=/app/storage/graph_storage
      - LOG_LEVEL=INFO
    volumes:
      - ./storage:/app/storage
      - ./logs:/app/logs
      - ./data:/app/data
      - ./src:/app/src
    networks:
      - graphrag-network
    depends_on:
      neo4j:
        condition: service_healthy
      ollama:
        condition: service_healthy
    command: uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped

  # Streamlit Frontend
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: finance-graphrag-frontend
    ports:
      - "8501:8501"
    environment:
      # Backend API URL
      - API_BASE_URL=http://backend:8000
      
      # Neo4j Configuration (for direct visualization)
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-your_password_here}
      
      # Streamlit Configuration
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    volumes:
      - ./storage:/app/storage
      - ./logs:/app/logs
      - ./data:/app/data
      - ./src:/app/src
    networks:
      - graphrag-network
    depends_on:
      - backend
    command: streamlit run src/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
    restart: unless-stopped

networks:
  graphrag-network:
    driver: bridge

volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  ollama_data:
    driver: local

